
@misc{meta_introducing_2024,
	title = {Introducing {Meta} {Llama} 3: {The} most capable openly available {LLM} to date},
	url = {https://ai.meta.com/blog/meta-llama-3/},
	urldate = {2024-05-24},
	journal = {Introducing Meta Llama 3: The most capable openly available LLM to date},
	author = {Meta},
	month = apr,
	year = {2024},
}

@misc{huang_clinicalbert_2020,
	title = {{ClinicalBERT}: {Modeling} {Clinical} {Notes} and {Predicting} {Hospital} {Readmission}},
	shorttitle = {{ClinicalBERT}},
	url = {http://arxiv.org/abs/1904.05342},
	abstract = {Clinical notes contain information about patients beyond structured data such as lab values or medications. However, clinical notes have been underused relative to structured data, because notes are highdimensional and sparse. We aim to develop and evaluate a continuous representation of clinical notes. Given this representation, our goal is to predict 30-day hospital readmission at various timepoints of admission, including early stages and at discharge. We apply bidirectional encoder representations from transformers (bert) to clinical text. Publicly-released bert parameters are trained on standard corpora such as Wikipedia and BookCorpus, which differ from clinical text. We therefore pre-train bert using clinical notes and finetune the network for the task of predicting hospital readmission. This defines ClinicalBERT. ClinicalBERT uncovers high-quality relationships between medical concepts, as judged by physicians. ClinicalBERT outperforms various baselines on 30-day hospital readmission prediction using both discharge summaries and the first few days of notes in the intensive care unit on various clinically-motivated metrics. The attention weights of ClinicalBERT can also be used to interpret predictions. To facilitate research, we open-source model parameters, and scripts for training and evaluation. ClinicalBERT is a flexible framework to represent clinical notes. It improves on previous clinical text processing methods and with little engineering can be adapted to other clinical predictive tasks.},
	language = {en},
	urldate = {2024-05-25},
	publisher = {arXiv},
	author = {Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
	month = nov,
	year = {2020},
	note = {arXiv:1904.05342 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: CHIL 2020 Workshop},
	file = {Huang et al. - 2020 - ClinicalBERT Modeling Clinical Notes and Predicti.pdf:/Users/mab/Zotero/storage/CS9MUPKI/Huang et al. - 2020 - ClinicalBERT Modeling Clinical Notes and Predicti.pdf:application/pdf},
}

@article{kesgin_introducing_nodate,
	title = {Introducing {cosmosGPT}: {Monolingual} {Training} for {Turkish} {Language} {Models}},
	abstract = {The number of open source language models that can produce Turkish is increasing day by day, as in other languages. In order to create the basic versions of such models, the training of multilingual models is usually continued with Turkish corpora. The alternative is to train the model with only Turkish corpora. In this study, we first introduce the cosmosGPT models that we created with this alternative method. Then, we introduce new finetune datasets for basic language models to fulfill user requests and new evaluation datasets for measuring the capabilities of Turkish language models. Finally, a comprehensive comparison of the adapted Turkish language models on different capabilities is presented. The results show that the language models we built with the monolingual corpus have promising performance despite being about 10 times smaller than the others.},
	language = {en},
	author = {Kesgin, H Toprak and Yuce, M Kaan and Dogan, Eren and Uzun, M Egemen and Uz, Atahan and Seyrek, H Emre and Zeer, Ahmed and Amasyali, M Fatih},
	annote = {başlık
},
	file = {Kesgin et al. - Introducing cosmosGPT Monolingual Training for Tu.pdf:/Users/mab/Zotero/storage/AILGJ2NY/Kesgin et al. - Introducing cosmosGPT Monolingual Training for Tu.pdf:application/pdf},
}

@misc{chalkidis_legal-bert_2020,
	title = {{LEGAL}-{BERT}: {The} {Muppets} straight out of {Law} {School}},
	shorttitle = {{LEGAL}-{BERT}},
	url = {http://arxiv.org/abs/2010.02559},
	abstract = {BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our ﬁndings indicate that the previous guidelines for pre-training and ﬁnetuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-speciﬁc corpora, and (c) pre-train BERT from scratch on domain-speciﬁc corpora. We also propose a broader hyper-parameter search space when ﬁne-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.},
	language = {en},
	urldate = {2024-05-25},
	publisher = {arXiv},
	author = {Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
	month = oct,
	year = {2020},
	note = {arXiv:2010.02559 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 5 pages, short paper in Findings of EMNLP 2020},
	file = {Chalkidis et al. - 2020 - LEGAL-BERT The Muppets straight out of Law School.pdf:/Users/mab/Zotero/storage/JUIISHS4/Chalkidis et al. - 2020 - LEGAL-BERT The Muppets straight out of Law School.pdf:application/pdf},
}

@misc{raval_llsourcellchatgpt_in_5_minutes_2024,
	title = {{llSourcell}/{ChatGPT}\_in\_5\_Minutes},
	url = {https://github.com/llSourcell/ChatGPT_in_5_Minutes},
	abstract = {This is the code for "ChatGPT in 5 Minutes" By Siraj Raval on Youtube},
	urldate = {2024-05-26},
	author = {Raval, Siraj},
	month = apr,
	year = {2024},
	note = {original-date: 2023-02-15T23:39:29Z},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:/Users/mab/Zotero/storage/N7XR8IFJ/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf},
}

@misc{zhou_lima_2023,
	title = {{LIMA}: {Less} {Is} {More} for {Alignment}},
	shorttitle = {{LIMA}},
	url = {http://arxiv.org/abs/2305.11206},
	abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model ﬁne-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow speciﬁc response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
	month = may,
	year = {2023},
	note = {arXiv:2305.11206 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {https://youtu.be/TCAEWap9mSQ
},
	file = {Zhou et al. - 2023 - LIMA Less Is More for Alignment.pdf:/Users/mab/Zotero/storage/PY6C36KS/Zhou et al. - 2023 - LIMA Less Is More for Alignment.pdf:application/pdf},
}

@misc{mukherjee_orca_2023,
	title = {Orca: {Progressive} {Learning} from {Complex} {Explanation} {Traces} of {GPT}-4},
	shorttitle = {Orca},
	url = {http://arxiv.org/abs/2306.02707},
	abstract = {Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model’s capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca, a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100\% in complex zero-shot reasoning benchmarks like BigBench Hard (BBH) and 42\% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02707 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {https://youtu.be/D8eZugu63vI

},
	file = {Mukherjee et al. - 2023 - Orca Progressive Learning from Complex Explanatio.pdf:/Users/mab/Zotero/storage/YF9DLY26/Mukherjee et al. - 2023 - Orca Progressive Learning from Complex Explanatio.pdf:application/pdf},
}

@misc{wu_reft_2024,
	title = {{ReFT}: {Representation} {Finetuning} for {Language} {Models}},
	shorttitle = {{ReFT}},
	url = {http://arxiv.org/abs/2404.03592},
	abstract = {Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. Here, we pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10×–50× more parameter-efficient than prior state-of-the-art PEFTs. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D. and Potts, Christopher},
	month = may,
	year = {2024},
	note = {arXiv:2404.03592 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: preprint},
	file = {Wu et al. - 2024 - ReFT Representation Finetuning for Language Model.pdf:/Users/mab/Zotero/storage/2YI74L66/Wu et al. - 2024 - ReFT Representation Finetuning for Language Model.pdf:application/pdf},
}

@article{li_chatdoctor_2023,
	title = {{ChatDoctor}: {A} {Medical} {Chat} {Model} {Fine}-{Tuned} on a {Large} {Language} {Model} {Meta}-{AI} ({LLaMA}) {Using} {Medical} {Domain} {Knowledge}},
	issn = {2168-8184},
	shorttitle = {{ChatDoctor}},
	url = {https://www.cureus.com/articles/152858-chatdoctor-a-medical-chat-model-fine-tuned-on-a-large-language-model-meta-ai-llama-using-medical-domain-knowledge},
	doi = {10.7759/cureus.40895},
	abstract = {Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice.
Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases.
Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses.
Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.},
	language = {en},
	urldate = {2024-05-26},
	journal = {Cureus},
	author = {Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
	month = jun,
	year = {2023},
	file = {Li et al. - 2023 - ChatDoctor A Medical Chat Model Fine-Tuned on a L.pdf:/Users/mab/Zotero/storage/KGPLWU8Z/Li et al. - 2023 - ChatDoctor A Medical Chat Model Fine-Tuned on a L.pdf:application/pdf},
}

@article{li_chatdoctor_2023-1,
	title = {{ChatDoctor}: {A} {Medical} {Chat} {Model} {Fine}-{Tuned} on a {Large} {Language} {Model} {Meta}-{AI} ({LLaMA}) {Using} {Medical} {Domain} {Knowledge}},
	issn = {2168-8184},
	shorttitle = {{ChatDoctor}},
	url = {https://www.cureus.com/articles/152858-chatdoctor-a-medical-chat-model-fine-tuned-on-a-large-language-model-meta-ai-llama-using-medical-domain-knowledge},
	doi = {10.7759/cureus.40895},
	abstract = {Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice.
Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases.
Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses.
Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.},
	language = {en},
	urldate = {2024-05-26},
	journal = {Cureus},
	author = {Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
	month = jun,
	year = {2023},
	file = {Li et al. - 2023 - ChatDoctor A Medical Chat Model Fine-Tuned on a L.pdf:/Users/mab/Zotero/storage/ZRQN9GLA/Li et al. - 2023 - ChatDoctor A Medical Chat Model Fine-Tuned on a L.pdf:application/pdf},
}

@misc{roziere_code_2024,
	title = {Code {Llama}: {Open} {Foundation} {Models} for {Code}},
	shorttitle = {Code {Llama}},
	url = {http://arxiv.org/abs/2308.12950},
	abstract = {We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67\% and 65\% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Rozière, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and Rapin, Jérémy and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and Défossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
	month = jan,
	year = {2024},
	note = {arXiv:2308.12950 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Rozière et al. - 2024 - Code Llama Open Foundation Models for Code.pdf:/Users/mab/Zotero/storage/H6KTUAJG/Rozière et al. - 2024 - Code Llama Open Foundation Models for Code.pdf:application/pdf},
}

@misc{yang_large_2024,
	title = {Large {Language} {Models} as {Optimizers}},
	url = {http://arxiv.org/abs/2309.03409},
	abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
	month = apr,
	year = {2024},
	note = {arXiv:2309.03409 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: ICLR 2024; 42 pages, 26 figures, 15 tables. Code at https://github.com/google-deepmind/opro},
	annote = {https://youtu.be/KKpTevFJpJw
},
	file = {Yang et al. - 2024 - Large Language Models as Optimizers.pdf:/Users/mab/Zotero/storage/EEFXVM32/Yang et al. - 2024 - Large Language Models as Optimizers.pdf:application/pdf},
}

@misc{chen_longlora_2024,
	title = {{LongLoRA}: {Efficient} {Fine}-tuning of {Long}-{Context} {Large} {Language} {Models}},
	shorttitle = {{LongLoRA}},
	url = {http://arxiv.org/abs/2309.12307},
	abstract = {We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16× computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention (S2-Attn) effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8× A100 machine. LongLoRA extends models’ context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset. All our code, models, dataset, and demo are available at github.com/dvlab-research/LongLoRA.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
	month = mar,
	year = {2024},
	note = {arXiv:2309.12307 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Code, models, dataset, and demo are available at https://github.com/dvlab-research/LongLoRA},
	annote = {https://youtu.be/hf5N-SlqRmA
},
	file = {Chen et al. - 2024 - LongLoRA Efficient Fine-tuning of Long-Context La.pdf:/Users/mab/Zotero/storage/8T6WR3CA/Chen et al. - 2024 - LongLoRA Efficient Fine-tuning of Long-Context La.pdf:application/pdf},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Extended NeurIPS submission},
	annote = {https://youtu.be/TPcXVJ1VSRI
},
	file = {Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:/Users/mab/Zotero/storage/PDKLIY4L/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf},
}

@misc{peng_q-peft_2024,
	title = {Q-{PEFT}: {Query}-dependent {Parameter} {Efficient} {Fine}-tuning for {Text} {Reranking} with {Large} {Language} {Models}},
	shorttitle = {Q-{PEFT}},
	url = {http://arxiv.org/abs/2404.04522},
	abstract = {Parameter Efficient Fine-Tuning (PEFT) methods have been extensively utilized in Large Language Models (LLMs) to improve the down-streaming tasks without the cost of fine-tuing the whole LLMs. Recent studies have shown how to effectively use PEFT for fine-tuning LLMs in ranking tasks with convincing performance; there are some limitations, including the learned prompt being fixed for different documents, overfitting to specific tasks, and low adaptation ability. In this paper, we propose a query-dependent parameter efficient fine-tuning (Q-PEFT) approach for text reranking, which provides LLMs with insights about the true queries, thereby facilitating the generation of true queries from input documents. Specifically, we utilize the query to extract the top-𝑘 tokens from concatenated documents, serving as contextual clues. We further augment Q-PEFT by substituting the retrieval mechanism with a multi-head attention layer to achieve end-to-end training and cover all the tokens in the documents, guiding the LLMs to generate more document-specific synthetic queries, thereby further improving the reranking performance. Extensive experiments are conducted on four public datasets, demonstrating the effectiveness of our proposed approach.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Peng, Zhiyuan and Wu, Xuyang and Wang, Qifan and Rajanala, Sravanthi and Fang, Yi},
	month = apr,
	year = {2024},
	note = {arXiv:2404.04522 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Peng et al. - 2024 - Q-PEFT Query-dependent Parameter Efficient Fine-t.pdf:/Users/mab/Zotero/storage/EZ5UJKSB/Peng et al. - 2024 - Q-PEFT Query-dependent Parameter Efficient Fine-t.pdf:application/pdf},
}

@misc{jiang_mora_2024,
	title = {{MoRA}: {High}-{Rank} {Updating} for {Parameter}-{Efficient} {Fine}-{Tuning}},
	shorttitle = {{MoRA}},
	url = {http://arxiv.org/abs/2405.12130},
	abstract = {Low-rank adaptation (LoRA) is a popular parameter-efficient fine-tuning (PEFT) method for large language models (LLMs). In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memoryintensive tasks and achieves comparable performance on other tasks. Our code will be available at https://github.com/kongds/MoRA.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Jiang, Ting and Huang, Shaohan and Luo, Shengyue and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and Zhuang, Fuzhen},
	month = may,
	year = {2024},
	note = {arXiv:2405.12130 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Work in Progress},
	file = {Jiang et al. - 2024 - MoRA High-Rank Updating for Parameter-Efficient F.pdf:/Users/mab/Zotero/storage/YMNFJ7SB/Jiang et al. - 2024 - MoRA High-Rank Updating for Parameter-Efficient F.pdf:application/pdf},
}

@misc{chen_parameter-efficient_2024,
	title = {Parameter-{Efficient} {Fine}-{Tuning} {With} {Adapters}},
	url = {http://arxiv.org/abs/2405.05493},
	abstract = {In the arena of language model ﬁne-tuning, the traditional approaches, such as Domain-Adaptive Pretraining (DAPT) and Task-Adaptive Pretraining (TAPT), although effective, but computational intensive. This research introduces a novel adaptation method utilizing the UniPELT framework as a base and added a PromptTuning Layer, which signiﬁcantly reduces the number of trainable parameters while maintaining competitive performance across various benchmarks. Our method employs adapters, which enable efﬁcient transfer of pretrained models to new tasks with minimal retraining of the base model parameters. We evaluate our approach using three diverse datasets: the GLUE benchmark, a domain-speciﬁc dataset comprising four distinct areas, and the Stanford Question Answering Dataset 1.1 (SQuAD). Our results demonstrate that our customized adapter-based method achieves performance comparable to full model ﬁne-tuning, DAPT+TAPT and UniPELT strategies while requiring fewer or equivalent amount of parameters. This parameter efﬁciency not only alleviates the computational burden but also expedites the adaptation process. The study underlines the potential of adapters in achieving high performance with signiﬁcantly reduced resource consumption, suggesting a promising direction for future research in parameter-efﬁcient ﬁnetuning.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Chen, Keyu and Pang, Yuan and Yang, Zi},
	month = may,
	year = {2024},
	note = {arXiv:2405.05493 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Chen et al. - 2024 - Parameter-Efficient Fine-Tuning With Adapters.pdf:/Users/mab/Zotero/storage/SMIFJRRP/Chen et al. - 2024 - Parameter-Efficient Fine-Tuning With Adapters.pdf:application/pdf},
}

@misc{gao_dlora_2024,
	title = {{DLoRA}: {Distributed} {Parameter}-{Efficient} {Fine}-{Tuning} {Solution} for {Large} {Language} {Model}},
	shorttitle = {{DLoRA}},
	url = {http://arxiv.org/abs/2404.05182},
	abstract = {To enhance the performance of large language models (LLM) on downstream tasks, one solution is to fine-tune certain LLM parameters and make it better align with the characteristics of the training dataset. This process is commonly known as parameter-efficient fine-tuning (PEFT). Due to the scale of LLM, PEFT operations are usually executed in the public environment (e.g., cloud server). This necessitates the sharing of sensitive user data across public environments, thereby raising potential privacy concerns. To tackle these challenges, we propose a distributed PEFT framework called DLoRA. DLoRA enables scalable PEFT operations to be performed collaboratively between the cloud and user devices. Coupled with the proposed Kill and Revive algorithm, the evaluation results demonstrate that DLoRA can significantly reduce the computation and communication workload over the user devices while achieving superior accuracy and privacy protection.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Gao, Chao and Zhang, Sai Qian},
	month = apr,
	year = {2024},
	note = {arXiv:2404.05182 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Gao and Zhang - 2024 - DLoRA Distributed Parameter-Efficient Fine-Tuning.pdf:/Users/mab/Zotero/storage/8L4RAPXC/Gao and Zhang - 2024 - DLoRA Distributed Parameter-Efficient Fine-Tuning.pdf:application/pdf},
}

@misc{saxena_training_2021,
	title = {Training {With} {Data} {Dependent} {Dynamic} {Learning} {Rates}},
	url = {http://arxiv.org/abs/2105.13464},
	abstract = {Recently many ﬁrst and second order variants of SGD have been proposed to facilitate training of Deep Neural Networks (DNNs). A common limitation of these works stem from the fact that they use the same learning rate across all instances present in the dataset. This setting is widely adopted under the assumption that lossfunctions for each instance are similar in nature, and hence, a common learning rate can be used. In this work, we relax this assumption and propose an optimization framework which accounts for difference in loss function characteristics across instances. More speciﬁcally, our optimizer learns a dynamic learning rate for each instance present in the dataset. Learning a dynamic learning rate for each instance allows our optimization framework to focus on different modes of training data during optimization. When applied to an image classiﬁcation task, across different CNN architectures, learning dynamic learning rates leads to consistent gains over standard optimizers. When applied to a dataset containing corrupt instances, our framework reduces the learning rates on noisy instances, and improves over the state-of-the-art. Finally, we show that our optimization framework can be used for personalization of a machine learning model towards a known targeted data distribution.},
	language = {en},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Saxena, Shreyas and Vyas, Nidhi and DeCoste, Dennis},
	month = may,
	year = {2021},
	note = {arXiv:2105.13464 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Saxena et al. - 2021 - Training With Data Dependent Dynamic Learning Rate.pdf:/Users/mab/Zotero/storage/AMB778HH/Saxena et al. - 2021 - Training With Data Dependent Dynamic Learning Rate.pdf:application/pdf},
}

@misc{alammar_visual_nodate,
	title = {A {Visual} and {Interactive} {Guide} to the {Basics} of {Neural} {Networks}},
	url = {https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/},
	abstract = {Discussions:
Hacker News (63 points, 8 comments), Reddit r/programming (312 points, 37 comments)

Translations: Arabic, French, Spanish



    


Update: Part 2 is now live: A Visual And Interactive Look at Basic Neural Network Math

Motivation
I’m not a machine learning expert. I’m a software engineer by training and I’ve had little interaction with AI. I had always wanted to delve deeper into machine learning, but never really found my “in”. That’s why when Google open sourced TensorFlow in November 2015, I got super excited and knew it was time to jump in and start the learning journey. Not to sound dramatic, but to me, it actually felt kind of like Prometheus handing down fire to mankind from the Mount Olympus of machine learning. In the back of my head was the idea that the entire field of Big Data and technologies like Hadoop were vastly accelerated when Google researchers released their Map Reduce paper. This time it’s not a paper – it’s the actual software they use internally after years and years of evolution.

So I started learning what I can about the basics of the topic, and saw the need for gentler resources for people with no experience in the field. This is my attempt at that.},
	urldate = {2024-05-26},
	author = {Alammar, Jay},
	file = {Snapshot:/Users/mab/Zotero/storage/D6NZCIK4/Alammar - A Visual and Interactive Guide to the Basics of Ne.html:text/html},
}
