\section{Conclusion}
\label{sec:conclusion}
We embark on employing LLMs as optimizers, where the LLM progressively generates new solutions to optimize an objective function.
We first motivate \name{} with linear regression and traveling salesman problems, then proceed to prompt optimization as a concrete application.
Our evaluation demonstrates that LLMs have the capacity of gradually improving the generated solutions based on the past optimization trajectory.
Interestingly, on small-scale traveling salesman problems, \name{} performs on par with some hand-crafted heuristic algorithms.
For prompt optimization, optimized prompts outperform human-designed prompts on GSM8K and Big-Bench Hard by a significant margin, sometimes over $50\%$.

A number of unresolved questions are open for future research on LLMs for optimization. 
In general, how to reduce the sensitivity to initialization and better balance exploitation with exploration remains a challenge. 
Specifically, for prompt optimization, one limitation of our current implementation is that the optimizer LLM does not effectively utilize error cases in the training set to infer promising directions to improve the generated instructions. 
In our experiments, we tried including error cases in the meta-prompt rather than randomly sampling from the training set at each optimization step, but the results are similar, indicating that the error cases alone are not informative enough for the optimizer LLM to grasp the cause of the wrong prediction.
Another limitation is that prompt optimization requires a training set to compute the accuracy that guides the optimization process. 
Currently the training set at least contains tens of samples, so that the optimized prompt does not severely overfit to the training samples. 
A promising direction is to incorporate richer feedback about the error cases besides the aggregated accuracy, and summarize the key features that distinguish between high-quality and low-quality generated prompts in the optimization trajectory.
Such information may inform the optimizer LLM of how to more efficiently improve over the past generated instructions, and potentially further reduce the example set size needed for prompt optimization.