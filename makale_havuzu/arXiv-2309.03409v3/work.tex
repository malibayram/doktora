\section{Related Work}
\label{sec:work}

\myparagraph{Prompt optimization} Prior works have developed soft prompt-tuning methods that optimize the prompt represented as task-specific continuous vectors~\citep{lester2021power,li2021prefix,liu2021gpt,qin2021learning}, as well as performing discrete prompt optimization by gradient-guided search~\citep{shin2020autoprompt,wen2023hard,gao2020making,chen2023instructzero} and reinforcement learning~\citep{deng2022rlprompt,zhang2022tempera}. 
These approaches become inapplicable when there is only API access to the LLM. 
Other works designed edit-based approaches for gradient-free prompt optimization~\citep{xu2022gps,prasad2022grips}, where the editing can be done with human-defined operations (e.g., swapping two phrases)~\citep{prasad2022grips} or language models (e.g., back translation)~\citep{xu2022gps}. 
Some recent works investigate LLMs for prompt optimization~\citep{zhou2022large,pryzant2023automatic,xu2023wizardlm}. 
Specifically, APE~\citep{zhou2022large} first uses the LLM to generate initial instructions. Afterwards, APE selects top instructions with the highest accuracies, then prompts the LLM with each individual instruction to generate a semantically similar variant of the initial instruction.
APO~\citep{pryzant2023automatic} in each step instructs the LLM to produce text feedback on how to update an old instruction. 
Different from edit-based approaches, the optimizer LLM in our work directly generates new instructions at each optimization step, and the optimizer LLM is merely asked to improve the task accuracy without being required to imitate past instructions. Compared to \citet{zhou2022large} and \citet{pryzant2023automatic}, our optimization process incorporates the past generated instructions with their scores in the meta-prompt, enabling the optimizer LLM to discover common patterns of high-quality instructions.

\myparagraph{Prompting with natural language feedback} A recent line of work investigates approaches to improve the LLM performance by prompting with natural language feedback to revise the model output, which has shown effectiveness in reducing harmful LLM outputs~\citep{bai2022constitutional,ganguli2023capacity}, improving reasoning~\citep{shinn2023reflexion,madaan2023self} and code generation performance~\citep{chen2023teaching,olausson2023demystifying,shinn2023reflexion,chen2023improving}, dialogue applications~\citep{nair2023dera,madaan2023self,yuan2023system}, and so on~\citep{kim2023language,wang2023voyager}. Specifically, \citet{yuan2023system} develops a human-in-the-loop framework for deriving system-level feedback from a collection of instance-level feedback, which is then used for refining data. In our work, the optimizer LLM utilizes the optimization trajectory in the prompt, which implicitly requires the LLM to summarize the common characteristics among solutions with similar scores. We consider incorporating explicit natural language feedback on generated solutions for later optimization steps as future work.

\myparagraph{Tuning language models for optimization}
Some previous works tune or prompt language models to behave as mutation and crossover operators in evolutionary algorithms.
\citet{meyerson2023language} utilizes language models with few-shot exemplars to propose evolutionary cross-overs on tasks such as image and code generation.
In \citet{lehman2022evolution}, the large language model trained on code diff generation is used as the mutation operator, and they further design a fine-tuning method to improve performance in the Sodarace domain for robot simulation.
EvoPrompting~\citep{chen2023evoprompting} uses large language models to evolve neural network architectures, where they combine evolutionary search with soft prompt tuning.
With respect to taking the trajectory as the input for optimization, OptFormer~\citep{chen2022towards} trains a transformer model on large collections of hyperparameter optimization data.
On the other hand, our work performs optimization solely by prompting without additional training.


