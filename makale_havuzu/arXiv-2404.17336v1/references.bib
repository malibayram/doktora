@article{uludougan2024turna,
  title   = {TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation},
  author  = {Uludo{\u{g}}an, G{\"o}k{\c{c}}e and Balal, Zeynep Yirmibe{\c{s}}o{\u{g}}lu and Akkurt, Furkan and T{\"u}rker, Melik{\c{s}}ah and G{\"u}ng{\"o}r, Onur and {\"U}sk{\"u}darl{\i}, Susan},
  journal = {arXiv preprint arXiv:2401.14373},
  year    = {2024}
}

@article{shliazhko2024mgpt,
  title     = {mGPT: Few-Shot Learners Go Multilingual},
  author    = {Shliazhko, Oleh and Fenogenova, Alena and Tikhonova, Maria and Kozlova, Anastasia and Mikhailov, Vladislav and Shavrina, Tatiana},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {12},
  pages     = {58--79},
  year      = {2024},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{nguyen2023culturax,
  title   = {Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages},
  author  = {Nguyen, Thuat and Van Nguyen, Chien and Lai, Viet Dac and Man, Hieu and Ngo, Nghia Trung and Dernoncourt, Franck and Rossi, Ryan A and Nguyen, Thien Huu},
  journal = {arXiv preprint arXiv:2309.09400},
  year    = {2023}
}

@article{achiam2023gpt,
  title   = {Gpt-4 technical report},
  author  = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023}
}

@article{touvron2023llama,
  title   = {Llama: Open and efficient foundation language models},
  author  = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023}
}

@article{jiang2023mistral,
  title   = {Mistral 7B},
  author  = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal = {arXiv preprint arXiv:2310.06825},
  year    = {2023}
}

@article{radford2019language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume  = {1},
  number  = {8},
  pages   = {9},
  year    = {2019}
}

@online{sentencetransformers,
  author = {Hugging Face},
  title  = {Model: sentence-transformers/all-MiniLM-L12-v2},
  year   = {2024},
  url    = {https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2},
  note   = {Available: Hugging Face [Online]. Accessed: April 24, 2024}
}


@online{merveturkishinstructions,
  author = {Hugging Face},
  title  = {Dataset: merve/turkish\_instructions},
  year   = {2024},
  url    = {https://huggingface.co/datasets/merve/turkish_instructions?row=1},
  note   = {Available: Hugging Face [Online]. Accessed: April 24, 2024}
}


@misc{alpaca,
  author       = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title        = {Stanford Alpaca: An Instruction-following LLaMA model},
  year         = {2023},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford\_alpaca}}
}

@misc{li2023bactrianx,
  title         = {Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation},
  author        = {Haonan Li and Fajri Koto and Minghao Wu and Alham Fikri Aji and Timothy Baldwin},
  year          = {2023},
  eprint        = {2305.15011},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{team2023gemini,
  title   = {Gemini: a family of highly capable multimodal models},
  author  = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal = {arXiv preprint arXiv:2312.11805},
  year    = {2023}
}

@article{turker2024vbart,
  title   = {VBART: The Turkish LLM},
  author  = {Turker, Meliksah and Ari, Mehmet Erdi and Han, Aydin},
  journal = {arXiv preprint arXiv:2403.01308},
  year    = {2024}
}

@online{Trendyol,
  author = {Trendyol},
  title  = {Trendyol Model at Hugging Face},
  year   = {2024},
  url    = {https://huggingface.co/Trendyol},
  note   = {Available: Hugging Face [Online]. Accessed: April 25, 2024}
}


@misc{Sambalingo,
  title         = {SambaLingo: Teaching Large Language Models New Languages},
  author        = {Zoltan Csaki and Bo Li and Jonathan Li and Qiantong Xu and Pian Pawakapan and Leon Zhang and Yun Du and Hengyu Zhao and Changran Hu and Urmish Thakker},
  year          = {2024},
  eprint        = {2404.05829},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{wang2023openchat,
  title   = {OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},
  author  = {Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal = {arXiv preprint arXiv:2309.11235},
  year    = {2023}
}

@online{googlegemma2b,
  author = {Google},
  title  = {GEMMA-2B-IT Model at Hugging Face},
  year   = {2024},
  url    = {https://huggingface.co/google/gemma-2b-it},
  note   = {Available: Hugging Face [Online]. Accessed: April 25, 2024}
}


@online{googlegemma7b,
  author = {Google},
  title  = {GEMMA-7B-IT Model},
  year   = {2024},
  url    = {https://huggingface.co/google/gemma-7b-it},
  note   = {Available: Hugging Face [Online]. Accessed: April 25, 2024}
}
@inproceedings{safaya-etal-2022-mukayese,
  title     = {Mukayese: {T}urkish {NLP} Strikes Back},
  author    = {Safaya, Ali  and
               Kurtulu{\c{s}}, Emirhan  and
               Goktogan, Arda  and
               Yuret, Deniz},
  editor    = {Muresan, Smaranda  and
               Nakov, Preslav  and
               Villavicencio, Aline},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
  month     = may,
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.findings-acl.69},
  doi       = {10.18653/v1/2022.findings-acl.69},
  pages     = {846--863},
  abstract  = {Having sufficient resources for language X lifts it from the under-resourced languages class, but not necessarily from the under-researched class. In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. We demonstrate that languages such as Turkish are left behind the state-of-the-art in NLP applications. As a solution, we present Mukayese, a set of NLP benchmarks for the Turkish language that contains several NLP tasks. We work on one or more datasets for each benchmark and present two or more baselines. Moreover, we present four new benchmarking datasets in Turkish for language modeling, sentence segmentation, and spell checking. All datasets and baselines are available under: \url{https://github.com/alisafaya/mukayese}}
}
@article{toprak2023developing,
  title   = {Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models},
  author  = {Toprak Kesgin, Himmet and Yuce, Muzaffer Kaan and Amasyali, Mehmet Fatih},
  journal = {arXiv e-prints},
  pages   = {arXiv--2307},
  year    = {2023}
}
@article{rafailov2024direct,
  title   = {Direct preference optimization: Your language model is secretly a reward model},
  author  = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2024}
}

@article{touvron2023llama2,
  title   = {Llama 2: Open foundation and fine-tuned chat models},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}

@online{OpenLLMTurkishLeaderboard2024,
  author = {malhajar},
  title  = {OpenLLM Turkish leaderboard},
  year   = {2024},
  url    = {https://huggingface.co/spaces/malhajar/OpenLLMTurkishLeaderboard},
  note   = {Available: Hugging Face Spaces [Online]. Accessed: April 26, 2024}
}
