% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}
@article{liu2024dora,
  title={DoRA: Weight-Decomposed Low-Rank Adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}

@article{meng2024periodiclora,
  title={PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization},
  author={Meng, Xiangdi and Dai, Damai and Luo, Weiyao and Yang, Zhe and Wu, Shaoxiang and Wang, Xiaochen and Wang, Peiyi and Dong, Qingxiu and Chen, Liang and Sui, Zhifang},
  journal={arXiv preprint arXiv:2402.16141},
  year={2024}
}

@article{zhu2024asymmetry,
  title={Asymmetry in Low-Rank Adapters of Foundation Models},
  author={Zhu, Jiacheng and Greenewald, Kristjan and Nadjahi, Kimia and Borde, Haitz S{\'a}ez de Oc{\'a}riz and Gabrielsson, Rickard Br{\"u}el and Choshen, Leshem and Ghassemi, Marzyeh and Yurochkin, Mikhail and Solomon, Justin},
  journal={arXiv preprint arXiv:2402.16842},
  year={2024}
}
@article{wang2024far,
  title={How far can camels go? exploring the state of instruction tuning on open resources},
  author={Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and Hessel, Jack and Khot, Tushar and Chandu, Khyathi and Wadden, David and MacMillan, Kelsey and Smith, Noah A and Beltagy, Iz and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{shi2024reslora,
  title={ResLoRA: Identity Residual Mapping in Low-Rank Adaption},
  author={Shi, Shuhua and Huang, Shaohan and Song, Minghui and Li, Zhoujun and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi},
  journal={arXiv preprint arXiv:2402.18039},
  year={2024}
}

@article{kopiczko2023vera,
  title={Vera: Vector-based random matrix adaptation},
  author={Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki Markus},
  journal={arXiv preprint arXiv:2310.11454},
  year={2023}
}

@article{renduchintala2023tied,
  title={Tied-lora: Enhacing parameter efficiency of lora with weight tying},
  author={Renduchintala, Adithya and Konuk, Tugrul and Kuchaiev, Oleksii},
  journal={arXiv preprint arXiv:2311.09578},
  year={2023}
}

@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{lialin2023stack,
  title={Stack more layers differently: High-rank training through low-rank updates},
  author={Lialin, Vladislav and Shivagunde, Namrata and Muckatira, Sherin and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2307.05695},
  year={2023}
}
@article{xia2024chain,
  title={Chain of lora: Efficient fine-tuning of language models via residual learning},
  author={Xia, Wenhan and Qin, Chengwei and Hazan, Elad},
  journal={arXiv preprint arXiv:2401.04151},
  year={2024}
}

@article{Hayou2024,
archivePrefix = {arXiv},
arxivId = {2402.12354},
author = {Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
eprint = {2402.12354},
file = {:Users/royokong/Documents/MendeleyDesktop/LoRA Efficient Low Rank Adaptation of Large Models.pdf:pdf},
title = {{LoRA+: Efficient Low Rank Adaptation of Large Models}},
url = {http://arxiv.org/abs/2402.12354},
volume = {3},
year = {2024}
}

@inproceedings{zhang2022adaptive,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{collins2023evaluating,
  title={Evaluating language models for mathematics through interactions},
  author={Collins, Katherine M and Jiang, Albert Q and Frieder, Simon and Wong, Lionel and Zilka, Miri and Bhatt, Umang and Lukasiewicz, Thomas and Wu, Yuhuai and Tenenbaum, Joshua B and Hart, William and others},
  journal={arXiv preprint arXiv:2306.01694},
  year={2023}
}
@article{imani2023mathprompter,
  title={Mathprompter: Mathematical reasoning using large language models},
  author={Imani, Shima and Du, Liang and Shrivastava, Harsh},
  journal={arXiv preprint arXiv:2303.05398},
  year={2023}
}
@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}
@inproceedings{fu2023specializing,
  title={Specializing smaller language models towards multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar},
  booktitle={International Conference on Machine Learning},
  pages={10421--10430},
  year={2023},
  organization={PMLR}
}

@article{chen2023disc,
  title={DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning},
  author={Chen, Wei and Wang, Qiushi and Long, Zefei and Zhang, Xianyin and Lu, Zhongtian and Li, Bingxuan and Wang, Siyuan and Xu, Jiarong and Bai, Xiang and Huang, Xuanjing and others},
  journal={arXiv preprint arXiv:2310.15205},
  year={2023}
}

@article{cheng2023adapting,
  title={Adapting large language models via reading comprehension},
  author={Cheng, Daixuan and Huang, Shaohan and Wei, Furu},
  journal={arXiv preprint arXiv:2309.09530},
  year={2023}
}

@article{han2023medalpaca,
  title={MedAlpaca--An Open-Source Collection of Medical Conversational AI Models and Training Data},
  author={Han, Tianyu and Adams, Lisa C and Papaioannou, Jens-Michalis and Grundmann, Paul and Oberhauser, Tom and L{\"o}ser, Alexander and Truhn, Daniel and Bressem, Keno K},
  journal={arXiv preprint arXiv:2304.08247},
  year={2023}
}

@article{liu2023chipnemo,
  title={ChipNeMo: Domain-Adapted LLMs for Chip Design},
  author={Liu, Mingjie and Ene, Teodor-Dumitru and Kirby, Robert and Cheng, Chris and Pinckney, Nathaniel and Liang, Rongjian and Alben, Jonah and Anand, Himyanshu and Banerjee, Sanmitra and Bayraktaroglu, Ismet and others},
  journal={arXiv preprint arXiv:2311.00176},
  year={2023}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{ivison2023camels,
  title={Camels in a changing climate: Enhancing lm adaptation with tulu 2},
  author={Ivison, Hamish and Wang, Yizhong and Pyatkin, Valentina and Lambert, Nathan and Peters, Matthew and Dasigi, Pradeep and Jang, Joel and Wadden, David and Smith, Noah A and Beltagy, Iz and others},
  journal={arXiv preprint arXiv:2311.10702},
  year={2023}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}
@inproceedings{jin2019pubmedqa,
  title={PubMedQA: A Dataset for Biomedical Research Question Answering},
  author={Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William and Lu, Xinghua},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2567--2577},
  year={2019}
}
@article{dernoncourt2017pubmed,
  title={Pubmed 200k rct: a dataset for sequential sentence classification in medical abstracts},
  author={Dernoncourt, Franck and Lee, Ji Young},
  journal={arXiv preprint arXiv:1710.06071},
  year={2017}
}

@article{jin2021disease,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={Applied Sciences},
  volume={11},
  number={14},
  pages={6421},
  year={2021},
  publisher={MDPI}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@inproceedings{maia201818,
  title={Www'18 open challenge: financial opinion mining and question answering},
  author={Maia, Macedo and Handschuh, Siegfried and Freitas, Andr{\'e} and Davis, Brian and McDermott, Ross and Zarrouk, Manel and Balahur, Alexandra},
  booktitle={Companion proceedings of the the web conference 2018},
  pages={1941--1942},
  year={2018}
}

@inproceedings{sinha2021impact,
  title={Impact of news on the commodity market: Dataset and results},
  author={Sinha, Ankur and Khandait, Tanmay},
  booktitle={Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2},
  pages={589--601},
  year={2021},
  organization={Springer}
}

@inproceedings{salinas-alvarado-etal-2015-domain,
    title = "Domain Adaption of Named Entity Recognition to Support Credit Risk Assessment",
    author = "Salinas Alvarado, Julio Cesar  and
      Verspoor, Karin  and
      Baldwin, Timothy",
    editor = "Hachey, Ben  and
      Webster, Kellie",
    booktitle = "Proceedings of the Australasian Language Technology Association Workshop 2015",
    month = dec,
    year = "2015",
    address = "Parramatta, Australia",
    url = "https://aclanthology.org/U15-1010",
    pages = "84--90",
}

@article{chen2022convfinqa,
  title={Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering},
  author={Chen, Zhiyu and Li, Shiyang and Smiley, Charese and Ma, Zhiqiang and Shah, Sameena and Wang, William Yang},
  journal={arXiv preprint arXiv:2210.03849},
  year={2022}
}

@article{malo2014good,
  title={Good debt or bad debt: Detecting semantic orientations in economic texts},
  author={Malo, Pekka and Sinha, Ankur and Korhonen, Pekka and Wallenius, Jyrki and Takala, Pyry},
  journal={Journal of the Association for Information Science and Technology},
  volume={65},
  number={4},
  pages={782--796},
  year={2014},
  publisher={Wiley Online Library}
}

@article{wu2023bloomberggpt,
  title={Bloomberggpt: A large language model for finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}
