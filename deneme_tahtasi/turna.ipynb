{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"boun-tabi-LMG/TURNA\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"boun-tabi-LMG/TURNA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "/Users/mab/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user',\n",
       "    'content': 'Bu caddenin taşlarının üzerinden yüzyıllar boyunca\\ngeçen insanların ayak seslerini takip edenler, bir kalenin\\noya gibi işlenmiş kapısı önünde bulurlar izlerini.\\n Bu cümledeki ögelerin dizilişi aşağıdakilerin\\nhangisinde sırasıyla verilmiştir?\\nÖzne - yüklem - belirtili nesne Zarf tümleci - özne - belirtili nesne - yüklem Özne - yer tamlayıcısı - yüklem - belirtili nesne Zarf tümleci - belirtili nesne - yüklem - özne Belirtili nesne - yer tamlayıcısı - yüklem - özne'},\n",
       "   {'role': 'assistant', 'content': ''}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "  {'role': 'user', 'content': '''Bu caddenin taşlarının üzerinden yüzyıllar boyunca\n",
    "geçen insanların ayak seslerini takip edenler, bir kalenin\n",
    "oya gibi işlenmiş kapısı önünde bulurlar izlerini.\n",
    " Bu cümledeki ögelerin dizilişi aşağıdakilerin\n",
    "hangisinde sırasıyla verilmiştir?\n",
    "Özne - yüklem - belirtili nesne Zarf tümleci - özne - belirtili nesne - yüklem Özne - yer tamlayıcısı - yüklem - belirtili nesne Zarf tümleci - belirtili nesne - yüklem - özne Belirtili nesne - yer tamlayıcısı - yüklem - özne'''},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user',\n",
       "    'content': '“Yazdıkların kime hitap ediyor?” sorusuna verilmiş net\\nbir cevabım yok. Bir iyelik ekiyle “okurlarım” demeyi de\\ndoğrusu beni hiç okumamış olanlara bir saygısızlık\\nolarak değerlendiriyorum. Ancak yine de <altı çizili söz>boşluğa\\nyazdığımı söyleyemiyorum.</altı çizili söz>\\n Bu parçanın yazarı, altı çizili sözle hangi özelliğine\\nvurgu yapmaktadır?\\nEserlerini zihninde tasarladığı bir kitleye yönelik olarak\\nürettiğine\\nEserleriyle her düzeyde okur kitlesine seslenmeyi\\nöncelediğine\\nSeçtiği temalarla mevcut ve potansiyel okurlarını\\nayrıştırdığına\\nSahiplendiği okurların duyarlılığını eserleriyle\\ngeliştirmeye çalıştığına\\nYazılarıyla bütün okurların beğenisini kazanmayı\\namaçladığına'},\n",
       "   {'role': 'assistant', 'content': ''}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "  {'role': 'user', 'content': '''“Yazdıkların kime hitap ediyor?” sorusuna verilmiş net\n",
    "bir cevabım yok. Bir iyelik ekiyle “okurlarım” demeyi de\n",
    "doğrusu beni hiç okumamış olanlara bir saygısızlık\n",
    "olarak değerlendiriyorum. Ancak yine de <altı çizili söz>boşluğa\n",
    "yazdığımı söyleyemiyorum.</altı çizili söz>\n",
    " Bu parçanın yazarı, altı çizili sözle hangi özelliğine\n",
    "vurgu yapmaktadır?\n",
    "Eserlerini zihninde tasarladığı bir kitleye yönelik olarak\n",
    "ürettiğine\n",
    "Eserleriyle her düzeyde okur kitlesine seslenmeyi\n",
    "öncelediğine\n",
    "Seçtiği temalarla mevcut ve potansiyel okurlarını\n",
    "ayrıştırdığına\n",
    "Sahiplendiği okurların duyarlılığını eserleriyle\n",
    "geliştirmeye çalıştığına\n",
    "Yazılarıyla bütün okurların beğenisini kazanmayı\n",
    "amaçladığına'''},\n",
    "]\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Nasılsın?sın Sen   Yasın Sen '}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Nasılsın?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
