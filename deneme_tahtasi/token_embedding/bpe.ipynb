{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibayram/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ca4a3ca0b24653bd8f783f5f275739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4168c7ee3447c989ae98ea75939644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484bebde8bef4477ae28326ffe2e08c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d75fcef93cd4b398d1f620422d6d9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448e08f1bceb40208d3d2e6c9348d0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course.': 1, 'chapter': 1, 'about': 1, 'tokenization.': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms.': 1, 'Hopefully,': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens.': 1}\n"
     ]
    }
   ],
   "source": [
    "kelime_frekanslari = {}\n",
    "\n",
    "for cumle in corpus:\n",
    "    for kelime in cumle.split():\n",
    "        kelime_frekanslari[kelime] = kelime_frekanslari.get(kelime, 0) + 1\n",
    "\n",
    "print(kelime_frekanslari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('Ġ', 'i'): 2\n",
      "('Ġ', 't'): 7\n",
      "('t', 'h'): 3\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ġ', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n",
    "vocab.append(\"Ġt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
    "print(splits[\"Ġtrained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok', ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe', ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni', ('Ġtokeni', 'z'): 'Ġtokeniz', ('a', 't'): 'at', ('i', 'o'): 'io', ('io', 'n'): 'ion', ('Ġ', 'se'): 'Ġse', ('h', 'o'): 'ho', ('ho', 'w'): 'how', ('l', 'l'): 'll', ('Ġ', 'H'): 'ĠH', ('ĠH', 'u'): 'ĠHu', ('ĠHu', 'g'): 'ĠHug', ('ĠHug', 'g'): 'ĠHugg', ('ĠHugg', 'in'): 'ĠHuggin', ('ĠHuggin', 'g'): 'ĠHugging', ('Ġ', 'F'): 'ĠF', ('ĠF', 'a'): 'ĠFa', ('ĠFa', 'c'): 'ĠFac', ('ĠFac', 'e'): 'ĠFace', ('Ġ', 'C'): 'ĠC', ('ĠC', 'ou'): 'ĠCou', ('ĠCou', 'r'): 'ĠCour', ('ĠCour', 'se'): 'ĠCourse', ('Ġ', 'c'): 'Ġc', ('Ġc', 'h'): 'Ġch', ('Ġch', 'a'): 'Ġcha', ('Ġcha', 'p'): 'Ġchap', ('Ġchap', 't'): 'Ġchapt', ('Ġchapt', 'er'): 'Ġchapter', ('Ġab', 'ou'): 'Ġabou', ('Ġabou', 't'): 'Ġabout', ('Ġtokeniz', 'at'): 'Ġtokenizat', ('Ġtokenizat', 'ion'): 'Ġtokenization', ('Ġse', 'c'): 'Ġsec', ('Ġsec', 't'): 'Ġsect', ('Ġsect', 'ion'): 'Ġsection', ('Ġ', 's'): 'Ġs', ('Ġs', 'how'): 'Ġshow', ('Ġshow', 's'): 'Ġshows', ('Ġse', 'v'): 'Ġsev', ('Ġsev', 'er'): 'Ġsever', ('Ġsever', 'a'): 'Ġsevera', ('Ġsevera', 'l'): 'Ġseveral', ('Ġtokeniz', 'er'): 'Ġtokenizer', ('Ġa', 'l'): 'Ġal', ('Ġal', 'g'): 'Ġalg', ('Ġalg', 'o'): 'Ġalgo', ('Ġalgo', 'r'): 'Ġalgor', ('Ġalgor', 'i'): 'Ġalgori', ('Ġalgori', 't'): 'Ġalgorit', ('Ġalgorit', 'h'): 'Ġalgorith', ('Ġalgorith', 'm'): 'Ġalgorithm', ('Ġalgorithm', 's'): 'Ġalgorithms', ('H', 'o'): 'Ho', ('Ho', 'p'): 'Hop', ('Hop', 'e'): 'Hope', ('Hope', 'f'): 'Hopef', ('Hopef', 'u'): 'Hopefu', ('Hopefu', 'll'): 'Hopefull', ('Hopefull', 'y'): 'Hopefully', ('Ġ', 'y'): 'Ġy', ('Ġy', 'ou'): 'Ġyou', ('Ġ', 'w'): 'Ġw', ('Ġw', 'i'): 'Ġwi', ('Ġwi', 'll'): 'Ġwill', ('Ġ', 'b'): 'Ġb', ('Ġb', 'e'): 'Ġbe', ('Ġab', 'l'): 'Ġabl', ('Ġabl', 'e'): 'Ġable', ('Ġ', 'u'): 'Ġu', ('Ġu', 'nd'): 'Ġund', ('Ġund', 'er'): 'Ġunder', ('Ġunder', 's'): 'Ġunders', ('Ġunders', 't'): 'Ġunderst', ('Ġunderst', 'a'): 'Ġundersta', ('Ġundersta', 'nd'): 'Ġunderstand', ('Ġ', 'how'): 'Ġhow', ('Ġthe', 'y'): 'Ġthey', ('Ġa', 'r'): 'Ġar', ('Ġar', 'e'): 'Ġare', ('Ġt', 'r'): 'Ġtr', ('Ġtr', 'a'): 'Ġtra', ('Ġtra', 'in'): 'Ġtrain', ('Ġtrain', 'e'): 'Ġtraine', ('Ġtraine', 'd'): 'Ġtrained', ('Ġa', 'nd'): 'Ġand', ('Ġ', 'g'): 'Ġg', ('Ġg', 'en'): 'Ġgen', ('Ġgen', 'er'): 'Ġgener', ('Ġgener', 'at'): 'Ġgenerat', ('Ġgenerat', 'e'): 'Ġgenerate', ('Ġtoken', 's'): 'Ġtokens'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se', 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni', 'Ġtokeniz', 'at', 'io', 'ion', 'Ġse', 'ho', 'how', 'll', 'ĠH', 'ĠHu', 'ĠHug', 'ĠHugg', 'ĠHuggin', 'ĠHugging', 'ĠF', 'ĠFa', 'ĠFac', 'ĠFace', 'ĠC', 'ĠCou', 'ĠCour', 'ĠCourse', 'Ġc', 'Ġch', 'Ġcha', 'Ġchap', 'Ġchapt', 'Ġchapter', 'Ġabou', 'Ġabout', 'Ġtokenizat', 'Ġtokenization', 'Ġsec', 'Ġsect', 'Ġsection', 'Ġs', 'Ġshow', 'Ġshows', 'Ġsev', 'Ġsever', 'Ġsevera', 'Ġseveral', 'Ġtokenizer', 'Ġal', 'Ġalg', 'Ġalgo', 'Ġalgor', 'Ġalgori', 'Ġalgorit', 'Ġalgorith', 'Ġalgorithm', 'Ġalgorithms', 'Ho', 'Hop', 'Hope', 'Hopef', 'Hopefu', 'Hopefull', 'Hopefully', 'Ġy', 'Ġyou', 'Ġw', 'Ġwi', 'Ġwill', 'Ġb', 'Ġbe', 'Ġabl', 'Ġable', 'Ġu', 'Ġund', 'Ġunder', 'Ġunders', 'Ġunderst', 'Ġundersta', 'Ġunderstand', 'Ġhow', 'Ġthey', 'Ġar', 'Ġare', 'Ġtr', 'Ġtra', 'Ġtrain', 'Ġtraine', 'Ġtrained', 'Ġand', 'Ġg', 'Ġgen', 'Ġgener', 'Ġgenerat', 'Ġgenerate', 'Ġtokens']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Ahmet', (0, 5)), ('Ġnasilsin', (5, 14)), ('?', (14, 15))],\n",
       " ['Ahmet', 'Ġnasilsin', '?'],\n",
       " {('Ġ', 't'): 'Ġt',\n",
       "  ('i', 's'): 'is',\n",
       "  ('e', 'r'): 'er',\n",
       "  ('Ġ', 'a'): 'Ġa',\n",
       "  ('Ġt', 'o'): 'Ġto',\n",
       "  ('e', 'n'): 'en',\n",
       "  ('T', 'h'): 'Th',\n",
       "  ('Th', 'is'): 'This',\n",
       "  ('o', 'u'): 'ou',\n",
       "  ('s', 'e'): 'se',\n",
       "  ('Ġto', 'k'): 'Ġtok',\n",
       "  ('Ġtok', 'en'): 'Ġtoken',\n",
       "  ('n', 'd'): 'nd',\n",
       "  ('Ġ', 'is'): 'Ġis',\n",
       "  ('Ġt', 'h'): 'Ġth',\n",
       "  ('Ġth', 'e'): 'Ġthe',\n",
       "  ('i', 'n'): 'in',\n",
       "  ('Ġa', 'b'): 'Ġab',\n",
       "  ('Ġtoken', 'i'): 'Ġtokeni',\n",
       "  ('Ġtokeni', 'z'): 'Ġtokeniz',\n",
       "  ('a', 't'): 'at',\n",
       "  ('i', 'o'): 'io',\n",
       "  ('io', 'n'): 'ion',\n",
       "  ('Ġ', 'se'): 'Ġse',\n",
       "  ('h', 'o'): 'ho',\n",
       "  ('ho', 'w'): 'how',\n",
       "  ('l', 'l'): 'll',\n",
       "  ('Ġ', 'H'): 'ĠH',\n",
       "  ('ĠH', 'u'): 'ĠHu',\n",
       "  ('ĠHu', 'g'): 'ĠHug',\n",
       "  ('ĠHug', 'g'): 'ĠHugg',\n",
       "  ('ĠHugg', 'in'): 'ĠHuggin',\n",
       "  ('ĠHuggin', 'g'): 'ĠHugging',\n",
       "  ('Ġ', 'F'): 'ĠF',\n",
       "  ('ĠF', 'a'): 'ĠFa',\n",
       "  ('ĠFa', 'c'): 'ĠFac',\n",
       "  ('ĠFac', 'e'): 'ĠFace',\n",
       "  ('Ġ', 'C'): 'ĠC',\n",
       "  ('ĠC', 'ou'): 'ĠCou',\n",
       "  ('ĠCou', 'r'): 'ĠCour',\n",
       "  ('ĠCour', 'se'): 'ĠCourse',\n",
       "  ('Ġ', 'c'): 'Ġc',\n",
       "  ('Ġc', 'h'): 'Ġch',\n",
       "  ('Ġch', 'a'): 'Ġcha',\n",
       "  ('Ġcha', 'p'): 'Ġchap',\n",
       "  ('Ġchap', 't'): 'Ġchapt',\n",
       "  ('Ġchapt', 'er'): 'Ġchapter',\n",
       "  ('Ġab', 'ou'): 'Ġabou',\n",
       "  ('Ġabou', 't'): 'Ġabout',\n",
       "  ('Ġtokeniz', 'at'): 'Ġtokenizat',\n",
       "  ('Ġtokenizat', 'ion'): 'Ġtokenization',\n",
       "  ('Ġse', 'c'): 'Ġsec',\n",
       "  ('Ġsec', 't'): 'Ġsect',\n",
       "  ('Ġsect', 'ion'): 'Ġsection',\n",
       "  ('Ġ', 's'): 'Ġs',\n",
       "  ('Ġs', 'how'): 'Ġshow',\n",
       "  ('Ġshow', 's'): 'Ġshows',\n",
       "  ('Ġse', 'v'): 'Ġsev',\n",
       "  ('Ġsev', 'er'): 'Ġsever',\n",
       "  ('Ġsever', 'a'): 'Ġsevera',\n",
       "  ('Ġsevera', 'l'): 'Ġseveral',\n",
       "  ('Ġtokeniz', 'er'): 'Ġtokenizer',\n",
       "  ('Ġa', 'l'): 'Ġal',\n",
       "  ('Ġal', 'g'): 'Ġalg',\n",
       "  ('Ġalg', 'o'): 'Ġalgo',\n",
       "  ('Ġalgo', 'r'): 'Ġalgor',\n",
       "  ('Ġalgor', 'i'): 'Ġalgori',\n",
       "  ('Ġalgori', 't'): 'Ġalgorit',\n",
       "  ('Ġalgorit', 'h'): 'Ġalgorith',\n",
       "  ('Ġalgorith', 'm'): 'Ġalgorithm',\n",
       "  ('Ġalgorithm', 's'): 'Ġalgorithms',\n",
       "  ('H', 'o'): 'Ho',\n",
       "  ('Ho', 'p'): 'Hop',\n",
       "  ('Hop', 'e'): 'Hope',\n",
       "  ('Hope', 'f'): 'Hopef',\n",
       "  ('Hopef', 'u'): 'Hopefu',\n",
       "  ('Hopefu', 'll'): 'Hopefull',\n",
       "  ('Hopefull', 'y'): 'Hopefully',\n",
       "  ('Ġ', 'y'): 'Ġy',\n",
       "  ('Ġy', 'ou'): 'Ġyou',\n",
       "  ('Ġ', 'w'): 'Ġw',\n",
       "  ('Ġw', 'i'): 'Ġwi',\n",
       "  ('Ġwi', 'll'): 'Ġwill',\n",
       "  ('Ġ', 'b'): 'Ġb',\n",
       "  ('Ġb', 'e'): 'Ġbe',\n",
       "  ('Ġab', 'l'): 'Ġabl',\n",
       "  ('Ġabl', 'e'): 'Ġable',\n",
       "  ('Ġ', 'u'): 'Ġu',\n",
       "  ('Ġu', 'nd'): 'Ġund',\n",
       "  ('Ġund', 'er'): 'Ġunder',\n",
       "  ('Ġunder', 's'): 'Ġunders',\n",
       "  ('Ġunders', 't'): 'Ġunderst',\n",
       "  ('Ġunderst', 'a'): 'Ġundersta',\n",
       "  ('Ġundersta', 'nd'): 'Ġunderstand',\n",
       "  ('Ġ', 'how'): 'Ġhow',\n",
       "  ('Ġthe', 'y'): 'Ġthey',\n",
       "  ('Ġa', 'r'): 'Ġar',\n",
       "  ('Ġar', 'e'): 'Ġare',\n",
       "  ('Ġt', 'r'): 'Ġtr',\n",
       "  ('Ġtr', 'a'): 'Ġtra',\n",
       "  ('Ġtra', 'in'): 'Ġtrain',\n",
       "  ('Ġtrain', 'e'): 'Ġtraine',\n",
       "  ('Ġtraine', 'd'): 'Ġtrained',\n",
       "  ('Ġa', 'nd'): 'Ġand',\n",
       "  ('Ġ', 'g'): 'Ġg',\n",
       "  ('Ġg', 'en'): 'Ġgen',\n",
       "  ('Ġgen', 'er'): 'Ġgener',\n",
       "  ('Ġgener', 'at'): 'Ġgenerat',\n",
       "  ('Ġgenerat', 'e'): 'Ġgenerate',\n",
       "  ('Ġtoken', 's'): 'Ġtokens'},\n",
       " {'This': ['This'],\n",
       "  'Ġis': ['Ġis'],\n",
       "  'Ġthe': ['Ġthe'],\n",
       "  'ĠHugging': ['ĠHugging'],\n",
       "  'ĠFace': ['ĠFace'],\n",
       "  'ĠCourse': ['ĠCourse'],\n",
       "  '.': ['.'],\n",
       "  'Ġchapter': ['Ġchapter'],\n",
       "  'Ġabout': ['Ġabout'],\n",
       "  'Ġtokenization': ['Ġtokenization'],\n",
       "  'Ġsection': ['Ġsection'],\n",
       "  'Ġshows': ['Ġshows'],\n",
       "  'Ġseveral': ['Ġseveral'],\n",
       "  'Ġtokenizer': ['Ġtokenizer'],\n",
       "  'Ġalgorithms': ['Ġalgorithms'],\n",
       "  'Hopefully': ['Hopefully'],\n",
       "  ',': [','],\n",
       "  'Ġyou': ['Ġyou'],\n",
       "  'Ġwill': ['Ġwill'],\n",
       "  'Ġbe': ['Ġbe'],\n",
       "  'Ġable': ['Ġable'],\n",
       "  'Ġto': ['Ġto'],\n",
       "  'Ġunderstand': ['Ġunderstand'],\n",
       "  'Ġhow': ['Ġhow'],\n",
       "  'Ġthey': ['Ġthey'],\n",
       "  'Ġare': ['Ġare'],\n",
       "  'Ġtrained': ['Ġtrained'],\n",
       "  'Ġand': ['Ġand'],\n",
       "  'Ġgenerate': ['Ġgenerate'],\n",
       "  'Ġtokens': ['Ġtokens']})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(\"Ahmet nasilsin?\")\n",
    "pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "\n",
    "pre_tokenize_result, pre_tokenized_text, merges, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'a', 's', 'i', 'l', 's', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"nasilsin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "Ġis\n",
      "Ġthe\n",
      "ĠHugging\n",
      "ĠFace\n",
      "ĠCourse\n",
      ".\n",
      "Ġchapter\n",
      "Ġabout\n",
      "Ġtokenization\n",
      "Ġsection\n",
      "Ġshows\n",
      "Ġseveral\n",
      "Ġtokenizer\n",
      "Ġalgorithms\n",
      "Hopefully\n",
      ",\n",
      "Ġyou\n",
      "Ġwill\n",
      "Ġbe\n",
      "Ġable\n",
      "Ġto\n",
      "Ġunderstand\n",
      "Ġhow\n",
      "Ġthey\n",
      "Ġare\n",
      "Ġtrained\n",
      "Ġand\n",
      "Ġgenerate\n",
      "Ġtokens\n"
     ]
    }
   ],
   "source": [
    "for idx, split in enumerate(splits):\n",
    "  print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "print(sum({'a': []}, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 0: ('n', 'o')\n",
      "Merge 200: ('i', 'l</w>')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m bpe_merges \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# BPE birleştirmelerini saklamak için\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_merges):\n\u001b[0;32m---> 37\u001b[0m     pairs \u001b[38;5;241m=\u001b[39m \u001b[43mget_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pairs:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[63], line 19\u001b[0m, in \u001b[0;36mget_stats\u001b[0;34m(vocab)\u001b[0m\n\u001b[1;32m     17\u001b[0m pairs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, freq \u001b[38;5;129;01min\u001b[39;00m vocab\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 19\u001b[0m     symbols \u001b[38;5;241m=\u001b[39m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(symbols) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     21\u001b[0m         pairs[symbols[i], symbols[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m freq\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "# Haber dosyasını oku\n",
    "with open(\"trmor2006.train\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    text = text.replace(\"+\", \" \")\n",
    "\n",
    "# Metni kelimelere böl\n",
    "words = re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Her kelimenin sonuna </w> ekleyin (kelimenin sonunu belirtmek için)\n",
    "vocab = collections.Counter([' '.join(word) + ' </w>' for word in words])\n",
    "\n",
    "# BPE işlemi için yardımcı işlevler\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in v_in:\n",
    "        w_out = re.sub(r'(?<!\\S)' + re.escape(bigram) + r'(?!\\S)', replacement, word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "# 32 bin kelimelik bir sözlük oluştur\n",
    "num_merges = 3200\n",
    "bpe_merges = []  # BPE birleştirmelerini saklamak için\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    bpe_merges.append(best)\n",
    "    if i % 200 == 0:\n",
    "        print(f'Merge {i}: {best}')\n",
    "\n",
    "# Token sözlüğünü oluştur\n",
    "bpe_tokens = set()\n",
    "for word in vocab:\n",
    "    bpe_tokens.update(word.split())\n",
    "\n",
    "print(f'\\nToplam Token Sayısı: {len(bpe_tokens)}')\n",
    "print('İlk 100 token:', list(bpe_tokens)[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode ve Decode işlevleri\n",
    "def encode(text, merges):\n",
    "    \"\"\"Verilen metni BPE tokenlarına dönüştürür\"\"\"\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    tokens = []\n",
    "\n",
    "    # Her kelimeyi BPE tokenlarına böl\n",
    "    for word in words:\n",
    "        word = list(word) + ['</w>']\n",
    "        while len(word) > 1:\n",
    "            pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
    "            pair_freq = {pair: merges.index(pair) for pair in pairs if pair in merges}\n",
    "            if not pair_freq:\n",
    "                break\n",
    "            best_pair = min(pair_freq, key=pair_freq.get)\n",
    "            i = pairs.index(best_pair)\n",
    "            word = word[:i] + [''.join(best_pair)] + word[i + 2:]\n",
    "        tokens.extend(word)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def decode(tokens):\n",
    "    \"\"\"BPE tokenlarından orijinal metne dönüştürür\"\"\"\n",
    "    words = []\n",
    "    current_word = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == '</w>':\n",
    "            words.append(''.join(current_word))\n",
    "            current_word = []\n",
    "        else:\n",
    "            current_word.append(token)\n",
    "\n",
    "    if current_word:\n",
    "        words.append(''.join(current_word))\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Örnek kullanım\n",
    "sample_text = \"Bu bir deneme metnidir\"\n",
    "encoded_tokens = encode(sample_text, bpe_merges)\n",
    "print(\"\\nEncoded Tokens:\", encoded_tokens)\n",
    "\n",
    "decoded_text = decode(encoded_tokens)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
