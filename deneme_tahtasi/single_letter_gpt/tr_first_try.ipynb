{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT modelinin parametre sayısını hesaplamak için aşağıdaki formüller kullanılır:\n",
    "\n",
    "1. **Embedding layer**: \\( vocab\\_size \\times n\\_embd \\)\n",
    "2. **Transformer block**: Her bir Transformer bloğunda toplam parametre sayısı şu şekildedir:\n",
    "   - **LayerNorm**: İki LayerNorm katmanı vardır, her biri \\( 2 \\times n\\_embd \\)\n",
    "   - **Attention (Q, K, V)**: \\( 3 \\times (n\\_embd \\times (n\\_embd // n\\_head) + n\\_embd) \\)\n",
    "   - **Attention output projection**: \\( n\\_embd \\times n\\_embd + n\\_embd \\)\n",
    "   - **MLP (intermediate dense layers)**: \\( 2 \\times (n\\_embd \\times 4 \\times n\\_embd + 4 \\times n\\_embd) \\)\n",
    "\n",
    "Bu formülleri kullanarak parametre sayısını hesaplayalım.\n",
    "\n",
    "### Hesaplamalar\n",
    "\n",
    "#### Embedding Layer\n",
    "\n",
    "\\[\n",
    "50257 \\times 768 = 38,609,856\n",
    "\\]\n",
    "\n",
    "#### Transformer Block\n",
    "\n",
    "Her bir Transformer bloğunda:\n",
    "\n",
    "- **LayerNorm**: \n",
    "  \\[\n",
    "  2 \\times 768 = 1,536\n",
    "  \\]\n",
    "\n",
    "- **Attention (Q, K, V)**:\n",
    "  \\[\n",
    "  3 \\times (768 \\times (768 // 12) + 768) = 3 \\times (768 \\times 64 + 768) = 3 \\times (49,152 + 768) = 3 \\times 49,920 = 149,760\n",
    "  \\]\n",
    "\n",
    "- **Attention output projection**:\n",
    "  \\[\n",
    "  768 \\times 768 + 768 = 590,592 + 768 = 591,360\n",
    "  \\]\n",
    "\n",
    "- **MLP**:\n",
    "  \\[\n",
    "  2 \\times (768 \\times 4 \\times 768 + 4 \\times 768) = 2 \\times (3,145,728 + 3,072) = 2 \\times 3,148,800 = 6,297,600\n",
    "  \\]\n",
    "\n",
    "Her bir Transformer bloğu için toplam:\n",
    "\\[\n",
    "1,536 + 149,760 + 591,360 + 6,297,600 = 7,040,256\n",
    "\\]\n",
    "\n",
    "#### Toplam Transformer Bloğu Parametreleri\n",
    "\\[\n",
    "12 \\times 7,040,256 = 84,483,072\n",
    "\\]\n",
    "\n",
    "#### Toplam Parametreler\n",
    "\\[\n",
    "38,609,856 (Embedding) + 84,483,072 (Transformer blokları) = 123,092,928\n",
    "\\]\n",
    "\n",
    "Bu hesaplamalar, yaklaşık 124 milyon parametreye oldukça yakındır. Modelin toplam parametre sayısının 124M olduğu, verilen konfigürasyon değerlerine göre bu şekilde hesaplanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import struct\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch._inductor.config as config\n",
    "import torch.nn as nn\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"Careful there are a few versions of GeLU, this one is the exact one used by OpenAI\"\"\"\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        if FLASH:\n",
    "            # flashattention\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            # this materializes the large (T,T) matrix for all the queries and keys\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = NewGELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1 # special flag for residual scaling.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" @dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768 \"\"\"\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 64\n",
    "    vocab_size: int = 32\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print0(*args, **kwargs):\n",
    "    # modified print that only prints from the master process\n",
    "    # if this is not a distributed run, it's just a print\n",
    "    if int(os.environ.get(\"RANK\", 0)) == 0:\n",
    "        print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # wte: word token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # wpe: positional embeddings\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # h: transformer blocks\n",
    "            ln_f = nn.LayerNorm(config.n_embd), # ln_f: final layer norm before output\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # lm_head: head for language modeling\n",
    "        self.lm_head.LLMC_SKIP_INIT = 1 # don't init this one, we will tie weights\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights, use a torch rng object to be very careful\n",
    "        self.init_rng = torch.Generator()\n",
    "        self.init_rng.manual_seed(42)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02 / math.sqrt(2 * self.config.n_layer)\n",
    "            # we want to skip initializing lm_head, which shares parameters with wte\n",
    "            # and wte was already initialized down below during the Embedding init\n",
    "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "\n",
    "    def forward(self, idx, targets=None, return_logits=True):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        # there are performance reasons why not returning logits is prudent, if not needed\n",
    "        if not return_logits:\n",
    "            logits = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type, zero_stage):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print0(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print0(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        print0(f\"using fused AdamW: {use_fused}\")\n",
    "        if zero_stage == 1:\n",
    "            print0(\"using ZeroRedundancyOptimizer\")\n",
    "            optimizer = ZeroRedundancyOptimizer(**optim_groups[0], optimizer_class=torch.optim.AdamW,\n",
    "                                                lr=learning_rate, betas=betas, fused=use_fused)\n",
    "            optimizer.add_param_group(optim_groups[1])\n",
    "        else:\n",
    "            print0(\"using regular AdamW\")\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peek_data_shard(filename):\n",
    "    # only reads the header, returns header data\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()\n",
    "        tokens = [x for x in tokens]\n",
    "    return len(tokens)\n",
    "\n",
    "def _load_data_shard(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()\n",
    "        tokens = [x for x in tokens]\n",
    "    return tokens\n",
    "\n",
    "class DistributedDataLoader:\n",
    "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # glob files that match the pattern\n",
    "        self.files = sorted(glob.glob(filename_pattern))\n",
    "        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n",
    "\n",
    "        # load and validate all data shards, count number of tokens in total\n",
    "        ntok_total = 0\n",
    "        for fname in self.files:\n",
    "            shard_ntok = _peek_data_shard(fname)\n",
    "            assert shard_ntok >= num_processes * B * T + 1, f\"dataset shard {fname} is too small for the current setting\"\n",
    "            ntok_total += shard_ntok\n",
    "        self.ntok_total = ntok_total\n",
    "        print0(f\"DataLoader: total number of tokens: {ntok_total:,} across {len(self.files)} files\")\n",
    "\n",
    "        # kick things off\n",
    "        self.current_shard = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # we're being a bit clever here: if we already had shard 0 loaded,\n",
    "        # then don't do the work to reload it, just reset the pointer\n",
    "        if self.current_shard != 0:\n",
    "            self.current_shard = 0\n",
    "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "\n",
    "    def advance(self): # advance to next data shard\n",
    "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        # buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n",
    "        buf = torch.tensor(buf, dtype=torch.long)\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the start pointer in current shard\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds advance the shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.advance()\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fp16(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.float16)\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)\n",
    "\n",
    "def write_fp32(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.float32)\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)\n",
    "\n",
    "def write_bf16(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.bfloat16)\n",
    "    # numpy doesn't have bf16 datatype so we have to trick it\n",
    "    t = t.view(torch.int16) # trick: reinterpret as int16\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tensors(model_tensors, L, file, dtype):\n",
    "    # writes the GPT-2 model's weights to a binary file\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"}\n",
    "    write_fun = write_fp16 if dtype == \"float16\" else write_fp32 if dtype == \"float32\" else write_bf16\n",
    "    write_fun(model_tensors[\"transformer.wte.weight\"], file) # (V, C)\n",
    "    write_fun(model_tensors[\"transformer.wpe.weight\"], file) # (T, C)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
    "    for i in range(L): # (L, 3C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
    "    for i in range(L): # (L, 3C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
    "    for i in range(L): # (L, C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
    "    for i in range(L): # (L, 4C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
    "    for i in range(L): # (L, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
    "    for i in range(L): # (L, C, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
    "    write_fun(model_tensors[\"transformer.ln_f.weight\"], file) # (C, )\n",
    "    write_fun(model_tensors[\"transformer.ln_f.bias\"], file) # (C, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pad_vocab(tensor, multiple=128, value=0):\n",
    "    \"\"\"\n",
    "    The dimension of the vocab size in GPT-2 is 50,257\n",
    "    which is unfortunately a very unfriendly number for a lot of\n",
    "    matrix operations on the GPU. So we pad it to the nearest\n",
    "    friendlier multiple, e.g. 50,304 if multiple=128 when we\n",
    "    export the weights into C land. This is a NOOP algorithmically\n",
    "    and is only done to make the tensor operations more efficient.\n",
    "    \"\"\"\n",
    "    assert tensor.ndim == 2\n",
    "    V, C = tensor.shape\n",
    "    # assert V == 50257, \"just being defensive here\"\n",
    "    # calculate padded vocab size by rounding up to nearest multiple\n",
    "    Vp = ((V + multiple - 1) // multiple) * multiple\n",
    "    # pad the tensor\n",
    "    pad_rows = Vp - V\n",
    "    padded = tensor if pad_rows == 0 else F.pad(tensor, (0, 0, 0, pad_rows), value=value)\n",
    "    assert padded.shape == (Vp, C)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(model, filename, dtype):\n",
    "    # everything we need to instantiate the model\n",
    "    # 1) header is: version int, GPTConfig ints, padding to 1024 bytes\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"} # float16 todo maybe later\n",
    "    version = {\n",
    "        \"float16\": 2, # 2: all tensors are fp16, padded vocab\n",
    "        \"float32\": 3, # 3: all tensors are fp32, padded vocab\n",
    "        \"bfloat16\": 5, # 5: all tensors are bf16, padded vocab\n",
    "    }[dtype]\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240326 # magic\n",
    "    header[1] = version # checkpoint version\n",
    "    header[2] = model.config.block_size\n",
    "    header[3] = model.config.vocab_size\n",
    "    header[4] = model.config.n_layer\n",
    "    header[5] = model.config.n_head\n",
    "    header[6] = model.config.n_embd\n",
    "    # 2) the parameters follow the header\n",
    "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
    "    # pad the vocab to a multiple of 128 here at export, for efficiency in C\n",
    "    wte = params[\"transformer.wte.weight\"] # (V, C)\n",
    "    wte_padded = pad_vocab(wte) # (Vp, C)\n",
    "    params[\"transformer.wte.weight\"] = wte_padded # (Vp, C)\n",
    "    print(f\"padded vocab size from {wte.size(0)} to {wte_padded.size(0)}\")\n",
    "    header[7] = wte_padded.size(0) # padded vocab size store in header\n",
    "    # now write to file\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes()) # header\n",
    "        write_tensors(params, model.config.n_layer, file, dtype) # params\n",
    "    print(f\"wrote {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_state(model, x, y, logits, loss, filename, dtype=\"float32\"):\n",
    "    # the state is used for debugging.\n",
    "    # it contains information about the input, logits, loss, and the parameter gradients\n",
    "    # this can be used for checking the computation correctness in C\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240327 # magic\n",
    "    header[1] = 2 # run state version = 2 (1 -> 2 for padded vocab changes)\n",
    "    header[2] = x.size(0) # batch size of the batch, B\n",
    "    header[3] = x.size(1) # temporal extent of the batch, T\n",
    "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
    "    # pad the vocab grads here as well, to mirror write_model\n",
    "    wte_grad = grads[\"transformer.wte.weight\"] # (V, C)\n",
    "    wte_grad_padded = pad_vocab(wte_grad, value=0) # (Vp, C) # TODO later maybe pad with nan?\n",
    "    grads[\"transformer.wte.weight\"] = wte_grad_padded # (Vp, C)\n",
    "    print(f\"padded vocab size in reference grads from {wte_grad.size(0)} to {wte_grad_padded.size(0)}\")\n",
    "    with open(filename, \"wb\") as file:\n",
    "        # header\n",
    "        file.write(header.numpy().tobytes())\n",
    "        # input x\n",
    "        file.write(x.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # targets y\n",
    "        file.write(y.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # logits (result of the model forward pass)\n",
    "        write_fp32(logits.cpu(), file)\n",
    "        # loss (single float, result of the cross entropy loss)\n",
    "        write_fp32(loss.cpu(), file)\n",
    "        # gradients\n",
    "        write_tensors(grads, model.config.n_layer, file, dtype)\n",
    "    print(f\"wrote {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokenizer(enc, filename):\n",
    "    n = enc.max_token_value + 1\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240328 # magic\n",
    "    header[1] = 2 # tokenizer version = 2 (1 -> 2: includes EOT token)\n",
    "    header[2] = n # number of tokens\n",
    "    header[3] = enc.eot_token # EOT token\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes())\n",
    "        for i in range(n):\n",
    "            b = enc.decode_bytes([i])\n",
    "            length = len(b)\n",
    "            assert length < 256, f\"Token length exceeds 255: {length}\"\n",
    "            file.write(struct.pack(\"<B\", length))  # Write the length as a 1-byte unsigned integer\n",
    "            file.write(b)  # Write the actual bytes\n",
    "    print(f\"wrote {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps (cpu)\n"
     ]
    }
   ],
   "source": [
    "# B, T = 2, 4\n",
    "\"\"\" \n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128\n",
    "    vocab_size: int = 64\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 16 \"\"\"\n",
    "\n",
    "B, T = 128, 32 # batch size, sequence length\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "print(f\"using device: {device} ({device_type})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp_rank = 0\n",
    "ddp_local_rank = 0\n",
    "zero_stage = 0\n",
    "ddp_world_size = 1\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "total_batch_size = B * T\n",
    "tokens_per_fwdbwd = B * T\n",
    "tokens_per_fwdbwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 4096\n",
      "=> calculated gradient accumulation steps: 1\n"
     ]
    }
   ],
   "source": [
    "assert total_batch_size % tokens_per_fwdbwd == 0\n",
    "grad_accum_steps = total_batch_size // tokens_per_fwdbwd\n",
    "print0(f\"total desired batch size: {total_batch_size}\")\n",
    "print0(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a context manager following the desired dtype and device\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16, 'float8': torch.float8_e4m3fn}['float16']\n",
    "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
    "# rng / reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(32, 144)\n",
       "    (wpe): Embedding(64, 144)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=144, out_features=432, bias=True)\n",
       "          (c_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=144, out_features=576, bias=True)\n",
       "          (gelu): NewGELU()\n",
       "          (c_proj): Linear(in_features=576, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=144, out_features=32, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "if False: # args.compile:\n",
    "    if hasattr(config, \"coordinate_descent_tuning\"):\n",
    "        config.coordinate_descent_tuning = True # suggested by @Chillee\n",
    "    print0(\"compiling the model...\")\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader: total number of tokens: 1,369,038 across 1 files\n",
      "DataLoader: total number of tokens: 171,129 across 1 files\n"
     ]
    }
   ],
   "source": [
    "train_loader = DistributedDataLoader(\"tokenizer/train_tokens_tr.bin\", B, T, ddp_rank, ddp_world_size)\n",
    "val_loader = DistributedDataLoader(\"tokenizer/val_tokens_tr.bin\", B, T, ddp_rank, ddp_world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 32]), torch.Size([128, 32]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = train_loader.next_batch()\n",
    "x, y = x.to(device), y.to(device)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded vocab size from 32 to 128\n",
      "wrote gpt2_12K.bin\n",
      "padded vocab size from 32 to 128\n",
      "wrote gpt2_12K_bf16.bin\n",
      "padded vocab size in reference grads from 32 to 128\n",
      "wrote gpt2_12K_debug_state.bin\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model(x, y)\n",
    "loss.backward()\n",
    "# save model params, in both float32 and bfloat16\n",
    "model_to_size = {\"gpt1Letter\": \"12K\",  \"gpt2\": \"124M\", \"gpt2-medium\": \"355M\", \"gpt2-large\": \"774M\", \"gpt2-xl\": \"1558M\"}\n",
    "model_to_size.update({f\"d{d}\": f\"d{d}\" for d in [12, 24, 36, 48]})\n",
    "model_size_str = model_to_size[\"gpt1Letter\"] # e.g. \"124M\", or \"d12\"\n",
    "write_model(model, f\"gpt2_{model_size_str}.bin\", dtype=\"float16\")\n",
    "write_model(model, f\"gpt2_{model_size_str}_bf16.bin\", dtype=\"bfloat16\")\n",
    "# save x, y, logits, loss, and parameter gradients, for debugging C\n",
    "# always store these in fp32 to have an accurate reference (?)\n",
    "write_state(model, x, y, logits, loss, f\"gpt2_{model_size_str}_debug_state.bin\")\n",
    "# reset the train_loader for the optimization below\n",
    "train_loader.reset()\n",
    "# clear the grads here explicitly because otherwise we'd have a duplicate grad accumulation\n",
    "# since in the training loop we do a backward() and then zero_grad() at the end of the loop\n",
    "# this would cause an incorrect first training step\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0001\n",
    "learning_rate = 3e-5\n",
    "learning_rate_decay_frac = 0.0001\n",
    "warmup_iters = 0\n",
    "num_iterations = 1000\n",
    "val_loss_every = 0\n",
    "val_max_steps = 20\n",
    "sample_every = 0\n",
    "overfit_single_batch = 1\n",
    "inference_only = 0\n",
    "grad_clip = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 2,999,808 parameters\n",
      "num non-decayed parameter tensors: 98, with 22,752 parameters\n",
      "using fused AdamW: False\n",
      "using regular AdamW\n"
     ]
    }
   ],
   "source": [
    "raw_model = model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "# init the optimizer\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=weight_decay,\n",
    "                                            learning_rate=learning_rate, betas=(0.9, 0.95),\n",
    "                                            device_type=device, zero_stage=zero_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    min_lr = learning_rate * learning_rate_decay_frac\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it+1) / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > num_iterations:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (num_iterations - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_list = ['a', 'b', 'c', 'ç', 'd', 'e', 'f', 'g', 'ğ', 'h', 'ı', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'ö', 'p', 'r', 's', 'ş', 't', 'u', 'ü', 'v', 'y', 'z', \n",
    "               ' ', '.', ',']\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "  return [letter_list.index(c) for c in text.lower()]\n",
    "\n",
    "def decode(ids):\n",
    "  return ''.join([letter_list[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0385,  0.0297,  0.0180, -0.0421,  0.0136],\n",
       "        [-0.0195,  0.0192,  0.0324,  0.0290,  0.0054],\n",
       "        [ 0.0195, -0.0203, -0.0108, -0.0088, -0.0063],\n",
       "        [-0.0099,  0.0227, -0.0092,  0.0284,  0.0170],\n",
       "        [-0.0161, -0.0224,  0.0039, -0.0156, -0.0358]], device='mps:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/1000 | train loss 4.875928 | norm 87.7134 | lr 3.00e-05 | (145.21 ms | 28207 tok/s)\n",
      "step    2/1000 | train loss 4.405680 | norm 83.7764 | lr 3.00e-05 | (115.14 ms | 35573 tok/s)\n",
      "step    3/1000 | train loss 3.992568 | norm 84.5736 | lr 3.00e-05 | (114.61 ms | 35739 tok/s)\n",
      "step    4/1000 | train loss 3.602044 | norm 70.9818 | lr 3.00e-05 | (113.81 ms | 35991 tok/s)\n",
      "step    5/1000 | train loss 3.289901 | norm 63.3888 | lr 3.00e-05 | (115.01 ms | 35614 tok/s)\n",
      "step    6/1000 | train loss 2.989498 | norm 53.5348 | lr 3.00e-05 | (114.32 ms | 35831 tok/s)\n",
      "step    7/1000 | train loss 2.748940 | norm 50.9413 | lr 3.00e-05 | (115.05 ms | 35602 tok/s)\n",
      "step    8/1000 | train loss 2.530151 | norm 35.5240 | lr 3.00e-05 | (115.04 ms | 35606 tok/s)\n",
      "step    9/1000 | train loss 2.354410 | norm 35.2989 | lr 3.00e-05 | (117.74 ms | 34788 tok/s)\n",
      "step   10/1000 | train loss 2.215710 | norm 31.9573 | lr 3.00e-05 | (115.76 ms | 35385 tok/s)\n",
      "step   11/1000 | train loss 2.101106 | norm 28.6883 | lr 3.00e-05 | (115.50 ms | 35464 tok/s)\n",
      "step   12/1000 | train loss 2.008826 | norm 32.2245 | lr 3.00e-05 | (113.71 ms | 36020 tok/s)\n",
      "step   13/1000 | train loss 1.915653 | norm 21.8129 | lr 3.00e-05 | (115.85 ms | 35358 tok/s)\n",
      "step   14/1000 | train loss 1.831964 | norm 23.4118 | lr 3.00e-05 | (115.82 ms | 35364 tok/s)\n",
      "step   15/1000 | train loss 1.753090 | norm 22.1991 | lr 3.00e-05 | (113.70 ms | 36026 tok/s)\n",
      "step   16/1000 | train loss 1.674258 | norm 22.6813 | lr 3.00e-05 | (115.29 ms | 35527 tok/s)\n",
      "step   17/1000 | train loss 1.598005 | norm 23.4747 | lr 3.00e-05 | (116.60 ms | 35128 tok/s)\n",
      "step   18/1000 | train loss 1.520095 | norm 20.6013 | lr 3.00e-05 | (113.59 ms | 36060 tok/s)\n",
      "step   19/1000 | train loss 1.447578 | norm 23.1282 | lr 3.00e-05 | (115.46 ms | 35475 tok/s)\n",
      "step   20/1000 | train loss 1.381934 | norm 27.1274 | lr 3.00e-05 | (115.67 ms | 35411 tok/s)\n",
      "step   21/1000 | train loss 1.318605 | norm 24.2982 | lr 3.00e-05 | (113.16 ms | 36197 tok/s)\n",
      "step   22/1000 | train loss 1.251642 | norm 20.1504 | lr 3.00e-05 | (116.82 ms | 35064 tok/s)\n",
      "step   23/1000 | train loss 1.190646 | norm 25.3964 | lr 3.00e-05 | (116.52 ms | 35152 tok/s)\n",
      "step   24/1000 | train loss 1.127530 | norm 19.6354 | lr 3.00e-05 | (114.02 ms | 35923 tok/s)\n",
      "step   25/1000 | train loss 1.066713 | norm 20.0035 | lr 3.00e-05 | (115.52 ms | 35457 tok/s)\n",
      "step   26/1000 | train loss 1.006794 | norm 20.4946 | lr 3.00e-05 | (116.78 ms | 35075 tok/s)\n",
      "step   27/1000 | train loss 0.944407 | norm 16.9968 | lr 2.99e-05 | (114.92 ms | 35643 tok/s)\n",
      "step   28/1000 | train loss 0.886270 | norm 19.9360 | lr 2.99e-05 | (115.12 ms | 35581 tok/s)\n",
      "step   29/1000 | train loss 0.830653 | norm 22.1336 | lr 2.99e-05 | (116.12 ms | 35273 tok/s)\n",
      "step   30/1000 | train loss 0.783182 | norm 24.2301 | lr 2.99e-05 | (112.89 ms | 36284 tok/s)\n",
      "step   31/1000 | train loss 0.738651 | norm 24.6983 | lr 2.99e-05 | (115.10 ms | 35586 tok/s)\n",
      "step   32/1000 | train loss 0.693994 | norm 23.1480 | lr 2.99e-05 | (114.41 ms | 35800 tok/s)\n",
      "step   33/1000 | train loss 0.650655 | norm 20.4086 | lr 2.99e-05 | (115.44 ms | 35482 tok/s)\n",
      "step   34/1000 | train loss 0.607628 | norm 20.4007 | lr 2.99e-05 | (115.72 ms | 35395 tok/s)\n",
      "step   35/1000 | train loss 0.571946 | norm 24.6998 | lr 2.99e-05 | (116.48 ms | 35165 tok/s)\n",
      "step   36/1000 | train loss 0.538766 | norm 27.4640 | lr 2.99e-05 | (115.65 ms | 35417 tok/s)\n",
      "step   37/1000 | train loss 0.508884 | norm 27.0503 | lr 2.99e-05 | (112.95 ms | 36263 tok/s)\n",
      "step   38/1000 | train loss 0.475406 | norm 22.7946 | lr 2.99e-05 | (115.14 ms | 35575 tok/s)\n",
      "step   39/1000 | train loss 0.450083 | norm 25.5547 | lr 2.99e-05 | (115.29 ms | 35528 tok/s)\n",
      "step   40/1000 | train loss 0.417066 | norm 17.7361 | lr 2.99e-05 | (114.43 ms | 35795 tok/s)\n",
      "step   41/1000 | train loss 0.389998 | norm 16.7382 | lr 2.99e-05 | (114.66 ms | 35723 tok/s)\n",
      "step   42/1000 | train loss 0.360872 | norm 14.4726 | lr 2.99e-05 | (114.62 ms | 35735 tok/s)\n",
      "step   43/1000 | train loss 0.342767 | norm 24.5460 | lr 2.99e-05 | (115.76 ms | 35382 tok/s)\n",
      "step   44/1000 | train loss 0.318853 | norm 22.0236 | lr 2.99e-05 | (115.28 ms | 35531 tok/s)\n",
      "step   45/1000 | train loss 0.301552 | norm 23.2291 | lr 2.99e-05 | (117.31 ms | 34915 tok/s)\n",
      "step   46/1000 | train loss 0.278110 | norm 12.6131 | lr 2.99e-05 | (114.09 ms | 35901 tok/s)\n",
      "step   47/1000 | train loss 0.264967 | norm 19.9379 | lr 2.98e-05 | (115.66 ms | 35415 tok/s)\n",
      "step   48/1000 | train loss 0.249352 | norm 18.4318 | lr 2.98e-05 | (114.66 ms | 35722 tok/s)\n",
      "step   49/1000 | train loss 0.233795 | norm 16.0429 | lr 2.98e-05 | (113.47 ms | 36097 tok/s)\n",
      "step   50/1000 | train loss 0.224268 | norm 21.2240 | lr 2.98e-05 | (114.89 ms | 35653 tok/s)\n",
      "step   51/1000 | train loss 0.219161 | norm 29.7212 | lr 2.98e-05 | (113.58 ms | 36062 tok/s)\n",
      "step   52/1000 | train loss 0.203557 | norm 14.0660 | lr 2.98e-05 | (115.00 ms | 35618 tok/s)\n",
      "step   53/1000 | train loss 0.193559 | norm 14.2660 | lr 2.98e-05 | (114.36 ms | 35816 tok/s)\n",
      "step   54/1000 | train loss 0.184670 | norm 14.1708 | lr 2.98e-05 | (142.49 ms | 28747 tok/s)\n",
      "step   55/1000 | train loss 0.176385 | norm 12.9910 | lr 2.98e-05 | (114.80 ms | 35678 tok/s)\n",
      "step   56/1000 | train loss 0.169300 | norm 11.1785 | lr 2.98e-05 | (116.82 ms | 35063 tok/s)\n",
      "step   57/1000 | train loss 0.161353 | norm 9.4372 | lr 2.98e-05 | (114.86 ms | 35659 tok/s)\n",
      "step   58/1000 | train loss 0.154804 | norm 10.5257 | lr 2.98e-05 | (116.00 ms | 35312 tok/s)\n",
      "step   59/1000 | train loss 0.147941 | norm 9.9209 | lr 2.98e-05 | (114.12 ms | 35893 tok/s)\n",
      "step   60/1000 | train loss 0.143584 | norm 10.5139 | lr 2.97e-05 | (113.55 ms | 36073 tok/s)\n",
      "step   61/1000 | train loss 0.137147 | norm 8.4412 | lr 2.97e-05 | (115.13 ms | 35576 tok/s)\n",
      "step   62/1000 | train loss 0.136450 | norm 13.1886 | lr 2.97e-05 | (113.63 ms | 36047 tok/s)\n",
      "step   63/1000 | train loss 0.129629 | norm 7.1068 | lr 2.97e-05 | (116.09 ms | 35282 tok/s)\n",
      "step   64/1000 | train loss 0.125672 | norm 7.2280 | lr 2.97e-05 | (115.41 ms | 35490 tok/s)\n",
      "step   65/1000 | train loss 0.123987 | norm 9.2531 | lr 2.97e-05 | (116.60 ms | 35128 tok/s)\n",
      "step   66/1000 | train loss 0.122908 | norm 9.9983 | lr 2.97e-05 | (116.52 ms | 35152 tok/s)\n",
      "step   67/1000 | train loss 0.116188 | norm 5.6596 | lr 2.97e-05 | (114.10 ms | 35899 tok/s)\n",
      "step   68/1000 | train loss 0.116599 | norm 8.9453 | lr 2.97e-05 | (115.46 ms | 35474 tok/s)\n",
      "step   69/1000 | train loss 0.113660 | norm 6.5320 | lr 2.97e-05 | (114.09 ms | 35902 tok/s)\n",
      "step   70/1000 | train loss 0.111204 | norm 8.4849 | lr 2.96e-05 | (114.34 ms | 35825 tok/s)\n",
      "step   71/1000 | train loss 0.109083 | norm 9.3812 | lr 2.96e-05 | (113.81 ms | 35990 tok/s)\n",
      "step   72/1000 | train loss 0.110780 | norm 12.8017 | lr 2.96e-05 | (114.66 ms | 35723 tok/s)\n",
      "step   73/1000 | train loss 0.110022 | norm 12.5156 | lr 2.96e-05 | (115.43 ms | 35484 tok/s)\n",
      "step   74/1000 | train loss 0.108125 | norm 8.7970 | lr 2.96e-05 | (117.69 ms | 34804 tok/s)\n",
      "step   75/1000 | train loss 0.106041 | norm 9.5314 | lr 2.96e-05 | (117.91 ms | 34739 tok/s)\n",
      "step   76/1000 | train loss 0.109799 | norm 16.5769 | lr 2.96e-05 | (115.32 ms | 35519 tok/s)\n",
      "step   77/1000 | train loss 0.109178 | norm 16.3370 | lr 2.96e-05 | (115.77 ms | 35379 tok/s)\n",
      "step   78/1000 | train loss 0.114881 | norm 23.6760 | lr 2.96e-05 | (115.02 ms | 35612 tok/s)\n",
      "step   79/1000 | train loss 0.139671 | norm 65.4032 | lr 2.96e-05 | (115.58 ms | 35437 tok/s)\n",
      "step   80/1000 | train loss 0.114475 | norm 25.3812 | lr 2.95e-05 | (115.52 ms | 35459 tok/s)\n",
      "step   81/1000 | train loss 0.113042 | norm 18.7688 | lr 2.95e-05 | (116.24 ms | 35238 tok/s)\n",
      "step   82/1000 | train loss 0.107158 | norm 10.4947 | lr 2.95e-05 | (115.35 ms | 35510 tok/s)\n",
      "step   83/1000 | train loss 0.108880 | norm 23.7172 | lr 2.95e-05 | (114.39 ms | 35807 tok/s)\n",
      "step   84/1000 | train loss 0.118425 | norm 23.9224 | lr 2.95e-05 | (116.14 ms | 35268 tok/s)\n",
      "step   85/1000 | train loss 0.125249 | norm 28.9830 | lr 2.95e-05 | (115.57 ms | 35440 tok/s)\n",
      "step   86/1000 | train loss 0.105214 | norm 7.7038 | lr 2.95e-05 | (113.36 ms | 36131 tok/s)\n",
      "step   87/1000 | train loss 0.115241 | norm 26.9329 | lr 2.95e-05 | (115.83 ms | 35362 tok/s)\n",
      "step   88/1000 | train loss 0.109122 | norm 11.3091 | lr 2.94e-05 | (113.78 ms | 35999 tok/s)\n",
      "step   89/1000 | train loss 0.104499 | norm 7.2951 | lr 2.94e-05 | (115.71 ms | 35399 tok/s)\n",
      "step   90/1000 | train loss 0.100778 | norm 8.6228 | lr 2.94e-05 | (113.39 ms | 36124 tok/s)\n",
      "step   91/1000 | train loss 0.103712 | norm 10.2793 | lr 2.94e-05 | (115.50 ms | 35462 tok/s)\n",
      "step   92/1000 | train loss 0.102611 | norm 10.1325 | lr 2.94e-05 | (113.69 ms | 36029 tok/s)\n",
      "step   93/1000 | train loss 0.099772 | norm 6.8788 | lr 2.94e-05 | (115.62 ms | 35427 tok/s)\n",
      "step   94/1000 | train loss 0.098448 | norm 6.8155 | lr 2.94e-05 | (113.89 ms | 35963 tok/s)\n",
      "step   95/1000 | train loss 0.098421 | norm 5.8923 | lr 2.94e-05 | (115.75 ms | 35386 tok/s)\n",
      "step   96/1000 | train loss 0.096277 | norm 5.8702 | lr 2.93e-05 | (114.03 ms | 35920 tok/s)\n",
      "step   97/1000 | train loss 0.096101 | norm 6.0345 | lr 2.93e-05 | (115.53 ms | 35455 tok/s)\n",
      "step   98/1000 | train loss 0.095380 | norm 5.4654 | lr 2.93e-05 | (115.65 ms | 35417 tok/s)\n",
      "step   99/1000 | train loss 0.093858 | norm 4.0507 | lr 2.93e-05 | (113.82 ms | 35987 tok/s)\n",
      "step  100/1000 | train loss 0.095369 | norm 7.5879 | lr 2.93e-05 | (115.37 ms | 35504 tok/s)\n",
      "step  101/1000 | train loss 0.095578 | norm 6.7979 | lr 2.93e-05 | (114.97 ms | 35627 tok/s)\n",
      "step  102/1000 | train loss 0.093338 | norm 6.6860 | lr 2.93e-05 | (115.14 ms | 35574 tok/s)\n",
      "step  103/1000 | train loss 0.097429 | norm 13.4114 | lr 2.92e-05 | (115.16 ms | 35568 tok/s)\n",
      "step  104/1000 | train loss 0.097812 | norm 9.0041 | lr 2.92e-05 | (115.49 ms | 35466 tok/s)\n",
      "step  105/1000 | train loss 0.100941 | norm 16.2946 | lr 2.92e-05 | (115.21 ms | 35554 tok/s)\n",
      "step  106/1000 | train loss 0.104113 | norm 20.1980 | lr 2.92e-05 | (116.12 ms | 35274 tok/s)\n",
      "step  107/1000 | train loss 0.104654 | norm 20.8780 | lr 2.92e-05 | (115.92 ms | 35334 tok/s)\n",
      "step  108/1000 | train loss 0.124826 | norm 34.4465 | lr 2.92e-05 | (114.48 ms | 35781 tok/s)\n",
      "step  109/1000 | train loss 0.118136 | norm 28.3773 | lr 2.91e-05 | (116.15 ms | 35265 tok/s)\n",
      "step  110/1000 | train loss 0.105914 | norm 13.9898 | lr 2.91e-05 | (114.60 ms | 35742 tok/s)\n",
      "step  111/1000 | train loss 0.100715 | norm 15.3755 | lr 2.91e-05 | (113.38 ms | 36126 tok/s)\n",
      "step  112/1000 | train loss 0.098404 | norm 9.0657 | lr 2.91e-05 | (115.48 ms | 35469 tok/s)\n",
      "step  113/1000 | train loss 0.095221 | norm 7.5256 | lr 2.91e-05 | (116.01 ms | 35307 tok/s)\n",
      "step  114/1000 | train loss 0.094568 | norm 7.8460 | lr 2.91e-05 | (115.29 ms | 35528 tok/s)\n",
      "step  115/1000 | train loss 0.094171 | norm 6.0398 | lr 2.90e-05 | (115.50 ms | 35462 tok/s)\n",
      "step  116/1000 | train loss 0.093686 | norm 7.6308 | lr 2.90e-05 | (114.97 ms | 35627 tok/s)\n",
      "step  117/1000 | train loss 0.093099 | norm 4.3088 | lr 2.90e-05 | (115.33 ms | 35517 tok/s)\n",
      "step  118/1000 | train loss 0.093922 | norm 11.6855 | lr 2.90e-05 | (114.80 ms | 35679 tok/s)\n",
      "step  119/1000 | train loss 0.098741 | norm 14.8891 | lr 2.90e-05 | (116.03 ms | 35300 tok/s)\n",
      "step  120/1000 | train loss 0.094606 | norm 5.0647 | lr 2.90e-05 | (114.78 ms | 35684 tok/s)\n",
      "step  121/1000 | train loss 0.092459 | norm 6.3472 | lr 2.89e-05 | (115.76 ms | 35383 tok/s)\n",
      "step  122/1000 | train loss 0.092545 | norm 5.1208 | lr 2.89e-05 | (115.46 ms | 35474 tok/s)\n",
      "step  123/1000 | train loss 0.091313 | norm 4.5771 | lr 2.89e-05 | (114.08 ms | 35904 tok/s)\n",
      "step  124/1000 | train loss 0.091068 | norm 4.5281 | lr 2.89e-05 | (114.55 ms | 35759 tok/s)\n",
      "step  125/1000 | train loss 0.090518 | norm 3.7786 | lr 2.89e-05 | (116.12 ms | 35273 tok/s)\n",
      "step  126/1000 | train loss 0.089569 | norm 4.1476 | lr 2.89e-05 | (113.31 ms | 36149 tok/s)\n",
      "step  127/1000 | train loss 0.088527 | norm 3.0508 | lr 2.88e-05 | (113.66 ms | 36038 tok/s)\n",
      "step  128/1000 | train loss 0.088224 | norm 3.3858 | lr 2.88e-05 | (115.62 ms | 35425 tok/s)\n",
      "step  129/1000 | train loss 0.088252 | norm 3.6542 | lr 2.88e-05 | (116.62 ms | 35122 tok/s)\n",
      "step  130/1000 | train loss 0.087635 | norm 3.4261 | lr 2.88e-05 | (113.78 ms | 35999 tok/s)\n",
      "step  131/1000 | train loss 0.087127 | norm 3.6050 | lr 2.88e-05 | (115.23 ms | 35545 tok/s)\n",
      "step  132/1000 | train loss 0.087201 | norm 3.5618 | lr 2.87e-05 | (115.39 ms | 35496 tok/s)\n",
      "step  133/1000 | train loss 0.086257 | norm 2.7941 | lr 2.87e-05 | (116.56 ms | 35140 tok/s)\n",
      "step  134/1000 | train loss 0.086554 | norm 3.7129 | lr 2.87e-05 | (113.34 ms | 36140 tok/s)\n",
      "step  135/1000 | train loss 0.085326 | norm 2.6918 | lr 2.87e-05 | (116.12 ms | 35274 tok/s)\n",
      "step  136/1000 | train loss 0.085829 | norm 3.3290 | lr 2.87e-05 | (115.15 ms | 35572 tok/s)\n",
      "step  137/1000 | train loss 0.084474 | norm 2.5799 | lr 2.87e-05 | (116.42 ms | 35182 tok/s)\n",
      "step  138/1000 | train loss 0.085865 | norm 3.4842 | lr 2.86e-05 | (115.79 ms | 35376 tok/s)\n",
      "step  139/1000 | train loss 0.083914 | norm 2.6470 | lr 2.86e-05 | (115.95 ms | 35326 tok/s)\n",
      "step  140/1000 | train loss 0.084052 | norm 3.7265 | lr 2.86e-05 | (115.10 ms | 35586 tok/s)\n",
      "step  141/1000 | train loss 0.085875 | norm 4.6096 | lr 2.86e-05 | (114.18 ms | 35873 tok/s)\n",
      "step  142/1000 | train loss 0.083704 | norm 4.1806 | lr 2.86e-05 | (113.09 ms | 36220 tok/s)\n",
      "step  143/1000 | train loss 0.087604 | norm 9.6013 | lr 2.85e-05 | (115.18 ms | 35561 tok/s)\n",
      "step  144/1000 | train loss 0.087492 | norm 6.3127 | lr 2.85e-05 | (115.39 ms | 35498 tok/s)\n",
      "step  145/1000 | train loss 0.086438 | norm 11.5829 | lr 2.85e-05 | (113.98 ms | 35937 tok/s)\n",
      "step  146/1000 | train loss 0.091553 | norm 17.1432 | lr 2.85e-05 | (114.65 ms | 35727 tok/s)\n",
      "step  147/1000 | train loss 0.088642 | norm 6.3461 | lr 2.84e-05 | (114.62 ms | 35735 tok/s)\n",
      "step  148/1000 | train loss 0.084971 | norm 3.8341 | lr 2.84e-05 | (114.08 ms | 35904 tok/s)\n",
      "step  149/1000 | train loss 0.084907 | norm 4.5937 | lr 2.84e-05 | (113.09 ms | 36220 tok/s)\n",
      "step  150/1000 | train loss 0.085665 | norm 4.3056 | lr 2.84e-05 | (115.27 ms | 35535 tok/s)\n",
      "step  151/1000 | train loss 0.084236 | norm 4.1414 | lr 2.84e-05 | (115.32 ms | 35518 tok/s)\n",
      "step  152/1000 | train loss 0.084552 | norm 4.0389 | lr 2.83e-05 | (115.76 ms | 35383 tok/s)\n",
      "step  153/1000 | train loss 0.084785 | norm 8.5720 | lr 2.83e-05 | (115.52 ms | 35456 tok/s)\n",
      "step  154/1000 | train loss 0.086480 | norm 4.9962 | lr 2.83e-05 | (113.46 ms | 36101 tok/s)\n",
      "step  155/1000 | train loss 0.085843 | norm 5.7812 | lr 2.83e-05 | (115.92 ms | 35334 tok/s)\n",
      "step  156/1000 | train loss 0.087109 | norm 7.8936 | lr 2.83e-05 | (115.77 ms | 35379 tok/s)\n",
      "step  157/1000 | train loss 0.084941 | norm 3.3547 | lr 2.82e-05 | (117.50 ms | 34860 tok/s)\n",
      "step  158/1000 | train loss 0.083263 | norm 2.6592 | lr 2.82e-05 | (114.01 ms | 35927 tok/s)\n",
      "step  159/1000 | train loss 0.083085 | norm 2.7208 | lr 2.82e-05 | (116.02 ms | 35305 tok/s)\n",
      "step  160/1000 | train loss 0.082387 | norm 2.4222 | lr 2.82e-05 | (113.01 ms | 36245 tok/s)\n",
      "step  161/1000 | train loss 0.081911 | norm 2.2473 | lr 2.81e-05 | (115.81 ms | 35368 tok/s)\n",
      "step  162/1000 | train loss 0.081835 | norm 2.5705 | lr 2.81e-05 | (112.75 ms | 36327 tok/s)\n",
      "step  163/1000 | train loss 0.081136 | norm 2.5336 | lr 2.81e-05 | (114.94 ms | 35634 tok/s)\n",
      "step  164/1000 | train loss 0.081216 | norm 2.5691 | lr 2.81e-05 | (115.00 ms | 35618 tok/s)\n",
      "step  165/1000 | train loss 0.081301 | norm 4.3038 | lr 2.81e-05 | (115.01 ms | 35613 tok/s)\n",
      "step  166/1000 | train loss 0.083294 | norm 4.0826 | lr 2.80e-05 | (114.05 ms | 35913 tok/s)\n",
      "step  167/1000 | train loss 0.083378 | norm 3.1284 | lr 2.80e-05 | (116.16 ms | 35261 tok/s)\n",
      "step  168/1000 | train loss 0.082319 | norm 2.6903 | lr 2.80e-05 | (111.90 ms | 36603 tok/s)\n",
      "step  169/1000 | train loss 0.080991 | norm 2.4792 | lr 2.80e-05 | (114.69 ms | 35714 tok/s)\n",
      "step  170/1000 | train loss 0.080999 | norm 2.4980 | lr 2.79e-05 | (115.50 ms | 35462 tok/s)\n",
      "step  171/1000 | train loss 0.080936 | norm 2.7979 | lr 2.79e-05 | (116.00 ms | 35312 tok/s)\n",
      "step  172/1000 | train loss 0.080676 | norm 2.8883 | lr 2.79e-05 | (115.12 ms | 35581 tok/s)\n",
      "step  173/1000 | train loss 0.080971 | norm 4.7796 | lr 2.79e-05 | (114.81 ms | 35676 tok/s)\n",
      "step  174/1000 | train loss 0.080425 | norm 2.2626 | lr 2.78e-05 | (114.96 ms | 35629 tok/s)\n",
      "step  175/1000 | train loss 0.081269 | norm 3.1852 | lr 2.78e-05 | (113.87 ms | 35972 tok/s)\n",
      "step  176/1000 | train loss 0.080058 | norm 2.0096 | lr 2.78e-05 | (114.84 ms | 35667 tok/s)\n",
      "step  177/1000 | train loss 0.080882 | norm 2.7325 | lr 2.78e-05 | (115.85 ms | 35355 tok/s)\n",
      "step  178/1000 | train loss 0.080502 | norm 2.5748 | lr 2.77e-05 | (115.44 ms | 35480 tok/s)\n",
      "step  179/1000 | train loss 0.079985 | norm 2.3170 | lr 2.77e-05 | (114.24 ms | 35854 tok/s)\n",
      "step  180/1000 | train loss 0.079552 | norm 2.1332 | lr 2.77e-05 | (115.85 ms | 35356 tok/s)\n",
      "step  181/1000 | train loss 0.079372 | norm 2.1144 | lr 2.77e-05 | (115.34 ms | 35511 tok/s)\n",
      "step  182/1000 | train loss 0.079007 | norm 2.0416 | lr 2.76e-05 | (112.33 ms | 36464 tok/s)\n",
      "step  183/1000 | train loss 0.079248 | norm 2.3434 | lr 2.76e-05 | (114.45 ms | 35789 tok/s)\n",
      "step  184/1000 | train loss 0.078463 | norm 1.7864 | lr 2.76e-05 | (113.04 ms | 36233 tok/s)\n",
      "step  185/1000 | train loss 0.078891 | norm 2.0776 | lr 2.76e-05 | (116.07 ms | 35288 tok/s)\n",
      "step  186/1000 | train loss 0.078336 | norm 1.7343 | lr 2.75e-05 | (114.77 ms | 35689 tok/s)\n",
      "step  187/1000 | train loss 0.078483 | norm 2.0273 | lr 2.75e-05 | (115.83 ms | 35362 tok/s)\n",
      "step  188/1000 | train loss 0.078115 | norm 1.7979 | lr 2.75e-05 | (113.99 ms | 35935 tok/s)\n",
      "step  189/1000 | train loss 0.078224 | norm 2.0830 | lr 2.75e-05 | (114.46 ms | 35786 tok/s)\n",
      "step  190/1000 | train loss 0.078359 | norm 1.9741 | lr 2.74e-05 | (115.45 ms | 35479 tok/s)\n",
      "step  191/1000 | train loss 0.077519 | norm 1.5936 | lr 2.74e-05 | (114.58 ms | 35749 tok/s)\n",
      "step  192/1000 | train loss 0.078140 | norm 2.0802 | lr 2.74e-05 | (115.66 ms | 35415 tok/s)\n",
      "step  193/1000 | train loss 0.077750 | norm 1.7852 | lr 2.74e-05 | (118.43 ms | 34586 tok/s)\n",
      "step  194/1000 | train loss 0.077279 | norm 1.6147 | lr 2.73e-05 | (114.60 ms | 35743 tok/s)\n",
      "step  195/1000 | train loss 0.077707 | norm 1.8931 | lr 2.73e-05 | (115.29 ms | 35528 tok/s)\n",
      "step  196/1000 | train loss 0.077118 | norm 1.6663 | lr 2.73e-05 | (112.10 ms | 36539 tok/s)\n",
      "step  197/1000 | train loss 0.077266 | norm 1.7725 | lr 2.72e-05 | (116.16 ms | 35262 tok/s)\n",
      "step  198/1000 | train loss 0.076577 | norm 1.1703 | lr 2.72e-05 | (112.04 ms | 36560 tok/s)\n",
      "step  199/1000 | train loss 0.077316 | norm 1.8759 | lr 2.72e-05 | (114.99 ms | 35622 tok/s)\n",
      "step  200/1000 | train loss 0.076911 | norm 1.3889 | lr 2.72e-05 | (115.22 ms | 35549 tok/s)\n",
      "step  201/1000 | train loss 0.076603 | norm 1.3607 | lr 2.71e-05 | (114.86 ms | 35660 tok/s)\n",
      "step  202/1000 | train loss 0.077110 | norm 1.9588 | lr 2.71e-05 | (114.90 ms | 35648 tok/s)\n",
      "step  203/1000 | train loss 0.077001 | norm 2.0606 | lr 2.71e-05 | (115.12 ms | 35579 tok/s)\n",
      "step  204/1000 | train loss 0.076417 | norm 1.5608 | lr 2.71e-05 | (114.93 ms | 35638 tok/s)\n",
      "step  205/1000 | train loss 0.076753 | norm 1.8620 | lr 2.70e-05 | (116.75 ms | 35084 tok/s)\n",
      "step  206/1000 | train loss 0.076528 | norm 1.5891 | lr 2.70e-05 | (113.49 ms | 36093 tok/s)\n",
      "step  207/1000 | train loss 0.076196 | norm 1.5089 | lr 2.70e-05 | (116.03 ms | 35302 tok/s)\n",
      "step  208/1000 | train loss 0.076433 | norm 1.9262 | lr 2.69e-05 | (115.23 ms | 35545 tok/s)\n",
      "step  209/1000 | train loss 0.076276 | norm 1.6627 | lr 2.69e-05 | (116.71 ms | 35094 tok/s)\n",
      "step  210/1000 | train loss 0.075874 | norm 1.4030 | lr 2.69e-05 | (115.34 ms | 35511 tok/s)\n",
      "step  211/1000 | train loss 0.076183 | norm 1.6918 | lr 2.69e-05 | (113.18 ms | 36190 tok/s)\n",
      "step  212/1000 | train loss 0.075937 | norm 1.4932 | lr 2.68e-05 | (114.22 ms | 35859 tok/s)\n",
      "step  213/1000 | train loss 0.075863 | norm 1.6395 | lr 2.68e-05 | (115.01 ms | 35614 tok/s)\n",
      "step  214/1000 | train loss 0.075840 | norm 1.5238 | lr 2.68e-05 | (115.05 ms | 35601 tok/s)\n",
      "step  215/1000 | train loss 0.075761 | norm 1.5662 | lr 2.67e-05 | (116.10 ms | 35278 tok/s)\n",
      "step  216/1000 | train loss 0.075878 | norm 1.7193 | lr 2.67e-05 | (114.77 ms | 35689 tok/s)\n",
      "step  217/1000 | train loss 0.075496 | norm 1.3545 | lr 2.67e-05 | (115.02 ms | 35611 tok/s)\n",
      "step  218/1000 | train loss 0.075435 | norm 1.4603 | lr 2.66e-05 | (114.21 ms | 35863 tok/s)\n",
      "step  219/1000 | train loss 0.075398 | norm 1.4169 | lr 2.66e-05 | (115.47 ms | 35473 tok/s)\n",
      "step  220/1000 | train loss 0.075411 | norm 1.4904 | lr 2.66e-05 | (115.66 ms | 35416 tok/s)\n",
      "step  221/1000 | train loss 0.075334 | norm 1.4424 | lr 2.66e-05 | (116.71 ms | 35096 tok/s)\n",
      "step  222/1000 | train loss 0.075201 | norm 1.4252 | lr 2.65e-05 | (115.73 ms | 35392 tok/s)\n",
      "step  223/1000 | train loss 0.075212 | norm 1.4851 | lr 2.65e-05 | (116.11 ms | 35276 tok/s)\n",
      "step  224/1000 | train loss 0.075298 | norm 1.5584 | lr 2.65e-05 | (114.88 ms | 35655 tok/s)\n",
      "step  225/1000 | train loss 0.074967 | norm 1.2939 | lr 2.64e-05 | (115.09 ms | 35590 tok/s)\n",
      "step  226/1000 | train loss 0.075323 | norm 1.7250 | lr 2.64e-05 | (114.50 ms | 35772 tok/s)\n",
      "step  227/1000 | train loss 0.074900 | norm 1.3379 | lr 2.64e-05 | (115.04 ms | 35605 tok/s)\n",
      "step  228/1000 | train loss 0.075032 | norm 1.4988 | lr 2.63e-05 | (112.36 ms | 36453 tok/s)\n",
      "step  229/1000 | train loss 0.074804 | norm 1.3024 | lr 2.63e-05 | (115.60 ms | 35434 tok/s)\n",
      "step  230/1000 | train loss 0.075124 | norm 1.6617 | lr 2.63e-05 | (115.28 ms | 35531 tok/s)\n",
      "step  231/1000 | train loss 0.074504 | norm 1.0546 | lr 2.63e-05 | (114.84 ms | 35667 tok/s)\n",
      "step  232/1000 | train loss 0.075396 | norm 1.8532 | lr 2.62e-05 | (113.92 ms | 35956 tok/s)\n",
      "step  233/1000 | train loss 0.074714 | norm 1.1083 | lr 2.62e-05 | (115.25 ms | 35541 tok/s)\n",
      "step  234/1000 | train loss 0.075333 | norm 1.6039 | lr 2.62e-05 | (113.93 ms | 35952 tok/s)\n",
      "step  235/1000 | train loss 0.075224 | norm 1.5525 | lr 2.61e-05 | (116.43 ms | 35179 tok/s)\n",
      "step  236/1000 | train loss 0.074615 | norm 1.2034 | lr 2.61e-05 | (113.52 ms | 36080 tok/s)\n",
      "step  237/1000 | train loss 0.074781 | norm 1.3773 | lr 2.61e-05 | (115.42 ms | 35487 tok/s)\n",
      "step  238/1000 | train loss 0.074539 | norm 1.2081 | lr 2.60e-05 | (114.22 ms | 35860 tok/s)\n",
      "step  239/1000 | train loss 0.074748 | norm 1.4513 | lr 2.60e-05 | (114.98 ms | 35625 tok/s)\n",
      "step  240/1000 | train loss 0.074252 | norm 1.1640 | lr 2.60e-05 | (114.36 ms | 35816 tok/s)\n",
      "step  241/1000 | train loss 0.074759 | norm 1.6384 | lr 2.59e-05 | (113.07 ms | 36225 tok/s)\n",
      "step  242/1000 | train loss 0.074330 | norm 1.1353 | lr 2.59e-05 | (112.88 ms | 36287 tok/s)\n",
      "step  243/1000 | train loss 0.074417 | norm 1.3680 | lr 2.59e-05 | (113.55 ms | 36071 tok/s)\n",
      "step  244/1000 | train loss 0.074721 | norm 1.4691 | lr 2.58e-05 | (114.91 ms | 35646 tok/s)\n",
      "step  245/1000 | train loss 0.073910 | norm 0.9474 | lr 2.58e-05 | (115.61 ms | 35431 tok/s)\n",
      "step  246/1000 | train loss 0.074500 | norm 1.4388 | lr 2.58e-05 | (115.97 ms | 35318 tok/s)\n",
      "step  247/1000 | train loss 0.074193 | norm 1.1415 | lr 2.57e-05 | (115.51 ms | 35459 tok/s)\n",
      "step  248/1000 | train loss 0.074274 | norm 1.4570 | lr 2.57e-05 | (114.02 ms | 35922 tok/s)\n",
      "step  249/1000 | train loss 0.074264 | norm 1.2641 | lr 2.57e-05 | (115.08 ms | 35593 tok/s)\n",
      "step  250/1000 | train loss 0.074127 | norm 1.3537 | lr 2.56e-05 | (115.62 ms | 35426 tok/s)\n",
      "step  251/1000 | train loss 0.073956 | norm 1.0892 | lr 2.56e-05 | (114.87 ms | 35659 tok/s)\n",
      "step  252/1000 | train loss 0.074073 | norm 1.4096 | lr 2.56e-05 | (114.45 ms | 35787 tok/s)\n",
      "step  253/1000 | train loss 0.073887 | norm 1.2299 | lr 2.55e-05 | (116.32 ms | 35214 tok/s)\n",
      "step  254/1000 | train loss 0.073985 | norm 1.3751 | lr 2.55e-05 | (115.28 ms | 35532 tok/s)\n",
      "step  255/1000 | train loss 0.073484 | norm 0.9426 | lr 2.55e-05 | (114.43 ms | 35796 tok/s)\n",
      "step  256/1000 | train loss 0.073917 | norm 1.3533 | lr 2.54e-05 | (114.66 ms | 35724 tok/s)\n",
      "step  257/1000 | train loss 0.073680 | norm 1.2053 | lr 2.54e-05 | (113.30 ms | 36151 tok/s)\n",
      "step  258/1000 | train loss 0.073592 | norm 1.2932 | lr 2.54e-05 | (113.42 ms | 36113 tok/s)\n",
      "step  259/1000 | train loss 0.073847 | norm 1.5620 | lr 2.53e-05 | (115.34 ms | 35513 tok/s)\n",
      "step  260/1000 | train loss 0.073395 | norm 1.0126 | lr 2.53e-05 | (113.17 ms | 36194 tok/s)\n",
      "step  261/1000 | train loss 0.073681 | norm 1.4117 | lr 2.53e-05 | (115.00 ms | 35617 tok/s)\n",
      "step  262/1000 | train loss 0.073533 | norm 1.1112 | lr 2.52e-05 | (114.71 ms | 35708 tok/s)\n",
      "step  263/1000 | train loss 0.073265 | norm 1.0553 | lr 2.52e-05 | (113.25 ms | 36167 tok/s)\n",
      "step  264/1000 | train loss 0.073627 | norm 1.4244 | lr 2.52e-05 | (115.67 ms | 35411 tok/s)\n",
      "step  265/1000 | train loss 0.073393 | norm 1.1502 | lr 2.51e-05 | (113.97 ms | 35940 tok/s)\n",
      "step  266/1000 | train loss 0.073371 | norm 1.2378 | lr 2.51e-05 | (115.36 ms | 35508 tok/s)\n",
      "step  267/1000 | train loss 0.073456 | norm 1.3429 | lr 2.51e-05 | (115.58 ms | 35437 tok/s)\n",
      "step  268/1000 | train loss 0.073232 | norm 1.0880 | lr 2.50e-05 | (114.18 ms | 35873 tok/s)\n",
      "step  269/1000 | train loss 0.073065 | norm 1.0470 | lr 2.50e-05 | (115.68 ms | 35409 tok/s)\n",
      "step  270/1000 | train loss 0.073366 | norm 1.3871 | lr 2.50e-05 | (115.17 ms | 35566 tok/s)\n",
      "step  271/1000 | train loss 0.072955 | norm 0.9234 | lr 2.49e-05 | (116.76 ms | 35081 tok/s)\n",
      "step  272/1000 | train loss 0.072999 | norm 1.0576 | lr 2.49e-05 | (113.92 ms | 35955 tok/s)\n",
      "step  273/1000 | train loss 0.073303 | norm 1.3097 | lr 2.48e-05 | (114.19 ms | 35871 tok/s)\n",
      "step  274/1000 | train loss 0.072752 | norm 0.7891 | lr 2.48e-05 | (114.99 ms | 35620 tok/s)\n",
      "step  275/1000 | train loss 0.073108 | norm 1.2102 | lr 2.48e-05 | (115.38 ms | 35499 tok/s)\n",
      "step  276/1000 | train loss 0.072906 | norm 1.0665 | lr 2.47e-05 | (114.91 ms | 35647 tok/s)\n",
      "step  277/1000 | train loss 0.073055 | norm 1.3533 | lr 2.47e-05 | (115.53 ms | 35453 tok/s)\n",
      "step  278/1000 | train loss 0.072926 | norm 1.1866 | lr 2.47e-05 | (115.13 ms | 35578 tok/s)\n",
      "step  279/1000 | train loss 0.072886 | norm 1.1572 | lr 2.46e-05 | (115.69 ms | 35404 tok/s)\n",
      "step  280/1000 | train loss 0.072744 | norm 1.1168 | lr 2.46e-05 | (115.79 ms | 35373 tok/s)\n",
      "step  281/1000 | train loss 0.072903 | norm 1.2281 | lr 2.46e-05 | (116.22 ms | 35245 tok/s)\n",
      "step  282/1000 | train loss 0.072590 | norm 0.9408 | lr 2.45e-05 | (115.49 ms | 35467 tok/s)\n",
      "step  283/1000 | train loss 0.072829 | norm 1.2572 | lr 2.45e-05 | (115.71 ms | 35398 tok/s)\n",
      "step  284/1000 | train loss 0.072505 | norm 0.9071 | lr 2.45e-05 | (113.11 ms | 36212 tok/s)\n",
      "step  285/1000 | train loss 0.072678 | norm 1.1934 | lr 2.44e-05 | (114.01 ms | 35927 tok/s)\n",
      "step  286/1000 | train loss 0.072538 | norm 1.0268 | lr 2.44e-05 | (114.75 ms | 35694 tok/s)\n",
      "step  287/1000 | train loss 0.072622 | norm 1.2291 | lr 2.43e-05 | (115.68 ms | 35409 tok/s)\n",
      "step  288/1000 | train loss 0.072392 | norm 0.9473 | lr 2.43e-05 | (115.41 ms | 35490 tok/s)\n",
      "step  289/1000 | train loss 0.072557 | norm 1.1460 | lr 2.43e-05 | (115.46 ms | 35476 tok/s)\n",
      "step  290/1000 | train loss 0.072472 | norm 1.1055 | lr 2.42e-05 | (113.31 ms | 36148 tok/s)\n",
      "step  291/1000 | train loss 0.072518 | norm 1.2290 | lr 2.42e-05 | (113.20 ms | 36184 tok/s)\n",
      "step  292/1000 | train loss 0.072293 | norm 0.9133 | lr 2.42e-05 | (114.67 ms | 35720 tok/s)\n",
      "step  293/1000 | train loss 0.072230 | norm 0.9177 | lr 2.41e-05 | (114.54 ms | 35760 tok/s)\n",
      "step  294/1000 | train loss 0.072390 | norm 1.0616 | lr 2.41e-05 | (116.13 ms | 35270 tok/s)\n",
      "step  295/1000 | train loss 0.072189 | norm 0.9891 | lr 2.40e-05 | (114.98 ms | 35623 tok/s)\n",
      "step  296/1000 | train loss 0.072380 | norm 1.1722 | lr 2.40e-05 | (114.44 ms | 35791 tok/s)\n",
      "step  297/1000 | train loss 0.072105 | norm 0.9129 | lr 2.40e-05 | (114.42 ms | 35798 tok/s)\n",
      "step  298/1000 | train loss 0.072265 | norm 1.1077 | lr 2.39e-05 | (113.85 ms | 35977 tok/s)\n",
      "step  299/1000 | train loss 0.072092 | norm 0.9586 | lr 2.39e-05 | (114.01 ms | 35928 tok/s)\n",
      "step  300/1000 | train loss 0.072060 | norm 0.9498 | lr 2.39e-05 | (114.47 ms | 35781 tok/s)\n",
      "step  301/1000 | train loss 0.071950 | norm 0.8657 | lr 2.38e-05 | (116.86 ms | 35052 tok/s)\n",
      "step  302/1000 | train loss 0.071945 | norm 0.8802 | lr 2.38e-05 | (114.78 ms | 35685 tok/s)\n",
      "step  303/1000 | train loss 0.071928 | norm 0.9414 | lr 2.37e-05 | (114.01 ms | 35926 tok/s)\n",
      "step  304/1000 | train loss 0.072049 | norm 1.0991 | lr 2.37e-05 | (115.08 ms | 35593 tok/s)\n",
      "step  305/1000 | train loss 0.071967 | norm 1.0053 | lr 2.37e-05 | (115.49 ms | 35465 tok/s)\n",
      "step  306/1000 | train loss 0.071852 | norm 0.9571 | lr 2.36e-05 | (115.24 ms | 35543 tok/s)\n",
      "step  307/1000 | train loss 0.071925 | norm 1.1032 | lr 2.36e-05 | (116.54 ms | 35146 tok/s)\n",
      "step  308/1000 | train loss 0.071877 | norm 1.1364 | lr 2.35e-05 | (115.23 ms | 35548 tok/s)\n",
      "step  309/1000 | train loss 0.071709 | norm 0.9190 | lr 2.35e-05 | (113.64 ms | 36042 tok/s)\n",
      "step  310/1000 | train loss 0.071805 | norm 1.0376 | lr 2.35e-05 | (114.64 ms | 35730 tok/s)\n",
      "step  311/1000 | train loss 0.071671 | norm 0.9775 | lr 2.34e-05 | (115.59 ms | 35434 tok/s)\n",
      "step  312/1000 | train loss 0.071860 | norm 1.2006 | lr 2.34e-05 | (114.31 ms | 35832 tok/s)\n",
      "step  313/1000 | train loss 0.071574 | norm 0.8690 | lr 2.34e-05 | (115.63 ms | 35424 tok/s)\n",
      "step  314/1000 | train loss 0.071560 | norm 0.8460 | lr 2.33e-05 | (115.20 ms | 35554 tok/s)\n",
      "step  315/1000 | train loss 0.071620 | norm 0.9919 | lr 2.33e-05 | (115.55 ms | 35448 tok/s)\n",
      "step  316/1000 | train loss 0.071588 | norm 1.0383 | lr 2.32e-05 | (115.13 ms | 35576 tok/s)\n",
      "step  317/1000 | train loss 0.071546 | norm 0.9358 | lr 2.32e-05 | (116.23 ms | 35240 tok/s)\n",
      "step  318/1000 | train loss 0.071385 | norm 0.7549 | lr 2.32e-05 | (113.19 ms | 36188 tok/s)\n",
      "step  319/1000 | train loss 0.071367 | norm 0.7743 | lr 2.31e-05 | (115.57 ms | 35442 tok/s)\n",
      "step  320/1000 | train loss 0.071309 | norm 0.7631 | lr 2.31e-05 | (114.78 ms | 35685 tok/s)\n",
      "step  321/1000 | train loss 0.071340 | norm 0.8182 | lr 2.30e-05 | (115.59 ms | 35436 tok/s)\n",
      "step  322/1000 | train loss 0.071340 | norm 0.8976 | lr 2.30e-05 | (114.54 ms | 35759 tok/s)\n",
      "step  323/1000 | train loss 0.071355 | norm 0.9569 | lr 2.30e-05 | (115.94 ms | 35327 tok/s)\n",
      "step  324/1000 | train loss 0.071401 | norm 1.0631 | lr 2.29e-05 | (115.46 ms | 35475 tok/s)\n",
      "step  325/1000 | train loss 0.071374 | norm 1.0520 | lr 2.29e-05 | (115.47 ms | 35472 tok/s)\n",
      "step  326/1000 | train loss 0.071170 | norm 0.8393 | lr 2.28e-05 | (114.88 ms | 35654 tok/s)\n",
      "step  327/1000 | train loss 0.071269 | norm 1.0027 | lr 2.28e-05 | (118.32 ms | 34619 tok/s)\n",
      "step  328/1000 | train loss 0.071349 | norm 1.1616 | lr 2.28e-05 | (114.09 ms | 35903 tok/s)\n",
      "step  329/1000 | train loss 0.071071 | norm 0.7646 | lr 2.27e-05 | (116.15 ms | 35265 tok/s)\n",
      "step  330/1000 | train loss 0.071083 | norm 0.7890 | lr 2.27e-05 | (115.49 ms | 35466 tok/s)\n",
      "step  331/1000 | train loss 0.071138 | norm 0.9388 | lr 2.26e-05 | (114.46 ms | 35784 tok/s)\n",
      "step  332/1000 | train loss 0.071119 | norm 0.9651 | lr 2.26e-05 | (114.49 ms | 35775 tok/s)\n",
      "step  333/1000 | train loss 0.071039 | norm 0.8614 | lr 2.26e-05 | (115.25 ms | 35541 tok/s)\n",
      "step  334/1000 | train loss 0.070964 | norm 0.7939 | lr 2.25e-05 | (114.67 ms | 35721 tok/s)\n",
      "step  335/1000 | train loss 0.070985 | norm 0.8379 | lr 2.25e-05 | (116.02 ms | 35305 tok/s)\n",
      "step  336/1000 | train loss 0.070923 | norm 0.8162 | lr 2.24e-05 | (115.33 ms | 35516 tok/s)\n",
      "step  337/1000 | train loss 0.070932 | norm 0.8814 | lr 2.24e-05 | (119.26 ms | 34346 tok/s)\n",
      "step  338/1000 | train loss 0.070925 | norm 0.9293 | lr 2.24e-05 | (115.00 ms | 35619 tok/s)\n",
      "step  339/1000 | train loss 0.070979 | norm 1.0000 | lr 2.23e-05 | (115.24 ms | 35544 tok/s)\n",
      "step  340/1000 | train loss 0.070918 | norm 0.9155 | lr 2.23e-05 | (115.48 ms | 35468 tok/s)\n",
      "step  341/1000 | train loss 0.070787 | norm 0.7800 | lr 2.22e-05 | (115.12 ms | 35580 tok/s)\n",
      "step  342/1000 | train loss 0.070837 | norm 0.8623 | lr 2.22e-05 | (115.13 ms | 35576 tok/s)\n",
      "step  343/1000 | train loss 0.070737 | norm 0.7751 | lr 2.21e-05 | (115.40 ms | 35495 tok/s)\n",
      "step  344/1000 | train loss 0.070737 | norm 0.7748 | lr 2.21e-05 | (114.87 ms | 35658 tok/s)\n",
      "step  345/1000 | train loss 0.070662 | norm 0.7159 | lr 2.21e-05 | (115.67 ms | 35411 tok/s)\n",
      "step  346/1000 | train loss 0.070623 | norm 0.7090 | lr 2.20e-05 | (114.36 ms | 35817 tok/s)\n",
      "step  347/1000 | train loss 0.070637 | norm 0.7447 | lr 2.20e-05 | (114.62 ms | 35736 tok/s)\n",
      "step  348/1000 | train loss 0.070549 | norm 0.6866 | lr 2.19e-05 | (114.81 ms | 35677 tok/s)\n",
      "step  349/1000 | train loss 0.070578 | norm 0.7682 | lr 2.19e-05 | (116.89 ms | 35040 tok/s)\n",
      "step  350/1000 | train loss 0.070560 | norm 0.7795 | lr 2.19e-05 | (113.95 ms | 35945 tok/s)\n",
      "step  351/1000 | train loss 0.070487 | norm 0.7488 | lr 2.18e-05 | (112.93 ms | 36270 tok/s)\n",
      "step  352/1000 | train loss 0.070523 | norm 0.8096 | lr 2.18e-05 | (116.87 ms | 35047 tok/s)\n",
      "step  353/1000 | train loss 0.070466 | norm 0.7918 | lr 2.17e-05 | (116.52 ms | 35151 tok/s)\n",
      "step  354/1000 | train loss 0.070489 | norm 0.8341 | lr 2.17e-05 | (116.49 ms | 35161 tok/s)\n",
      "step  355/1000 | train loss 0.070483 | norm 0.8607 | lr 2.16e-05 | (114.97 ms | 35627 tok/s)\n",
      "step  356/1000 | train loss 0.070539 | norm 0.9530 | lr 2.16e-05 | (113.63 ms | 36048 tok/s)\n",
      "step  357/1000 | train loss 0.070500 | norm 0.9374 | lr 2.16e-05 | (116.65 ms | 35114 tok/s)\n",
      "step  358/1000 | train loss 0.070421 | norm 0.8708 | lr 2.15e-05 | (114.11 ms | 35895 tok/s)\n",
      "step  359/1000 | train loss 0.070474 | norm 0.9884 | lr 2.15e-05 | (115.85 ms | 35355 tok/s)\n",
      "step  360/1000 | train loss 0.070534 | norm 1.0589 | lr 2.14e-05 | (114.02 ms | 35924 tok/s)\n",
      "step  361/1000 | train loss 0.070387 | norm 0.8616 | lr 2.14e-05 | (114.59 ms | 35745 tok/s)\n",
      "step  362/1000 | train loss 0.070343 | norm 0.8171 | lr 2.13e-05 | (113.50 ms | 36087 tok/s)\n",
      "step  363/1000 | train loss 0.070309 | norm 0.8398 | lr 2.13e-05 | (116.22 ms | 35242 tok/s)\n",
      "step  364/1000 | train loss 0.070397 | norm 0.9062 | lr 2.13e-05 | (153.96 ms | 26605 tok/s)\n",
      "step  365/1000 | train loss 0.070202 | norm 0.6844 | lr 2.12e-05 | (118.06 ms | 34695 tok/s)\n",
      "step  366/1000 | train loss 0.070228 | norm 0.7482 | lr 2.12e-05 | (116.36 ms | 35202 tok/s)\n",
      "step  367/1000 | train loss 0.070238 | norm 0.7480 | lr 2.11e-05 | (115.71 ms | 35398 tok/s)\n",
      "step  368/1000 | train loss 0.070063 | norm 0.5883 | lr 2.11e-05 | (115.21 ms | 35552 tok/s)\n",
      "step  369/1000 | train loss 0.070266 | norm 0.8446 | lr 2.10e-05 | (115.84 ms | 35358 tok/s)\n",
      "step  370/1000 | train loss 0.070141 | norm 0.7824 | lr 2.10e-05 | (116.56 ms | 35142 tok/s)\n",
      "step  371/1000 | train loss 0.070199 | norm 0.8600 | lr 2.10e-05 | (116.88 ms | 35043 tok/s)\n",
      "step  372/1000 | train loss 0.070111 | norm 0.8251 | lr 2.09e-05 | (115.17 ms | 35566 tok/s)\n",
      "step  373/1000 | train loss 0.070183 | norm 0.8757 | lr 2.09e-05 | (116.02 ms | 35303 tok/s)\n",
      "step  374/1000 | train loss 0.069977 | norm 0.6483 | lr 2.08e-05 | (115.76 ms | 35384 tok/s)\n",
      "step  375/1000 | train loss 0.069971 | norm 0.6116 | lr 2.08e-05 | (114.53 ms | 35765 tok/s)\n",
      "step  376/1000 | train loss 0.069947 | norm 0.6577 | lr 2.07e-05 | (113.07 ms | 36227 tok/s)\n",
      "step  377/1000 | train loss 0.069989 | norm 0.7470 | lr 2.07e-05 | (114.69 ms | 35713 tok/s)\n",
      "step  378/1000 | train loss 0.069926 | norm 0.7033 | lr 2.07e-05 | (117.58 ms | 34836 tok/s)\n",
      "step  379/1000 | train loss 0.069929 | norm 0.7289 | lr 2.06e-05 | (114.89 ms | 35650 tok/s)\n",
      "step  380/1000 | train loss 0.069925 | norm 0.7640 | lr 2.06e-05 | (115.42 ms | 35487 tok/s)\n",
      "step  381/1000 | train loss 0.069941 | norm 0.8404 | lr 2.05e-05 | (116.46 ms | 35172 tok/s)\n",
      "step  382/1000 | train loss 0.069950 | norm 0.8781 | lr 2.05e-05 | (117.56 ms | 34841 tok/s)\n",
      "step  383/1000 | train loss 0.069839 | norm 0.7505 | lr 2.04e-05 | (115.10 ms | 35586 tok/s)\n",
      "step  384/1000 | train loss 0.069804 | norm 0.6716 | lr 2.04e-05 | (113.19 ms | 36185 tok/s)\n",
      "step  385/1000 | train loss 0.069726 | norm 0.5945 | lr 2.03e-05 | (116.51 ms | 35155 tok/s)\n",
      "step  386/1000 | train loss 0.069745 | norm 0.6275 | lr 2.03e-05 | (114.59 ms | 35744 tok/s)\n",
      "step  387/1000 | train loss 0.069699 | norm 0.6459 | lr 2.03e-05 | (114.40 ms | 35803 tok/s)\n",
      "step  388/1000 | train loss 0.069775 | norm 0.7888 | lr 2.02e-05 | (114.97 ms | 35625 tok/s)\n",
      "step  389/1000 | train loss 0.069784 | norm 0.8682 | lr 2.02e-05 | (114.04 ms | 35918 tok/s)\n",
      "step  390/1000 | train loss 0.069913 | norm 1.1306 | lr 2.01e-05 | (115.42 ms | 35486 tok/s)\n",
      "step  391/1000 | train loss 0.069844 | norm 1.0195 | lr 2.01e-05 | (115.08 ms | 35591 tok/s)\n",
      "step  392/1000 | train loss 0.069781 | norm 0.9036 | lr 2.00e-05 | (115.55 ms | 35447 tok/s)\n",
      "step  393/1000 | train loss 0.069751 | norm 0.8900 | lr 2.00e-05 | (115.91 ms | 35339 tok/s)\n",
      "step  394/1000 | train loss 0.069799 | norm 0.9996 | lr 1.99e-05 | (115.45 ms | 35477 tok/s)\n",
      "step  395/1000 | train loss 0.069680 | norm 0.8500 | lr 1.99e-05 | (115.55 ms | 35449 tok/s)\n",
      "step  396/1000 | train loss 0.069577 | norm 0.6695 | lr 1.99e-05 | (113.45 ms | 36103 tok/s)\n",
      "step  397/1000 | train loss 0.069618 | norm 0.8016 | lr 1.98e-05 | (115.81 ms | 35369 tok/s)\n",
      "step  398/1000 | train loss 0.069586 | norm 0.7317 | lr 1.98e-05 | (114.65 ms | 35726 tok/s)\n",
      "step  399/1000 | train loss 0.069478 | norm 0.6142 | lr 1.97e-05 | (114.98 ms | 35622 tok/s)\n",
      "step  400/1000 | train loss 0.069545 | norm 0.7472 | lr 1.97e-05 | (115.94 ms | 35330 tok/s)\n",
      "step  401/1000 | train loss 0.069404 | norm 0.5567 | lr 1.96e-05 | (115.53 ms | 35455 tok/s)\n",
      "step  402/1000 | train loss 0.069456 | norm 0.6336 | lr 1.96e-05 | (115.43 ms | 35484 tok/s)\n",
      "step  403/1000 | train loss 0.069385 | norm 0.5957 | lr 1.95e-05 | (114.46 ms | 35784 tok/s)\n",
      "step  404/1000 | train loss 0.069406 | norm 0.6561 | lr 1.95e-05 | (115.26 ms | 35537 tok/s)\n",
      "step  405/1000 | train loss 0.069446 | norm 0.7789 | lr 1.95e-05 | (117.23 ms | 34941 tok/s)\n",
      "step  406/1000 | train loss 0.069468 | norm 0.8259 | lr 1.94e-05 | (117.05 ms | 34994 tok/s)\n",
      "step  407/1000 | train loss 0.069478 | norm 0.8462 | lr 1.94e-05 | (117.22 ms | 34944 tok/s)\n",
      "step  408/1000 | train loss 0.069384 | norm 0.7330 | lr 1.93e-05 | (115.54 ms | 35451 tok/s)\n",
      "step  409/1000 | train loss 0.069251 | norm 0.5319 | lr 1.93e-05 | (117.07 ms | 34987 tok/s)\n",
      "step  410/1000 | train loss 0.069304 | norm 0.6314 | lr 1.92e-05 | (113.94 ms | 35949 tok/s)\n",
      "step  411/1000 | train loss 0.069252 | norm 0.6355 | lr 1.92e-05 | (114.68 ms | 35717 tok/s)\n",
      "step  412/1000 | train loss 0.069277 | norm 0.6921 | lr 1.91e-05 | (116.28 ms | 35224 tok/s)\n",
      "step  413/1000 | train loss 0.069256 | norm 0.6553 | lr 1.91e-05 | (114.77 ms | 35689 tok/s)\n",
      "step  414/1000 | train loss 0.069153 | norm 0.5037 | lr 1.91e-05 | (114.39 ms | 35808 tok/s)\n",
      "step  415/1000 | train loss 0.069166 | norm 0.5497 | lr 1.90e-05 | (115.87 ms | 35351 tok/s)\n",
      "step  416/1000 | train loss 0.069169 | norm 0.6076 | lr 1.90e-05 | (116.05 ms | 35295 tok/s)\n",
      "step  417/1000 | train loss 0.069174 | norm 0.6821 | lr 1.89e-05 | (116.87 ms | 35048 tok/s)\n",
      "step  418/1000 | train loss 0.069168 | norm 0.7161 | lr 1.89e-05 | (116.40 ms | 35190 tok/s)\n",
      "step  419/1000 | train loss 0.069157 | norm 0.7181 | lr 1.88e-05 | (115.93 ms | 35331 tok/s)\n",
      "step  420/1000 | train loss 0.069132 | norm 0.7010 | lr 1.88e-05 | (114.01 ms | 35928 tok/s)\n",
      "step  421/1000 | train loss 0.069089 | norm 0.6808 | lr 1.87e-05 | (118.93 ms | 34440 tok/s)\n",
      "step  422/1000 | train loss 0.069135 | norm 0.7823 | lr 1.87e-05 | (115.56 ms | 35446 tok/s)\n",
      "step  423/1000 | train loss 0.069190 | norm 0.9122 | lr 1.86e-05 | (114.37 ms | 35813 tok/s)\n",
      "step  424/1000 | train loss 0.069199 | norm 0.9288 | lr 1.86e-05 | (115.99 ms | 35313 tok/s)\n",
      "step  425/1000 | train loss 0.069113 | norm 0.8103 | lr 1.85e-05 | (117.63 ms | 34821 tok/s)\n",
      "step  426/1000 | train loss 0.069034 | norm 0.6908 | lr 1.85e-05 | (115.68 ms | 35408 tok/s)\n",
      "step  427/1000 | train loss 0.069046 | norm 0.6803 | lr 1.85e-05 | (115.09 ms | 35590 tok/s)\n",
      "step  428/1000 | train loss 0.068940 | norm 0.5772 | lr 1.84e-05 | (113.64 ms | 36043 tok/s)\n",
      "step  429/1000 | train loss 0.069018 | norm 0.7144 | lr 1.84e-05 | (118.02 ms | 34707 tok/s)\n",
      "step  430/1000 | train loss 0.068987 | norm 0.6814 | lr 1.83e-05 | (113.26 ms | 36164 tok/s)\n",
      "step  431/1000 | train loss 0.068886 | norm 0.5189 | lr 1.83e-05 | (113.52 ms | 36081 tok/s)\n",
      "step  432/1000 | train loss 0.068908 | norm 0.5885 | lr 1.82e-05 | (115.95 ms | 35326 tok/s)\n",
      "step  433/1000 | train loss 0.068876 | norm 0.5641 | lr 1.82e-05 | (116.53 ms | 35149 tok/s)\n",
      "step  434/1000 | train loss 0.068815 | norm 0.4737 | lr 1.81e-05 | (116.41 ms | 35185 tok/s)\n",
      "step  435/1000 | train loss 0.068827 | norm 0.5314 | lr 1.81e-05 | (115.81 ms | 35368 tok/s)\n",
      "step  436/1000 | train loss 0.068781 | norm 0.4867 | lr 1.80e-05 | (115.65 ms | 35416 tok/s)\n",
      "step  437/1000 | train loss 0.068770 | norm 0.4741 | lr 1.80e-05 | (116.23 ms | 35240 tok/s)\n",
      "step  438/1000 | train loss 0.068750 | norm 0.4972 | lr 1.80e-05 | (113.35 ms | 36136 tok/s)\n",
      "step  439/1000 | train loss 0.068776 | norm 0.5741 | lr 1.79e-05 | (116.10 ms | 35279 tok/s)\n",
      "step  440/1000 | train loss 0.068799 | norm 0.6712 | lr 1.79e-05 | (114.35 ms | 35819 tok/s)\n",
      "step  441/1000 | train loss 0.068862 | norm 0.8321 | lr 1.78e-05 | (115.96 ms | 35322 tok/s)\n",
      "step  442/1000 | train loss 0.068872 | norm 0.8611 | lr 1.78e-05 | (115.04 ms | 35606 tok/s)\n",
      "step  443/1000 | train loss 0.068765 | norm 0.7213 | lr 1.77e-05 | (115.95 ms | 35327 tok/s)\n",
      "step  444/1000 | train loss 0.068757 | norm 0.6919 | lr 1.77e-05 | (115.05 ms | 35603 tok/s)\n",
      "step  445/1000 | train loss 0.068717 | norm 0.6656 | lr 1.76e-05 | (115.23 ms | 35547 tok/s)\n",
      "step  446/1000 | train loss 0.068756 | norm 0.7365 | lr 1.76e-05 | (115.97 ms | 35320 tok/s)\n",
      "step  447/1000 | train loss 0.068728 | norm 0.7352 | lr 1.75e-05 | (116.61 ms | 35125 tok/s)\n",
      "step  448/1000 | train loss 0.068675 | norm 0.6739 | lr 1.75e-05 | (115.09 ms | 35588 tok/s)\n",
      "step  449/1000 | train loss 0.068692 | norm 0.7308 | lr 1.74e-05 | (116.03 ms | 35302 tok/s)\n",
      "step  450/1000 | train loss 0.068654 | norm 0.6941 | lr 1.74e-05 | (115.63 ms | 35424 tok/s)\n",
      "step  451/1000 | train loss 0.068576 | norm 0.5526 | lr 1.73e-05 | (115.89 ms | 35345 tok/s)\n",
      "step  452/1000 | train loss 0.068542 | norm 0.5121 | lr 1.73e-05 | (113.64 ms | 36043 tok/s)\n",
      "step  453/1000 | train loss 0.068576 | norm 0.5986 | lr 1.73e-05 | (114.71 ms | 35706 tok/s)\n",
      "step  454/1000 | train loss 0.068515 | norm 0.5502 | lr 1.72e-05 | (113.87 ms | 35972 tok/s)\n",
      "step  455/1000 | train loss 0.068534 | norm 0.5796 | lr 1.72e-05 | (116.21 ms | 35245 tok/s)\n",
      "step  456/1000 | train loss 0.068544 | norm 0.6320 | lr 1.71e-05 | (114.60 ms | 35742 tok/s)\n",
      "step  457/1000 | train loss 0.068563 | norm 0.6833 | lr 1.71e-05 | (116.05 ms | 35296 tok/s)\n",
      "step  458/1000 | train loss 0.068543 | norm 0.6853 | lr 1.70e-05 | (115.62 ms | 35425 tok/s)\n",
      "step  459/1000 | train loss 0.068487 | norm 0.6494 | lr 1.70e-05 | (117.22 ms | 34944 tok/s)\n",
      "step  460/1000 | train loss 0.068516 | norm 0.7260 | lr 1.69e-05 | (116.24 ms | 35238 tok/s)\n",
      "step  461/1000 | train loss 0.068501 | norm 0.7300 | lr 1.69e-05 | (116.27 ms | 35228 tok/s)\n",
      "step  462/1000 | train loss 0.068441 | norm 0.6161 | lr 1.68e-05 | (113.50 ms | 36088 tok/s)\n",
      "step  463/1000 | train loss 0.068373 | norm 0.4966 | lr 1.68e-05 | (116.47 ms | 35169 tok/s)\n",
      "step  464/1000 | train loss 0.068387 | norm 0.5558 | lr 1.67e-05 | (115.09 ms | 35591 tok/s)\n",
      "step  465/1000 | train loss 0.068381 | norm 0.5569 | lr 1.67e-05 | (116.26 ms | 35232 tok/s)\n",
      "step  466/1000 | train loss 0.068337 | norm 0.4860 | lr 1.66e-05 | (116.02 ms | 35304 tok/s)\n",
      "step  467/1000 | train loss 0.068325 | norm 0.5062 | lr 1.66e-05 | (115.39 ms | 35496 tok/s)\n",
      "step  468/1000 | train loss 0.068329 | norm 0.5401 | lr 1.66e-05 | (115.92 ms | 35335 tok/s)\n",
      "step  469/1000 | train loss 0.068295 | norm 0.4714 | lr 1.65e-05 | (115.67 ms | 35410 tok/s)\n",
      "step  470/1000 | train loss 0.068244 | norm 0.3958 | lr 1.65e-05 | (116.48 ms | 35166 tok/s)\n",
      "step  471/1000 | train loss 0.068266 | norm 0.4750 | lr 1.64e-05 | (117.14 ms | 34968 tok/s)\n",
      "step  472/1000 | train loss 0.068265 | norm 0.5298 | lr 1.64e-05 | (113.06 ms | 36229 tok/s)\n",
      "step  473/1000 | train loss 0.068283 | norm 0.5902 | lr 1.63e-05 | (117.59 ms | 34832 tok/s)\n",
      "step  474/1000 | train loss 0.068277 | norm 0.6479 | lr 1.63e-05 | (114.88 ms | 35656 tok/s)\n",
      "step  475/1000 | train loss 0.068334 | norm 0.7334 | lr 1.62e-05 | (115.81 ms | 35369 tok/s)\n",
      "step  476/1000 | train loss 0.068270 | norm 0.6526 | lr 1.62e-05 | (115.63 ms | 35422 tok/s)\n",
      "step  477/1000 | train loss 0.068176 | norm 0.4928 | lr 1.61e-05 | (115.90 ms | 35342 tok/s)\n",
      "step  478/1000 | train loss 0.068187 | norm 0.5527 | lr 1.61e-05 | (114.30 ms | 35837 tok/s)\n",
      "step  479/1000 | train loss 0.068215 | norm 0.6567 | lr 1.60e-05 | (114.08 ms | 35906 tok/s)\n",
      "step  480/1000 | train loss 0.068210 | norm 0.6758 | lr 1.60e-05 | (116.39 ms | 35192 tok/s)\n",
      "step  481/1000 | train loss 0.068202 | norm 0.6657 | lr 1.59e-05 | (115.43 ms | 35484 tok/s)\n",
      "step  482/1000 | train loss 0.068145 | norm 0.5776 | lr 1.59e-05 | (115.58 ms | 35438 tok/s)\n",
      "step  483/1000 | train loss 0.068136 | norm 0.5728 | lr 1.58e-05 | (115.51 ms | 35460 tok/s)\n",
      "step  484/1000 | train loss 0.068165 | norm 0.6599 | lr 1.58e-05 | (115.16 ms | 35569 tok/s)\n",
      "step  485/1000 | train loss 0.068154 | norm 0.6534 | lr 1.58e-05 | (115.72 ms | 35397 tok/s)\n",
      "step  486/1000 | train loss 0.068075 | norm 0.5265 | lr 1.57e-05 | (116.59 ms | 35131 tok/s)\n",
      "step  487/1000 | train loss 0.068089 | norm 0.5677 | lr 1.57e-05 | (114.12 ms | 35891 tok/s)\n",
      "step  488/1000 | train loss 0.068084 | norm 0.5842 | lr 1.56e-05 | (116.56 ms | 35140 tok/s)\n",
      "step  489/1000 | train loss 0.068042 | norm 0.5220 | lr 1.56e-05 | (116.98 ms | 35015 tok/s)\n",
      "step  490/1000 | train loss 0.068016 | norm 0.4929 | lr 1.55e-05 | (115.05 ms | 35602 tok/s)\n",
      "step  491/1000 | train loss 0.068004 | norm 0.4834 | lr 1.55e-05 | (116.07 ms | 35289 tok/s)\n",
      "step  492/1000 | train loss 0.067976 | norm 0.4572 | lr 1.54e-05 | (116.10 ms | 35279 tok/s)\n",
      "step  493/1000 | train loss 0.067953 | norm 0.4272 | lr 1.54e-05 | (114.95 ms | 35633 tok/s)\n",
      "step  494/1000 | train loss 0.067942 | norm 0.4282 | lr 1.53e-05 | (116.06 ms | 35294 tok/s)\n",
      "step  495/1000 | train loss 0.067934 | norm 0.4531 | lr 1.53e-05 | (116.58 ms | 35134 tok/s)\n",
      "step  496/1000 | train loss 0.067935 | norm 0.4953 | lr 1.52e-05 | (115.29 ms | 35527 tok/s)\n",
      "step  497/1000 | train loss 0.067940 | norm 0.5615 | lr 1.52e-05 | (114.76 ms | 35692 tok/s)\n",
      "step  498/1000 | train loss 0.067994 | norm 0.7150 | lr 1.51e-05 | (115.46 ms | 35476 tok/s)\n",
      "step  499/1000 | train loss 0.068041 | norm 0.8208 | lr 1.51e-05 | (118.41 ms | 34592 tok/s)\n",
      "step  500/1000 | train loss 0.068010 | norm 0.7664 | lr 1.50e-05 | (115.85 ms | 35357 tok/s)\n",
      "step  501/1000 | train loss 0.067923 | norm 0.5854 | lr 1.50e-05 | (114.24 ms | 35854 tok/s)\n",
      "step  502/1000 | train loss 0.067892 | norm 0.5690 | lr 1.50e-05 | (114.56 ms | 35753 tok/s)\n",
      "step  503/1000 | train loss 0.067995 | norm 0.7978 | lr 1.49e-05 | (115.48 ms | 35469 tok/s)\n",
      "step  504/1000 | train loss 0.067958 | norm 0.7561 | lr 1.49e-05 | (113.71 ms | 36021 tok/s)\n",
      "step  505/1000 | train loss 0.067838 | norm 0.4995 | lr 1.48e-05 | (114.98 ms | 35623 tok/s)\n",
      "step  506/1000 | train loss 0.067811 | norm 0.4757 | lr 1.48e-05 | (115.60 ms | 35433 tok/s)\n",
      "step  507/1000 | train loss 0.067826 | norm 0.5738 | lr 1.47e-05 | (114.30 ms | 35834 tok/s)\n",
      "step  508/1000 | train loss 0.067820 | norm 0.5377 | lr 1.47e-05 | (115.69 ms | 35405 tok/s)\n",
      "step  509/1000 | train loss 0.067756 | norm 0.4173 | lr 1.46e-05 | (115.76 ms | 35383 tok/s)\n",
      "step  510/1000 | train loss 0.067764 | norm 0.4818 | lr 1.46e-05 | (116.36 ms | 35201 tok/s)\n",
      "step  511/1000 | train loss 0.067757 | norm 0.4803 | lr 1.45e-05 | (116.63 ms | 35118 tok/s)\n",
      "step  512/1000 | train loss 0.067697 | norm 0.3419 | lr 1.45e-05 | (116.28 ms | 35226 tok/s)\n",
      "step  513/1000 | train loss 0.067706 | norm 0.3966 | lr 1.44e-05 | (116.50 ms | 35158 tok/s)\n",
      "step  514/1000 | train loss 0.067703 | norm 0.4265 | lr 1.44e-05 | (116.97 ms | 35017 tok/s)\n",
      "step  515/1000 | train loss 0.067665 | norm 0.3710 | lr 1.43e-05 | (113.50 ms | 36087 tok/s)\n",
      "step  516/1000 | train loss 0.067677 | norm 0.4097 | lr 1.43e-05 | (116.46 ms | 35171 tok/s)\n",
      "step  517/1000 | train loss 0.067651 | norm 0.4032 | lr 1.42e-05 | (114.54 ms | 35759 tok/s)\n",
      "step  518/1000 | train loss 0.067674 | norm 0.4841 | lr 1.42e-05 | (116.76 ms | 35080 tok/s)\n",
      "step  519/1000 | train loss 0.067671 | norm 0.5314 | lr 1.42e-05 | (116.76 ms | 35082 tok/s)\n",
      "step  520/1000 | train loss 0.067708 | norm 0.6325 | lr 1.41e-05 | (114.97 ms | 35626 tok/s)\n",
      "step  521/1000 | train loss 0.067716 | norm 0.6758 | lr 1.41e-05 | (116.93 ms | 35028 tok/s)\n",
      "step  522/1000 | train loss 0.067655 | norm 0.5737 | lr 1.40e-05 | (113.70 ms | 36026 tok/s)\n",
      "step  523/1000 | train loss 0.067628 | norm 0.5044 | lr 1.40e-05 | (114.44 ms | 35790 tok/s)\n",
      "step  524/1000 | train loss 0.067643 | norm 0.5685 | lr 1.39e-05 | (114.93 ms | 35640 tok/s)\n",
      "step  525/1000 | train loss 0.067650 | norm 0.5999 | lr 1.39e-05 | (115.89 ms | 35343 tok/s)\n",
      "step  526/1000 | train loss 0.067576 | norm 0.4580 | lr 1.38e-05 | (114.13 ms | 35888 tok/s)\n",
      "step  527/1000 | train loss 0.067539 | norm 0.3819 | lr 1.38e-05 | (116.25 ms | 35234 tok/s)\n",
      "step  528/1000 | train loss 0.067562 | norm 0.4666 | lr 1.37e-05 | (114.01 ms | 35925 tok/s)\n",
      "step  529/1000 | train loss 0.067537 | norm 0.4447 | lr 1.37e-05 | (114.46 ms | 35786 tok/s)\n",
      "step  530/1000 | train loss 0.067520 | norm 0.4093 | lr 1.36e-05 | (115.84 ms | 35359 tok/s)\n",
      "step  531/1000 | train loss 0.067505 | norm 0.4071 | lr 1.36e-05 | (115.34 ms | 35514 tok/s)\n",
      "step  532/1000 | train loss 0.067500 | norm 0.4332 | lr 1.35e-05 | (115.09 ms | 35588 tok/s)\n",
      "step  533/1000 | train loss 0.067517 | norm 0.4815 | lr 1.35e-05 | (113.67 ms | 36033 tok/s)\n",
      "step  534/1000 | train loss 0.067503 | norm 0.4909 | lr 1.34e-05 | (115.45 ms | 35477 tok/s)\n",
      "step  535/1000 | train loss 0.067501 | norm 0.5024 | lr 1.34e-05 | (115.76 ms | 35383 tok/s)\n",
      "step  536/1000 | train loss 0.067466 | norm 0.4411 | lr 1.34e-05 | (113.48 ms | 36094 tok/s)\n",
      "step  537/1000 | train loss 0.067443 | norm 0.4160 | lr 1.33e-05 | (114.68 ms | 35717 tok/s)\n",
      "step  538/1000 | train loss 0.067444 | norm 0.4463 | lr 1.33e-05 | (115.93 ms | 35332 tok/s)\n",
      "step  539/1000 | train loss 0.067421 | norm 0.4195 | lr 1.32e-05 | (116.72 ms | 35093 tok/s)\n",
      "step  540/1000 | train loss 0.067428 | norm 0.4560 | lr 1.32e-05 | (114.15 ms | 35884 tok/s)\n",
      "step  541/1000 | train loss 0.067417 | norm 0.4818 | lr 1.31e-05 | (113.99 ms | 35933 tok/s)\n",
      "step  542/1000 | train loss 0.067423 | norm 0.5244 | lr 1.31e-05 | (115.34 ms | 35511 tok/s)\n",
      "step  543/1000 | train loss 0.067429 | norm 0.5687 | lr 1.30e-05 | (114.64 ms | 35728 tok/s)\n",
      "step  544/1000 | train loss 0.067421 | norm 0.5673 | lr 1.30e-05 | (115.12 ms | 35581 tok/s)\n",
      "step  545/1000 | train loss 0.067380 | norm 0.4856 | lr 1.29e-05 | (115.36 ms | 35506 tok/s)\n",
      "step  546/1000 | train loss 0.067355 | norm 0.4277 | lr 1.29e-05 | (115.65 ms | 35417 tok/s)\n",
      "step  547/1000 | train loss 0.067363 | norm 0.4776 | lr 1.28e-05 | (115.87 ms | 35349 tok/s)\n",
      "step  548/1000 | train loss 0.067364 | norm 0.5223 | lr 1.28e-05 | (115.93 ms | 35331 tok/s)\n",
      "step  549/1000 | train loss 0.067354 | norm 0.4956 | lr 1.27e-05 | (114.10 ms | 35898 tok/s)\n",
      "step  550/1000 | train loss 0.067301 | norm 0.3867 | lr 1.27e-05 | (113.95 ms | 35946 tok/s)\n",
      "step  551/1000 | train loss 0.067307 | norm 0.4310 | lr 1.27e-05 | (116.74 ms | 35086 tok/s)\n",
      "step  552/1000 | train loss 0.067286 | norm 0.4051 | lr 1.26e-05 | (115.50 ms | 35464 tok/s)\n",
      "step  553/1000 | train loss 0.067257 | norm 0.3425 | lr 1.26e-05 | (116.03 ms | 35302 tok/s)\n",
      "step  554/1000 | train loss 0.067261 | norm 0.3769 | lr 1.25e-05 | (115.94 ms | 35328 tok/s)\n",
      "step  555/1000 | train loss 0.067238 | norm 0.3519 | lr 1.25e-05 | (116.14 ms | 35266 tok/s)\n",
      "step  556/1000 | train loss 0.067239 | norm 0.3619 | lr 1.24e-05 | (115.08 ms | 35591 tok/s)\n",
      "step  557/1000 | train loss 0.067222 | norm 0.3599 | lr 1.24e-05 | (117.11 ms | 34977 tok/s)\n",
      "step  558/1000 | train loss 0.067212 | norm 0.3552 | lr 1.23e-05 | (115.09 ms | 35589 tok/s)\n",
      "step  559/1000 | train loss 0.067207 | norm 0.3834 | lr 1.23e-05 | (116.72 ms | 35092 tok/s)\n",
      "step  560/1000 | train loss 0.067231 | norm 0.4674 | lr 1.22e-05 | (115.08 ms | 35592 tok/s)\n",
      "step  561/1000 | train loss 0.067228 | norm 0.5311 | lr 1.22e-05 | (115.75 ms | 35388 tok/s)\n",
      "step  562/1000 | train loss 0.067272 | norm 0.6309 | lr 1.21e-05 | (115.58 ms | 35440 tok/s)\n",
      "step  563/1000 | train loss 0.067239 | norm 0.6104 | lr 1.21e-05 | (113.55 ms | 36073 tok/s)\n",
      "step  564/1000 | train loss 0.067232 | norm 0.5871 | lr 1.21e-05 | (115.78 ms | 35376 tok/s)\n",
      "step  565/1000 | train loss 0.067226 | norm 0.5713 | lr 1.20e-05 | (115.59 ms | 35434 tok/s)\n",
      "step  566/1000 | train loss 0.067186 | norm 0.5086 | lr 1.20e-05 | (115.39 ms | 35498 tok/s)\n",
      "step  567/1000 | train loss 0.067173 | norm 0.4959 | lr 1.19e-05 | (116.42 ms | 35183 tok/s)\n",
      "step  568/1000 | train loss 0.067146 | norm 0.4375 | lr 1.19e-05 | (116.23 ms | 35239 tok/s)\n",
      "step  569/1000 | train loss 0.067125 | norm 0.4081 | lr 1.18e-05 | (118.66 ms | 34519 tok/s)\n",
      "step  570/1000 | train loss 0.067144 | norm 0.4634 | lr 1.18e-05 | (113.65 ms | 36042 tok/s)\n",
      "step  571/1000 | train loss 0.067094 | norm 0.3782 | lr 1.17e-05 | (115.53 ms | 35453 tok/s)\n",
      "step  572/1000 | train loss 0.067112 | norm 0.4271 | lr 1.17e-05 | (115.58 ms | 35440 tok/s)\n",
      "step  573/1000 | train loss 0.067082 | norm 0.4052 | lr 1.16e-05 | (116.72 ms | 35092 tok/s)\n",
      "step  574/1000 | train loss 0.067082 | norm 0.4247 | lr 1.16e-05 | (116.23 ms | 35242 tok/s)\n",
      "step  575/1000 | train loss 0.067082 | norm 0.4450 | lr 1.15e-05 | (116.77 ms | 35076 tok/s)\n",
      "step  576/1000 | train loss 0.067050 | norm 0.3831 | lr 1.15e-05 | (115.88 ms | 35347 tok/s)\n",
      "step  577/1000 | train loss 0.067037 | norm 0.3616 | lr 1.15e-05 | (116.41 ms | 35186 tok/s)\n",
      "step  578/1000 | train loss 0.067028 | norm 0.3533 | lr 1.14e-05 | (115.60 ms | 35431 tok/s)\n",
      "step  579/1000 | train loss 0.067011 | norm 0.3289 | lr 1.14e-05 | (116.60 ms | 35128 tok/s)\n",
      "step  580/1000 | train loss 0.067004 | norm 0.3197 | lr 1.13e-05 | (115.83 ms | 35362 tok/s)\n",
      "step  581/1000 | train loss 0.066980 | norm 0.2707 | lr 1.13e-05 | (114.82 ms | 35672 tok/s)\n",
      "step  582/1000 | train loss 0.066980 | norm 0.3156 | lr 1.12e-05 | (115.35 ms | 35511 tok/s)\n",
      "step  583/1000 | train loss 0.067004 | norm 0.4167 | lr 1.12e-05 | (114.66 ms | 35723 tok/s)\n",
      "step  584/1000 | train loss 0.067000 | norm 0.4564 | lr 1.11e-05 | (115.10 ms | 35586 tok/s)\n",
      "step  585/1000 | train loss 0.067019 | norm 0.5132 | lr 1.11e-05 | (114.67 ms | 35721 tok/s)\n",
      "step  586/1000 | train loss 0.066990 | norm 0.4593 | lr 1.10e-05 | (113.73 ms | 36016 tok/s)\n",
      "step  587/1000 | train loss 0.066953 | norm 0.3762 | lr 1.10e-05 | (116.42 ms | 35183 tok/s)\n",
      "step  588/1000 | train loss 0.066959 | norm 0.4030 | lr 1.10e-05 | (114.58 ms | 35747 tok/s)\n",
      "step  589/1000 | train loss 0.066950 | norm 0.4177 | lr 1.09e-05 | (115.59 ms | 35437 tok/s)\n",
      "step  590/1000 | train loss 0.066950 | norm 0.4258 | lr 1.09e-05 | (113.13 ms | 36205 tok/s)\n",
      "step  591/1000 | train loss 0.066915 | norm 0.3455 | lr 1.08e-05 | (116.91 ms | 35035 tok/s)\n",
      "step  592/1000 | train loss 0.066912 | norm 0.3670 | lr 1.08e-05 | (116.19 ms | 35253 tok/s)\n",
      "step  593/1000 | train loss 0.066920 | norm 0.4226 | lr 1.07e-05 | (116.89 ms | 35041 tok/s)\n",
      "step  594/1000 | train loss 0.066899 | norm 0.3948 | lr 1.07e-05 | (115.67 ms | 35410 tok/s)\n",
      "step  595/1000 | train loss 0.066896 | norm 0.4011 | lr 1.06e-05 | (115.66 ms | 35414 tok/s)\n",
      "step  596/1000 | train loss 0.066897 | norm 0.4323 | lr 1.06e-05 | (115.35 ms | 35510 tok/s)\n",
      "step  597/1000 | train loss 0.066901 | norm 0.4540 | lr 1.05e-05 | (116.29 ms | 35221 tok/s)\n",
      "step  598/1000 | train loss 0.066869 | norm 0.3926 | lr 1.05e-05 | (115.80 ms | 35371 tok/s)\n",
      "step  599/1000 | train loss 0.066859 | norm 0.3795 | lr 1.05e-05 | (117.34 ms | 34907 tok/s)\n",
      "step  600/1000 | train loss 0.066865 | norm 0.4143 | lr 1.04e-05 | (114.20 ms | 35868 tok/s)\n",
      "step  601/1000 | train loss 0.066836 | norm 0.3571 | lr 1.04e-05 | (115.46 ms | 35477 tok/s)\n",
      "step  602/1000 | train loss 0.066823 | norm 0.3353 | lr 1.03e-05 | (114.57 ms | 35752 tok/s)\n",
      "step  603/1000 | train loss 0.066831 | norm 0.3802 | lr 1.03e-05 | (114.32 ms | 35831 tok/s)\n",
      "step  604/1000 | train loss 0.066808 | norm 0.3419 | lr 1.02e-05 | (115.69 ms | 35406 tok/s)\n",
      "step  605/1000 | train loss 0.066791 | norm 0.3153 | lr 1.02e-05 | (114.46 ms | 35784 tok/s)\n",
      "step  606/1000 | train loss 0.066796 | norm 0.3454 | lr 1.01e-05 | (115.20 ms | 35557 tok/s)\n",
      "step  607/1000 | train loss 0.066782 | norm 0.3213 | lr 1.01e-05 | (115.42 ms | 35487 tok/s)\n",
      "step  608/1000 | train loss 0.066770 | norm 0.3160 | lr 1.01e-05 | (114.82 ms | 35673 tok/s)\n",
      "step  609/1000 | train loss 0.066770 | norm 0.3492 | lr 1.00e-05 | (115.95 ms | 35327 tok/s)\n",
      "step  610/1000 | train loss 0.066776 | norm 0.4160 | lr 9.97e-06 | (119.10 ms | 34393 tok/s)\n",
      "step  611/1000 | train loss 0.066787 | norm 0.4748 | lr 9.92e-06 | (114.33 ms | 35825 tok/s)\n",
      "step  612/1000 | train loss 0.066775 | norm 0.4727 | lr 9.88e-06 | (116.10 ms | 35279 tok/s)\n",
      "step  613/1000 | train loss 0.066763 | norm 0.4417 | lr 9.83e-06 | (116.23 ms | 35241 tok/s)\n",
      "step  614/1000 | train loss 0.066739 | norm 0.3777 | lr 9.79e-06 | (115.77 ms | 35380 tok/s)\n",
      "step  615/1000 | train loss 0.066723 | norm 0.3307 | lr 9.74e-06 | (115.37 ms | 35502 tok/s)\n",
      "step  616/1000 | train loss 0.066711 | norm 0.3291 | lr 9.70e-06 | (116.20 ms | 35250 tok/s)\n",
      "step  617/1000 | train loss 0.066717 | norm 0.3684 | lr 9.66e-06 | (116.05 ms | 35294 tok/s)\n",
      "step  618/1000 | train loss 0.066695 | norm 0.3239 | lr 9.61e-06 | (116.06 ms | 35291 tok/s)\n",
      "step  619/1000 | train loss 0.066681 | norm 0.2964 | lr 9.57e-06 | (116.51 ms | 35155 tok/s)\n",
      "step  620/1000 | train loss 0.066682 | norm 0.3156 | lr 9.52e-06 | (115.98 ms | 35315 tok/s)\n",
      "step  621/1000 | train loss 0.066667 | norm 0.2899 | lr 9.48e-06 | (115.79 ms | 35375 tok/s)\n",
      "step  622/1000 | train loss 0.066667 | norm 0.3302 | lr 9.44e-06 | (115.28 ms | 35530 tok/s)\n",
      "step  623/1000 | train loss 0.066675 | norm 0.3801 | lr 9.39e-06 | (117.03 ms | 35000 tok/s)\n",
      "step  624/1000 | train loss 0.066651 | norm 0.3439 | lr 9.35e-06 | (116.08 ms | 35287 tok/s)\n",
      "step  625/1000 | train loss 0.066658 | norm 0.3868 | lr 9.31e-06 | (116.33 ms | 35209 tok/s)\n",
      "step  626/1000 | train loss 0.066660 | norm 0.4062 | lr 9.26e-06 | (114.86 ms | 35662 tok/s)\n",
      "step  627/1000 | train loss 0.066623 | norm 0.2888 | lr 9.22e-06 | (115.28 ms | 35531 tok/s)\n",
      "step  628/1000 | train loss 0.066601 | norm 0.2127 | lr 9.17e-06 | (115.90 ms | 35341 tok/s)\n",
      "step  629/1000 | train loss 0.066609 | norm 0.2856 | lr 9.13e-06 | (116.93 ms | 35030 tok/s)\n",
      "step  630/1000 | train loss 0.066609 | norm 0.3347 | lr 9.09e-06 | (113.20 ms | 36183 tok/s)\n",
      "step  631/1000 | train loss 0.066613 | norm 0.3748 | lr 9.04e-06 | (115.62 ms | 35427 tok/s)\n",
      "step  632/1000 | train loss 0.066601 | norm 0.3529 | lr 9.00e-06 | (116.17 ms | 35259 tok/s)\n",
      "step  633/1000 | train loss 0.066597 | norm 0.3537 | lr 8.96e-06 | (116.43 ms | 35180 tok/s)\n",
      "step  634/1000 | train loss 0.066608 | norm 0.4088 | lr 8.92e-06 | (113.70 ms | 36024 tok/s)\n",
      "step  635/1000 | train loss 0.066590 | norm 0.3814 | lr 8.87e-06 | (114.42 ms | 35799 tok/s)\n",
      "step  636/1000 | train loss 0.066567 | norm 0.3116 | lr 8.83e-06 | (115.20 ms | 35556 tok/s)\n",
      "step  637/1000 | train loss 0.066551 | norm 0.2680 | lr 8.79e-06 | (115.43 ms | 35484 tok/s)\n",
      "step  638/1000 | train loss 0.066555 | norm 0.3131 | lr 8.74e-06 | (115.20 ms | 35555 tok/s)\n",
      "step  639/1000 | train loss 0.066554 | norm 0.3328 | lr 8.70e-06 | (115.73 ms | 35394 tok/s)\n",
      "step  640/1000 | train loss 0.066537 | norm 0.2915 | lr 8.66e-06 | (113.54 ms | 36075 tok/s)\n",
      "step  641/1000 | train loss 0.066527 | norm 0.2895 | lr 8.62e-06 | (116.85 ms | 35053 tok/s)\n",
      "step  642/1000 | train loss 0.066533 | norm 0.3380 | lr 8.57e-06 | (114.66 ms | 35723 tok/s)\n",
      "step  643/1000 | train loss 0.066519 | norm 0.3087 | lr 8.53e-06 | (116.37 ms | 35198 tok/s)\n",
      "step  644/1000 | train loss 0.066504 | norm 0.2790 | lr 8.49e-06 | (115.39 ms | 35497 tok/s)\n",
      "step  645/1000 | train loss 0.066511 | norm 0.3398 | lr 8.45e-06 | (116.25 ms | 35236 tok/s)\n",
      "step  646/1000 | train loss 0.066513 | norm 0.3671 | lr 8.40e-06 | (116.16 ms | 35261 tok/s)\n",
      "step  647/1000 | train loss 0.066494 | norm 0.3374 | lr 8.36e-06 | (115.60 ms | 35433 tok/s)\n",
      "step  648/1000 | train loss 0.066494 | norm 0.3647 | lr 8.32e-06 | (115.28 ms | 35530 tok/s)\n",
      "step  649/1000 | train loss 0.066506 | norm 0.4146 | lr 8.28e-06 | (116.24 ms | 35236 tok/s)\n",
      "step  650/1000 | train loss 0.066496 | norm 0.3836 | lr 8.23e-06 | (114.37 ms | 35814 tok/s)\n",
      "step  651/1000 | train loss 0.066470 | norm 0.3172 | lr 8.19e-06 | (114.39 ms | 35806 tok/s)\n",
      "step  652/1000 | train loss 0.066464 | norm 0.3118 | lr 8.15e-06 | (115.32 ms | 35519 tok/s)\n",
      "step  653/1000 | train loss 0.066452 | norm 0.2958 | lr 8.11e-06 | (143.59 ms | 28525 tok/s)\n",
      "step  654/1000 | train loss 0.066441 | norm 0.2802 | lr 8.07e-06 | (114.27 ms | 35843 tok/s)\n",
      "step  655/1000 | train loss 0.066438 | norm 0.2941 | lr 8.02e-06 | (115.54 ms | 35452 tok/s)\n",
      "step  656/1000 | train loss 0.066435 | norm 0.2890 | lr 7.98e-06 | (115.43 ms | 35484 tok/s)\n",
      "step  657/1000 | train loss 0.066408 | norm 0.2003 | lr 7.94e-06 | (114.32 ms | 35830 tok/s)\n",
      "step  658/1000 | train loss 0.066412 | norm 0.2403 | lr 7.90e-06 | (115.56 ms | 35446 tok/s)\n",
      "step  659/1000 | train loss 0.066406 | norm 0.2525 | lr 7.86e-06 | (116.48 ms | 35165 tok/s)\n",
      "step  660/1000 | train loss 0.066395 | norm 0.2366 | lr 7.82e-06 | (115.44 ms | 35482 tok/s)\n",
      "step  661/1000 | train loss 0.066398 | norm 0.2614 | lr 7.78e-06 | (114.60 ms | 35741 tok/s)\n",
      "step  662/1000 | train loss 0.066382 | norm 0.2211 | lr 7.73e-06 | (115.74 ms | 35391 tok/s)\n",
      "step  663/1000 | train loss 0.066379 | norm 0.2382 | lr 7.69e-06 | (116.21 ms | 35245 tok/s)\n",
      "step  664/1000 | train loss 0.066387 | norm 0.3079 | lr 7.65e-06 | (113.12 ms | 36209 tok/s)\n",
      "step  665/1000 | train loss 0.066392 | norm 0.3610 | lr 7.61e-06 | (116.79 ms | 35071 tok/s)\n",
      "step  666/1000 | train loss 0.066392 | norm 0.3880 | lr 7.57e-06 | (113.83 ms | 35983 tok/s)\n",
      "step  667/1000 | train loss 0.066380 | norm 0.3563 | lr 7.53e-06 | (115.85 ms | 35355 tok/s)\n",
      "step  668/1000 | train loss 0.066351 | norm 0.2508 | lr 7.49e-06 | (115.36 ms | 35506 tok/s)\n",
      "step  669/1000 | train loss 0.066345 | norm 0.2395 | lr 7.45e-06 | (115.13 ms | 35578 tok/s)\n",
      "step  670/1000 | train loss 0.066351 | norm 0.3057 | lr 7.41e-06 | (112.91 ms | 36277 tok/s)\n",
      "step  671/1000 | train loss 0.066348 | norm 0.3165 | lr 7.37e-06 | (115.66 ms | 35413 tok/s)\n",
      "step  672/1000 | train loss 0.066326 | norm 0.2342 | lr 7.33e-06 | (115.43 ms | 35486 tok/s)\n",
      "step  673/1000 | train loss 0.066319 | norm 0.2176 | lr 7.29e-06 | (114.64 ms | 35728 tok/s)\n",
      "step  674/1000 | train loss 0.066326 | norm 0.2879 | lr 7.25e-06 | (116.12 ms | 35275 tok/s)\n",
      "step  675/1000 | train loss 0.066319 | norm 0.2891 | lr 7.21e-06 | (117.18 ms | 34954 tok/s)\n",
      "step  676/1000 | train loss 0.066304 | norm 0.2364 | lr 7.16e-06 | (115.94 ms | 35327 tok/s)\n",
      "step  677/1000 | train loss 0.066301 | norm 0.2458 | lr 7.12e-06 | (116.40 ms | 35189 tok/s)\n",
      "step  678/1000 | train loss 0.066301 | norm 0.2787 | lr 7.08e-06 | (115.30 ms | 35523 tok/s)\n",
      "step  679/1000 | train loss 0.066289 | norm 0.2474 | lr 7.04e-06 | (116.77 ms | 35078 tok/s)\n",
      "step  680/1000 | train loss 0.066279 | norm 0.2187 | lr 7.00e-06 | (113.98 ms | 35935 tok/s)\n",
      "step  681/1000 | train loss 0.066280 | norm 0.2414 | lr 6.96e-06 | (116.38 ms | 35194 tok/s)\n",
      "step  682/1000 | train loss 0.066270 | norm 0.2257 | lr 6.93e-06 | (115.81 ms | 35368 tok/s)\n",
      "step  683/1000 | train loss 0.066267 | norm 0.2383 | lr 6.89e-06 | (115.92 ms | 35335 tok/s)\n",
      "step  684/1000 | train loss 0.066270 | norm 0.2794 | lr 6.85e-06 | (115.31 ms | 35520 tok/s)\n",
      "step  685/1000 | train loss 0.066265 | norm 0.2808 | lr 6.81e-06 | (116.01 ms | 35308 tok/s)\n",
      "step  686/1000 | train loss 0.066257 | norm 0.2585 | lr 6.77e-06 | (113.05 ms | 36231 tok/s)\n",
      "step  687/1000 | train loss 0.066247 | norm 0.2462 | lr 6.73e-06 | (118.47 ms | 34573 tok/s)\n",
      "step  688/1000 | train loss 0.066247 | norm 0.2754 | lr 6.69e-06 | (115.93 ms | 35332 tok/s)\n",
      "step  689/1000 | train loss 0.066240 | norm 0.2771 | lr 6.65e-06 | (115.41 ms | 35491 tok/s)\n",
      "step  690/1000 | train loss 0.066237 | norm 0.2853 | lr 6.61e-06 | (114.68 ms | 35718 tok/s)\n",
      "step  691/1000 | train loss 0.066233 | norm 0.2769 | lr 6.57e-06 | (117.41 ms | 34887 tok/s)\n",
      "step  692/1000 | train loss 0.066216 | norm 0.2095 | lr 6.53e-06 | (114.31 ms | 35833 tok/s)\n",
      "step  693/1000 | train loss 0.066206 | norm 0.1799 | lr 6.49e-06 | (116.81 ms | 35066 tok/s)\n",
      "step  694/1000 | train loss 0.066207 | norm 0.2177 | lr 6.45e-06 | (114.34 ms | 35824 tok/s)\n",
      "step  695/1000 | train loss 0.066200 | norm 0.2110 | lr 6.42e-06 | (116.69 ms | 35100 tok/s)\n",
      "step  696/1000 | train loss 0.066197 | norm 0.2191 | lr 6.38e-06 | (114.65 ms | 35726 tok/s)\n",
      "step  697/1000 | train loss 0.066189 | norm 0.2015 | lr 6.34e-06 | (116.55 ms | 35142 tok/s)\n",
      "step  698/1000 | train loss 0.066178 | norm 0.1623 | lr 6.30e-06 | (114.08 ms | 35906 tok/s)\n",
      "step  699/1000 | train loss 0.066180 | norm 0.2118 | lr 6.26e-06 | (115.87 ms | 35350 tok/s)\n",
      "step  700/1000 | train loss 0.066180 | norm 0.2507 | lr 6.22e-06 | (116.02 ms | 35305 tok/s)\n",
      "step  701/1000 | train loss 0.066175 | norm 0.2560 | lr 6.19e-06 | (115.92 ms | 35333 tok/s)\n",
      "step  702/1000 | train loss 0.066180 | norm 0.3020 | lr 6.15e-06 | (115.29 ms | 35528 tok/s)\n",
      "step  703/1000 | train loss 0.066182 | norm 0.3357 | lr 6.11e-06 | (114.74 ms | 35698 tok/s)\n",
      "step  704/1000 | train loss 0.066169 | norm 0.3073 | lr 6.07e-06 | (115.84 ms | 35360 tok/s)\n",
      "step  705/1000 | train loss 0.066162 | norm 0.2926 | lr 6.03e-06 | (114.32 ms | 35829 tok/s)\n",
      "step  706/1000 | train loss 0.066164 | norm 0.3178 | lr 6.00e-06 | (113.53 ms | 36079 tok/s)\n",
      "step  707/1000 | train loss 0.066158 | norm 0.3093 | lr 5.96e-06 | (117.41 ms | 34887 tok/s)\n",
      "step  708/1000 | train loss 0.066148 | norm 0.2807 | lr 5.92e-06 | (113.72 ms | 36019 tok/s)\n",
      "step  709/1000 | train loss 0.066145 | norm 0.2713 | lr 5.88e-06 | (115.40 ms | 35493 tok/s)\n",
      "step  710/1000 | train loss 0.066135 | norm 0.2505 | lr 5.85e-06 | (115.18 ms | 35563 tok/s)\n",
      "step  711/1000 | train loss 0.066127 | norm 0.2274 | lr 5.81e-06 | (115.62 ms | 35426 tok/s)\n",
      "step  712/1000 | train loss 0.066119 | norm 0.2029 | lr 5.77e-06 | (115.20 ms | 35555 tok/s)\n",
      "step  713/1000 | train loss 0.066115 | norm 0.2103 | lr 5.73e-06 | (117.20 ms | 34950 tok/s)\n",
      "step  714/1000 | train loss 0.066109 | norm 0.2031 | lr 5.70e-06 | (115.06 ms | 35598 tok/s)\n",
      "step  715/1000 | train loss 0.066105 | norm 0.1907 | lr 5.66e-06 | (115.72 ms | 35396 tok/s)\n",
      "step  716/1000 | train loss 0.066093 | norm 0.1534 | lr 5.62e-06 | (114.85 ms | 35665 tok/s)\n",
      "step  717/1000 | train loss 0.066094 | norm 0.1819 | lr 5.59e-06 | (116.65 ms | 35115 tok/s)\n",
      "step  718/1000 | train loss 0.066087 | norm 0.1704 | lr 5.55e-06 | (114.18 ms | 35872 tok/s)\n",
      "step  719/1000 | train loss 0.066080 | norm 0.1431 | lr 5.51e-06 | (114.32 ms | 35828 tok/s)\n",
      "step  720/1000 | train loss 0.066075 | norm 0.1335 | lr 5.48e-06 | (118.44 ms | 34584 tok/s)\n",
      "step  721/1000 | train loss 0.066071 | norm 0.1412 | lr 5.44e-06 | (116.19 ms | 35252 tok/s)\n",
      "step  722/1000 | train loss 0.066068 | norm 0.1514 | lr 5.40e-06 | (113.69 ms | 36027 tok/s)\n",
      "step  723/1000 | train loss 0.066062 | norm 0.1406 | lr 5.37e-06 | (116.17 ms | 35257 tok/s)\n",
      "step  724/1000 | train loss 0.066055 | norm 0.1147 | lr 5.33e-06 | (115.86 ms | 35354 tok/s)\n",
      "step  725/1000 | train loss 0.066054 | norm 0.1459 | lr 5.30e-06 | (113.59 ms | 36059 tok/s)\n",
      "step  726/1000 | train loss 0.066053 | norm 0.1758 | lr 5.26e-06 | (116.35 ms | 35203 tok/s)\n",
      "step  727/1000 | train loss 0.066053 | norm 0.2107 | lr 5.22e-06 | (116.38 ms | 35195 tok/s)\n",
      "step  728/1000 | train loss 0.066058 | norm 0.2680 | lr 5.19e-06 | (115.54 ms | 35451 tok/s)\n",
      "step  729/1000 | train loss 0.066059 | norm 0.3028 | lr 5.15e-06 | (115.58 ms | 35439 tok/s)\n",
      "step  730/1000 | train loss 0.066052 | norm 0.2845 | lr 5.12e-06 | (115.37 ms | 35504 tok/s)\n",
      "step  731/1000 | train loss 0.066039 | norm 0.2330 | lr 5.08e-06 | (116.23 ms | 35240 tok/s)\n",
      "step  732/1000 | train loss 0.066038 | norm 0.2508 | lr 5.05e-06 | (117.09 ms | 34981 tok/s)\n",
      "step  733/1000 | train loss 0.066040 | norm 0.2804 | lr 5.01e-06 | (117.53 ms | 34851 tok/s)\n",
      "step  734/1000 | train loss 0.066027 | norm 0.2224 | lr 4.98e-06 | (115.44 ms | 35483 tok/s)\n",
      "step  735/1000 | train loss 0.066016 | norm 0.1592 | lr 4.94e-06 | (116.11 ms | 35277 tok/s)\n",
      "step  736/1000 | train loss 0.066016 | norm 0.2012 | lr 4.91e-06 | (115.19 ms | 35558 tok/s)\n",
      "step  737/1000 | train loss 0.066012 | norm 0.1992 | lr 4.87e-06 | (116.24 ms | 35238 tok/s)\n",
      "step  738/1000 | train loss 0.066002 | norm 0.1503 | lr 4.84e-06 | (115.44 ms | 35483 tok/s)\n",
      "step  739/1000 | train loss 0.066001 | norm 0.1698 | lr 4.80e-06 | (118.54 ms | 34553 tok/s)\n",
      "step  740/1000 | train loss 0.065997 | norm 0.1740 | lr 4.77e-06 | (113.93 ms | 35952 tok/s)\n",
      "step  741/1000 | train loss 0.065988 | norm 0.1265 | lr 4.73e-06 | (115.48 ms | 35470 tok/s)\n",
      "step  742/1000 | train loss 0.065987 | norm 0.1424 | lr 4.70e-06 | (115.09 ms | 35591 tok/s)\n",
      "step  743/1000 | train loss 0.065982 | norm 0.1433 | lr 4.67e-06 | (114.54 ms | 35760 tok/s)\n",
      "step  744/1000 | train loss 0.065978 | norm 0.1319 | lr 4.63e-06 | (115.64 ms | 35422 tok/s)\n",
      "step  745/1000 | train loss 0.065973 | norm 0.1260 | lr 4.60e-06 | (115.47 ms | 35473 tok/s)\n",
      "step  746/1000 | train loss 0.065971 | norm 0.1424 | lr 4.56e-06 | (114.72 ms | 35703 tok/s)\n",
      "step  747/1000 | train loss 0.065968 | norm 0.1494 | lr 4.53e-06 | (115.45 ms | 35479 tok/s)\n",
      "step  748/1000 | train loss 0.065966 | norm 0.1631 | lr 4.50e-06 | (114.89 ms | 35653 tok/s)\n",
      "step  749/1000 | train loss 0.065967 | norm 0.2001 | lr 4.46e-06 | (115.61 ms | 35428 tok/s)\n",
      "step  750/1000 | train loss 0.065965 | norm 0.2153 | lr 4.43e-06 | (115.48 ms | 35470 tok/s)\n",
      "step  751/1000 | train loss 0.065961 | norm 0.2080 | lr 4.40e-06 | (114.91 ms | 35644 tok/s)\n",
      "step  752/1000 | train loss 0.065952 | norm 0.1671 | lr 4.36e-06 | (114.79 ms | 35683 tok/s)\n",
      "step  753/1000 | train loss 0.065944 | norm 0.1312 | lr 4.33e-06 | (114.37 ms | 35814 tok/s)\n",
      "step  754/1000 | train loss 0.065945 | norm 0.1697 | lr 4.30e-06 | (115.64 ms | 35422 tok/s)\n",
      "step  755/1000 | train loss 0.065944 | norm 0.1928 | lr 4.26e-06 | (115.66 ms | 35413 tok/s)\n",
      "step  756/1000 | train loss 0.065936 | norm 0.1578 | lr 4.23e-06 | (116.26 ms | 35233 tok/s)\n",
      "step  757/1000 | train loss 0.065931 | norm 0.1319 | lr 4.20e-06 | (114.89 ms | 35652 tok/s)\n",
      "step  758/1000 | train loss 0.065927 | norm 0.1255 | lr 4.17e-06 | (118.27 ms | 34631 tok/s)\n",
      "step  759/1000 | train loss 0.065925 | norm 0.1469 | lr 4.13e-06 | (116.44 ms | 35177 tok/s)\n",
      "step  760/1000 | train loss 0.065923 | norm 0.1558 | lr 4.10e-06 | (115.21 ms | 35553 tok/s)\n",
      "step  761/1000 | train loss 0.065917 | norm 0.1295 | lr 4.07e-06 | (117.29 ms | 34923 tok/s)\n",
      "step  762/1000 | train loss 0.065913 | norm 0.1303 | lr 4.04e-06 | (115.54 ms | 35450 tok/s)\n",
      "step  763/1000 | train loss 0.065912 | norm 0.1576 | lr 4.00e-06 | (116.46 ms | 35172 tok/s)\n",
      "step  764/1000 | train loss 0.065912 | norm 0.1808 | lr 3.97e-06 | (115.42 ms | 35487 tok/s)\n",
      "step  765/1000 | train loss 0.065909 | norm 0.1851 | lr 3.94e-06 | (116.01 ms | 35306 tok/s)\n",
      "step  766/1000 | train loss 0.065905 | norm 0.1836 | lr 3.91e-06 | (113.49 ms | 36090 tok/s)\n",
      "step  767/1000 | train loss 0.065903 | norm 0.1842 | lr 3.88e-06 | (114.21 ms | 35864 tok/s)\n",
      "step  768/1000 | train loss 0.065899 | norm 0.1824 | lr 3.84e-06 | (116.13 ms | 35270 tok/s)\n",
      "step  769/1000 | train loss 0.065895 | norm 0.1744 | lr 3.81e-06 | (114.29 ms | 35839 tok/s)\n",
      "step  770/1000 | train loss 0.065890 | norm 0.1537 | lr 3.78e-06 | (115.63 ms | 35423 tok/s)\n",
      "step  771/1000 | train loss 0.065886 | norm 0.1474 | lr 3.75e-06 | (116.27 ms | 35228 tok/s)\n",
      "step  772/1000 | train loss 0.065883 | norm 0.1465 | lr 3.72e-06 | (115.08 ms | 35592 tok/s)\n",
      "step  773/1000 | train loss 0.065879 | norm 0.1309 | lr 3.69e-06 | (116.03 ms | 35301 tok/s)\n",
      "step  774/1000 | train loss 0.065874 | norm 0.1126 | lr 3.66e-06 | (115.81 ms | 35368 tok/s)\n",
      "step  775/1000 | train loss 0.065873 | norm 0.1347 | lr 3.63e-06 | (115.27 ms | 35533 tok/s)\n",
      "step  776/1000 | train loss 0.065868 | norm 0.1164 | lr 3.60e-06 | (116.13 ms | 35271 tok/s)\n",
      "step  777/1000 | train loss 0.065865 | norm 0.1066 | lr 3.57e-06 | (114.75 ms | 35697 tok/s)\n",
      "step  778/1000 | train loss 0.065861 | norm 0.1052 | lr 3.54e-06 | (114.66 ms | 35723 tok/s)\n",
      "step  779/1000 | train loss 0.065858 | norm 0.0961 | lr 3.51e-06 | (115.55 ms | 35449 tok/s)\n",
      "step  780/1000 | train loss 0.065855 | norm 0.1036 | lr 3.48e-06 | (115.49 ms | 35467 tok/s)\n",
      "step  781/1000 | train loss 0.065853 | norm 0.1154 | lr 3.44e-06 | (115.74 ms | 35391 tok/s)\n",
      "step  782/1000 | train loss 0.065849 | norm 0.1057 | lr 3.41e-06 | (115.93 ms | 35333 tok/s)\n",
      "step  783/1000 | train loss 0.065847 | norm 0.1113 | lr 3.39e-06 | (116.40 ms | 35190 tok/s)\n",
      "step  784/1000 | train loss 0.065846 | norm 0.1442 | lr 3.36e-06 | (116.16 ms | 35261 tok/s)\n",
      "step  785/1000 | train loss 0.065846 | norm 0.1731 | lr 3.33e-06 | (116.13 ms | 35269 tok/s)\n",
      "step  786/1000 | train loss 0.065845 | norm 0.1908 | lr 3.30e-06 | (115.16 ms | 35568 tok/s)\n",
      "step  787/1000 | train loss 0.065840 | norm 0.1723 | lr 3.27e-06 | (116.78 ms | 35076 tok/s)\n",
      "step  788/1000 | train loss 0.065835 | norm 0.1335 | lr 3.24e-06 | (115.87 ms | 35349 tok/s)\n",
      "step  789/1000 | train loss 0.065831 | norm 0.1252 | lr 3.21e-06 | (119.44 ms | 34292 tok/s)\n",
      "step  790/1000 | train loss 0.065830 | norm 0.1522 | lr 3.18e-06 | (115.81 ms | 35369 tok/s)\n",
      "step  791/1000 | train loss 0.065828 | norm 0.1511 | lr 3.15e-06 | (115.55 ms | 35449 tok/s)\n",
      "step  792/1000 | train loss 0.065823 | norm 0.1194 | lr 3.12e-06 | (114.94 ms | 35636 tok/s)\n",
      "step  793/1000 | train loss 0.065820 | norm 0.1095 | lr 3.09e-06 | (115.76 ms | 35384 tok/s)\n",
      "step  794/1000 | train loss 0.065816 | norm 0.1090 | lr 3.06e-06 | (114.93 ms | 35640 tok/s)\n",
      "step  795/1000 | train loss 0.065814 | norm 0.1075 | lr 3.04e-06 | (116.16 ms | 35262 tok/s)\n",
      "step  796/1000 | train loss 0.065811 | norm 0.1055 | lr 3.01e-06 | (115.71 ms | 35398 tok/s)\n",
      "step  797/1000 | train loss 0.065808 | norm 0.0896 | lr 2.98e-06 | (115.00 ms | 35617 tok/s)\n",
      "step  798/1000 | train loss 0.065805 | norm 0.0868 | lr 2.95e-06 | (114.43 ms | 35796 tok/s)\n",
      "step  799/1000 | train loss 0.065803 | norm 0.0948 | lr 2.92e-06 | (116.14 ms | 35266 tok/s)\n",
      "step  800/1000 | train loss 0.065800 | norm 0.0909 | lr 2.90e-06 | (114.23 ms | 35858 tok/s)\n",
      "step  801/1000 | train loss 0.065797 | norm 0.0718 | lr 2.87e-06 | (116.65 ms | 35113 tok/s)\n",
      "step  802/1000 | train loss 0.065794 | norm 0.0770 | lr 2.84e-06 | (114.04 ms | 35917 tok/s)\n",
      "step  803/1000 | train loss 0.065792 | norm 0.0877 | lr 2.81e-06 | (113.37 ms | 36129 tok/s)\n",
      "step  804/1000 | train loss 0.065790 | norm 0.0857 | lr 2.78e-06 | (114.79 ms | 35683 tok/s)\n",
      "step  805/1000 | train loss 0.065788 | norm 0.0863 | lr 2.76e-06 | (115.60 ms | 35433 tok/s)\n",
      "step  806/1000 | train loss 0.065786 | norm 0.1019 | lr 2.73e-06 | (115.21 ms | 35551 tok/s)\n",
      "step  807/1000 | train loss 0.065786 | norm 0.1316 | lr 2.70e-06 | (115.38 ms | 35501 tok/s)\n",
      "step  808/1000 | train loss 0.065786 | norm 0.1609 | lr 2.68e-06 | (112.79 ms | 36315 tok/s)\n",
      "step  809/1000 | train loss 0.065784 | norm 0.1660 | lr 2.65e-06 | (115.49 ms | 35467 tok/s)\n",
      "step  810/1000 | train loss 0.065779 | norm 0.1348 | lr 2.62e-06 | (114.12 ms | 35893 tok/s)\n",
      "step  811/1000 | train loss 0.065775 | norm 0.1128 | lr 2.60e-06 | (116.13 ms | 35270 tok/s)\n",
      "step  812/1000 | train loss 0.065775 | norm 0.1374 | lr 2.57e-06 | (115.15 ms | 35571 tok/s)\n",
      "step  813/1000 | train loss 0.065772 | norm 0.1277 | lr 2.54e-06 | (116.36 ms | 35202 tok/s)\n",
      "step  814/1000 | train loss 0.065766 | norm 0.0800 | lr 2.52e-06 | (115.89 ms | 35345 tok/s)\n",
      "step  815/1000 | train loss 0.065765 | norm 0.0967 | lr 2.49e-06 | (113.81 ms | 35991 tok/s)\n",
      "step  816/1000 | train loss 0.065764 | norm 0.1108 | lr 2.47e-06 | (115.71 ms | 35399 tok/s)\n",
      "step  817/1000 | train loss 0.065760 | norm 0.0860 | lr 2.44e-06 | (116.39 ms | 35193 tok/s)\n",
      "step  818/1000 | train loss 0.065758 | norm 0.0793 | lr 2.41e-06 | (113.78 ms | 35998 tok/s)\n",
      "step  819/1000 | train loss 0.065756 | norm 0.0858 | lr 2.39e-06 | (115.42 ms | 35488 tok/s)\n",
      "step  820/1000 | train loss 0.065754 | norm 0.0780 | lr 2.36e-06 | (117.05 ms | 34994 tok/s)\n",
      "step  821/1000 | train loss 0.065751 | norm 0.0714 | lr 2.34e-06 | (116.87 ms | 35046 tok/s)\n",
      "step  822/1000 | train loss 0.065749 | norm 0.0702 | lr 2.31e-06 | (113.81 ms | 35989 tok/s)\n",
      "step  823/1000 | train loss 0.065747 | norm 0.0677 | lr 2.29e-06 | (115.98 ms | 35316 tok/s)\n",
      "step  824/1000 | train loss 0.065745 | norm 0.0674 | lr 2.26e-06 | (114.00 ms | 35929 tok/s)\n",
      "step  825/1000 | train loss 0.065743 | norm 0.0622 | lr 2.24e-06 | (115.15 ms | 35572 tok/s)\n",
      "step  826/1000 | train loss 0.065740 | norm 0.0550 | lr 2.21e-06 | (113.67 ms | 36035 tok/s)\n",
      "step  827/1000 | train loss 0.065738 | norm 0.0549 | lr 2.19e-06 | (115.36 ms | 35507 tok/s)\n",
      "step  828/1000 | train loss 0.065736 | norm 0.0547 | lr 2.16e-06 | (115.77 ms | 35382 tok/s)\n",
      "step  829/1000 | train loss 0.065735 | norm 0.0571 | lr 2.14e-06 | (115.31 ms | 35521 tok/s)\n",
      "step  830/1000 | train loss 0.065732 | norm 0.0471 | lr 2.12e-06 | (115.84 ms | 35358 tok/s)\n",
      "step  831/1000 | train loss 0.065730 | norm 0.0441 | lr 2.09e-06 | (114.00 ms | 35930 tok/s)\n",
      "step  832/1000 | train loss 0.065728 | norm 0.0482 | lr 2.07e-06 | (113.61 ms | 36053 tok/s)\n",
      "step  833/1000 | train loss 0.065727 | norm 0.0545 | lr 2.04e-06 | (116.05 ms | 35297 tok/s)\n",
      "step  834/1000 | train loss 0.065725 | norm 0.0574 | lr 2.02e-06 | (116.63 ms | 35121 tok/s)\n",
      "step  835/1000 | train loss 0.065723 | norm 0.0649 | lr 2.00e-06 | (115.83 ms | 35361 tok/s)\n",
      "step  836/1000 | train loss 0.065723 | norm 0.0959 | lr 1.97e-06 | (112.98 ms | 36255 tok/s)\n",
      "step  837/1000 | train loss 0.065723 | norm 0.1268 | lr 1.95e-06 | (118.15 ms | 34669 tok/s)\n",
      "step  838/1000 | train loss 0.065722 | norm 0.1344 | lr 1.93e-06 | (115.95 ms | 35326 tok/s)\n",
      "step  839/1000 | train loss 0.065718 | norm 0.0975 | lr 1.90e-06 | (116.61 ms | 35126 tok/s)\n",
      "step  840/1000 | train loss 0.065715 | norm 0.0651 | lr 1.88e-06 | (145.71 ms | 28111 tok/s)\n",
      "step  841/1000 | train loss 0.065715 | norm 0.1024 | lr 1.86e-06 | (114.54 ms | 35759 tok/s)\n",
      "step  842/1000 | train loss 0.065713 | norm 0.1011 | lr 1.84e-06 | (115.01 ms | 35615 tok/s)\n",
      "step  843/1000 | train loss 0.065709 | norm 0.0588 | lr 1.81e-06 | (116.93 ms | 35029 tok/s)\n",
      "step  844/1000 | train loss 0.065708 | norm 0.0723 | lr 1.79e-06 | (116.43 ms | 35180 tok/s)\n",
      "step  845/1000 | train loss 0.065707 | norm 0.0852 | lr 1.77e-06 | (125.37 ms | 32671 tok/s)\n",
      "step  846/1000 | train loss 0.065704 | norm 0.0595 | lr 1.75e-06 | (119.66 ms | 34230 tok/s)\n",
      "step  847/1000 | train loss 0.065703 | norm 0.0578 | lr 1.72e-06 | (117.05 ms | 34994 tok/s)\n",
      "step  848/1000 | train loss 0.065701 | norm 0.0714 | lr 1.70e-06 | (114.80 ms | 35678 tok/s)\n",
      "step  849/1000 | train loss 0.065699 | norm 0.0578 | lr 1.68e-06 | (117.31 ms | 34917 tok/s)\n",
      "step  850/1000 | train loss 0.065697 | norm 0.0451 | lr 1.66e-06 | (116.65 ms | 35112 tok/s)\n",
      "step  851/1000 | train loss 0.065696 | norm 0.0595 | lr 1.64e-06 | (118.35 ms | 34609 tok/s)\n",
      "step  852/1000 | train loss 0.065695 | norm 0.0503 | lr 1.62e-06 | (115.22 ms | 35550 tok/s)\n",
      "step  853/1000 | train loss 0.065693 | norm 0.0400 | lr 1.60e-06 | (114.64 ms | 35730 tok/s)\n",
      "step  854/1000 | train loss 0.065692 | norm 0.0508 | lr 1.57e-06 | (113.24 ms | 36172 tok/s)\n",
      "step  855/1000 | train loss 0.065690 | norm 0.0444 | lr 1.55e-06 | (115.09 ms | 35591 tok/s)\n",
      "step  856/1000 | train loss 0.065688 | norm 0.0406 | lr 1.53e-06 | (117.14 ms | 34966 tok/s)\n",
      "step  857/1000 | train loss 0.065687 | norm 0.0454 | lr 1.51e-06 | (116.07 ms | 35288 tok/s)\n",
      "step  858/1000 | train loss 0.065686 | norm 0.0457 | lr 1.49e-06 | (116.14 ms | 35268 tok/s)\n",
      "step  859/1000 | train loss 0.065684 | norm 0.0449 | lr 1.47e-06 | (117.07 ms | 34987 tok/s)\n",
      "step  860/1000 | train loss 0.065683 | norm 0.0554 | lr 1.45e-06 | (114.94 ms | 35635 tok/s)\n",
      "step  861/1000 | train loss 0.065682 | norm 0.0694 | lr 1.43e-06 | (116.27 ms | 35229 tok/s)\n",
      "step  862/1000 | train loss 0.065681 | norm 0.0820 | lr 1.41e-06 | (116.29 ms | 35223 tok/s)\n",
      "step  863/1000 | train loss 0.065680 | norm 0.0811 | lr 1.39e-06 | (117.08 ms | 34984 tok/s)\n",
      "step  864/1000 | train loss 0.065678 | norm 0.0615 | lr 1.37e-06 | (115.49 ms | 35468 tok/s)\n",
      "step  865/1000 | train loss 0.065676 | norm 0.0364 | lr 1.35e-06 | (117.14 ms | 34966 tok/s)\n",
      "step  866/1000 | train loss 0.065675 | norm 0.0498 | lr 1.33e-06 | (115.14 ms | 35574 tok/s)\n",
      "step  867/1000 | train loss 0.065674 | norm 0.0642 | lr 1.31e-06 | (115.35 ms | 35510 tok/s)\n",
      "step  868/1000 | train loss 0.065672 | norm 0.0503 | lr 1.29e-06 | (114.94 ms | 35637 tok/s)\n",
      "step  869/1000 | train loss 0.065671 | norm 0.0330 | lr 1.27e-06 | (115.60 ms | 35431 tok/s)\n",
      "step  870/1000 | train loss 0.065670 | norm 0.0415 | lr 1.26e-06 | (114.70 ms | 35710 tok/s)\n",
      "step  871/1000 | train loss 0.065669 | norm 0.0512 | lr 1.24e-06 | (115.07 ms | 35595 tok/s)\n",
      "step  872/1000 | train loss 0.065667 | norm 0.0393 | lr 1.22e-06 | (114.90 ms | 35649 tok/s)\n",
      "step  873/1000 | train loss 0.065666 | norm 0.0300 | lr 1.20e-06 | (115.67 ms | 35411 tok/s)\n",
      "step  874/1000 | train loss 0.065665 | norm 0.0378 | lr 1.18e-06 | (115.52 ms | 35457 tok/s)\n",
      "step  875/1000 | train loss 0.065664 | norm 0.0394 | lr 1.16e-06 | (116.36 ms | 35201 tok/s)\n",
      "step  876/1000 | train loss 0.065662 | norm 0.0330 | lr 1.14e-06 | (113.25 ms | 36168 tok/s)\n",
      "step  877/1000 | train loss 0.065661 | norm 0.0270 | lr 1.13e-06 | (116.15 ms | 35264 tok/s)\n",
      "step  878/1000 | train loss 0.065660 | norm 0.0298 | lr 1.11e-06 | (115.16 ms | 35569 tok/s)\n",
      "step  879/1000 | train loss 0.065659 | norm 0.0329 | lr 1.09e-06 | (117.34 ms | 34909 tok/s)\n",
      "step  880/1000 | train loss 0.065658 | norm 0.0297 | lr 1.07e-06 | (116.40 ms | 35188 tok/s)\n",
      "step  881/1000 | train loss 0.065657 | norm 0.0237 | lr 1.06e-06 | (118.01 ms | 34710 tok/s)\n",
      "step  882/1000 | train loss 0.065656 | norm 0.0264 | lr 1.04e-06 | (114.79 ms | 35681 tok/s)\n",
      "step  883/1000 | train loss 0.065655 | norm 0.0296 | lr 1.02e-06 | (116.05 ms | 35295 tok/s)\n",
      "step  884/1000 | train loss 0.065654 | norm 0.0266 | lr 1.00e-06 | (115.80 ms | 35372 tok/s)\n",
      "step  885/1000 | train loss 0.065652 | norm 0.0240 | lr 9.88e-07 | (114.78 ms | 35685 tok/s)\n",
      "step  886/1000 | train loss 0.065651 | norm 0.0271 | lr 9.71e-07 | (115.85 ms | 35357 tok/s)\n",
      "step  887/1000 | train loss 0.065651 | norm 0.0307 | lr 9.55e-07 | (116.13 ms | 35271 tok/s)\n",
      "step  888/1000 | train loss 0.065650 | norm 0.0344 | lr 9.38e-07 | (115.33 ms | 35515 tok/s)\n",
      "step  889/1000 | train loss 0.065649 | norm 0.0397 | lr 9.22e-07 | (115.02 ms | 35612 tok/s)\n",
      "step  890/1000 | train loss 0.065648 | norm 0.0448 | lr 9.06e-07 | (116.40 ms | 35189 tok/s)\n",
      "step  891/1000 | train loss 0.065647 | norm 0.0456 | lr 8.90e-07 | (116.65 ms | 35114 tok/s)\n",
      "step  892/1000 | train loss 0.065646 | norm 0.0404 | lr 8.74e-07 | (114.77 ms | 35690 tok/s)\n",
      "step  893/1000 | train loss 0.065645 | norm 0.0295 | lr 8.58e-07 | (115.17 ms | 35565 tok/s)\n",
      "step  894/1000 | train loss 0.065644 | norm 0.0270 | lr 8.42e-07 | (116.11 ms | 35276 tok/s)\n",
      "step  895/1000 | train loss 0.065643 | norm 0.0371 | lr 8.27e-07 | (116.44 ms | 35175 tok/s)\n",
      "step  896/1000 | train loss 0.065642 | norm 0.0369 | lr 8.12e-07 | (114.23 ms | 35859 tok/s)\n",
      "step  897/1000 | train loss 0.065641 | norm 0.0256 | lr 7.96e-07 | (115.42 ms | 35487 tok/s)\n",
      "step  898/1000 | train loss 0.065640 | norm 0.0218 | lr 7.81e-07 | (115.83 ms | 35362 tok/s)\n",
      "step  899/1000 | train loss 0.065640 | norm 0.0297 | lr 7.66e-07 | (115.83 ms | 35362 tok/s)\n",
      "step  900/1000 | train loss 0.065639 | norm 0.0289 | lr 7.52e-07 | (115.30 ms | 35525 tok/s)\n",
      "step  901/1000 | train loss 0.065638 | norm 0.0232 | lr 7.37e-07 | (114.94 ms | 35636 tok/s)\n",
      "step  902/1000 | train loss 0.065637 | norm 0.0236 | lr 7.23e-07 | (114.93 ms | 35638 tok/s)\n",
      "step  903/1000 | train loss 0.065636 | norm 0.0244 | lr 7.08e-07 | (116.74 ms | 35086 tok/s)\n",
      "step  904/1000 | train loss 0.065636 | norm 0.0236 | lr 6.94e-07 | (115.83 ms | 35362 tok/s)\n",
      "step  905/1000 | train loss 0.065635 | norm 0.0211 | lr 6.80e-07 | (116.21 ms | 35246 tok/s)\n",
      "step  906/1000 | train loss 0.065634 | norm 0.0213 | lr 6.66e-07 | (116.06 ms | 35291 tok/s)\n",
      "step  907/1000 | train loss 0.065633 | norm 0.0221 | lr 6.52e-07 | (116.29 ms | 35221 tok/s)\n",
      "step  908/1000 | train loss 0.065633 | norm 0.0207 | lr 6.39e-07 | (115.53 ms | 35455 tok/s)\n",
      "step  909/1000 | train loss 0.065632 | norm 0.0193 | lr 6.25e-07 | (116.20 ms | 35249 tok/s)\n",
      "step  910/1000 | train loss 0.065631 | norm 0.0203 | lr 6.12e-07 | (112.98 ms | 36255 tok/s)\n",
      "step  911/1000 | train loss 0.065631 | norm 0.0198 | lr 5.99e-07 | (116.34 ms | 35208 tok/s)\n",
      "step  912/1000 | train loss 0.065630 | norm 0.0182 | lr 5.85e-07 | (115.76 ms | 35383 tok/s)\n",
      "step  913/1000 | train loss 0.065629 | norm 0.0192 | lr 5.73e-07 | (114.50 ms | 35774 tok/s)\n",
      "step  914/1000 | train loss 0.065629 | norm 0.0188 | lr 5.60e-07 | (114.39 ms | 35806 tok/s)\n",
      "step  915/1000 | train loss 0.065628 | norm 0.0180 | lr 5.47e-07 | (114.00 ms | 35930 tok/s)\n",
      "step  916/1000 | train loss 0.065627 | norm 0.0180 | lr 5.35e-07 | (115.23 ms | 35545 tok/s)\n",
      "step  917/1000 | train loss 0.065627 | norm 0.0179 | lr 5.22e-07 | (113.78 ms | 35999 tok/s)\n",
      "step  918/1000 | train loss 0.065626 | norm 0.0179 | lr 5.10e-07 | (114.89 ms | 35653 tok/s)\n",
      "step  919/1000 | train loss 0.065626 | norm 0.0173 | lr 4.98e-07 | (116.55 ms | 35145 tok/s)\n",
      "step  920/1000 | train loss 0.065625 | norm 0.0173 | lr 4.86e-07 | (115.13 ms | 35577 tok/s)\n",
      "step  921/1000 | train loss 0.065624 | norm 0.0177 | lr 4.74e-07 | (117.48 ms | 34865 tok/s)\n",
      "step  922/1000 | train loss 0.065624 | norm 0.0171 | lr 4.63e-07 | (115.42 ms | 35489 tok/s)\n",
      "step  923/1000 | train loss 0.065623 | norm 0.0168 | lr 4.51e-07 | (116.79 ms | 35072 tok/s)\n",
      "step  924/1000 | train loss 0.065623 | norm 0.0172 | lr 4.40e-07 | (114.16 ms | 35880 tok/s)\n",
      "step  925/1000 | train loss 0.065622 | norm 0.0170 | lr 4.28e-07 | (114.78 ms | 35686 tok/s)\n",
      "step  926/1000 | train loss 0.065622 | norm 0.0166 | lr 4.17e-07 | (116.11 ms | 35276 tok/s)\n",
      "step  927/1000 | train loss 0.065621 | norm 0.0168 | lr 4.06e-07 | (116.56 ms | 35142 tok/s)\n",
      "step  928/1000 | train loss 0.065621 | norm 0.0167 | lr 3.96e-07 | (115.20 ms | 35556 tok/s)\n",
      "step  929/1000 | train loss 0.065620 | norm 0.0166 | lr 3.85e-07 | (115.26 ms | 35537 tok/s)\n",
      "step  930/1000 | train loss 0.065620 | norm 0.0166 | lr 3.75e-07 | (115.82 ms | 35366 tok/s)\n",
      "step  931/1000 | train loss 0.065619 | norm 0.0165 | lr 3.64e-07 | (115.48 ms | 35469 tok/s)\n",
      "step  932/1000 | train loss 0.065619 | norm 0.0165 | lr 3.54e-07 | (114.00 ms | 35929 tok/s)\n",
      "step  933/1000 | train loss 0.065618 | norm 0.0165 | lr 3.44e-07 | (116.48 ms | 35165 tok/s)\n",
      "step  934/1000 | train loss 0.065618 | norm 0.0164 | lr 3.34e-07 | (115.38 ms | 35501 tok/s)\n",
      "step  935/1000 | train loss 0.065617 | norm 0.0164 | lr 3.24e-07 | (116.19 ms | 35254 tok/s)\n",
      "step  936/1000 | train loss 0.065617 | norm 0.0163 | lr 3.15e-07 | (114.49 ms | 35776 tok/s)\n",
      "step  937/1000 | train loss 0.065617 | norm 0.0163 | lr 3.05e-07 | (114.25 ms | 35852 tok/s)\n",
      "step  938/1000 | train loss 0.065616 | norm 0.0163 | lr 2.96e-07 | (115.05 ms | 35602 tok/s)\n",
      "step  939/1000 | train loss 0.065616 | norm 0.0162 | lr 2.87e-07 | (116.09 ms | 35283 tok/s)\n",
      "step  940/1000 | train loss 0.065615 | norm 0.0163 | lr 2.78e-07 | (114.86 ms | 35660 tok/s)\n",
      "step  941/1000 | train loss 0.065615 | norm 0.0162 | lr 2.69e-07 | (114.51 ms | 35768 tok/s)\n",
      "step  942/1000 | train loss 0.065615 | norm 0.0162 | lr 2.60e-07 | (115.69 ms | 35406 tok/s)\n",
      "step  943/1000 | train loss 0.065614 | norm 0.0162 | lr 2.51e-07 | (116.31 ms | 35217 tok/s)\n",
      "step  944/1000 | train loss 0.065614 | norm 0.0162 | lr 2.43e-07 | (116.52 ms | 35152 tok/s)\n",
      "step  945/1000 | train loss 0.065614 | norm 0.0162 | lr 2.35e-07 | (116.82 ms | 35063 tok/s)\n",
      "step  946/1000 | train loss 0.065613 | norm 0.0162 | lr 2.26e-07 | (115.41 ms | 35490 tok/s)\n",
      "step  947/1000 | train loss 0.065613 | norm 0.0162 | lr 2.18e-07 | (115.12 ms | 35582 tok/s)\n",
      "step  948/1000 | train loss 0.065613 | norm 0.0161 | lr 2.10e-07 | (114.28 ms | 35843 tok/s)\n",
      "step  949/1000 | train loss 0.065612 | norm 0.0161 | lr 2.03e-07 | (116.37 ms | 35199 tok/s)\n",
      "step  950/1000 | train loss 0.065612 | norm 0.0161 | lr 1.95e-07 | (113.23 ms | 36173 tok/s)\n",
      "step  951/1000 | train loss 0.065612 | norm 0.0161 | lr 1.88e-07 | (116.66 ms | 35112 tok/s)\n",
      "step  952/1000 | train loss 0.065611 | norm 0.0161 | lr 1.80e-07 | (116.52 ms | 35152 tok/s)\n",
      "step  953/1000 | train loss 0.065611 | norm 0.0161 | lr 1.73e-07 | (115.53 ms | 35455 tok/s)\n",
      "step  954/1000 | train loss 0.065611 | norm 0.0161 | lr 1.66e-07 | (114.49 ms | 35777 tok/s)\n",
      "step  955/1000 | train loss 0.065611 | norm 0.0161 | lr 1.59e-07 | (115.01 ms | 35613 tok/s)\n",
      "step  956/1000 | train loss 0.065610 | norm 0.0161 | lr 1.53e-07 | (115.31 ms | 35521 tok/s)\n",
      "step  957/1000 | train loss 0.065610 | norm 0.0161 | lr 1.46e-07 | (116.85 ms | 35052 tok/s)\n",
      "step  958/1000 | train loss 0.065610 | norm 0.0161 | lr 1.40e-07 | (118.08 ms | 34687 tok/s)\n",
      "step  959/1000 | train loss 0.065610 | norm 0.0161 | lr 1.33e-07 | (116.94 ms | 35028 tok/s)\n",
      "step  960/1000 | train loss 0.065610 | norm 0.0161 | lr 1.27e-07 | (115.48 ms | 35470 tok/s)\n",
      "step  961/1000 | train loss 0.065609 | norm 0.0161 | lr 1.21e-07 | (116.61 ms | 35124 tok/s)\n",
      "step  962/1000 | train loss 0.065609 | norm 0.0161 | lr 1.15e-07 | (114.93 ms | 35640 tok/s)\n",
      "step  963/1000 | train loss 0.065609 | norm 0.0161 | lr 1.10e-07 | (116.18 ms | 35255 tok/s)\n",
      "step  964/1000 | train loss 0.065609 | norm 0.0161 | lr 1.04e-07 | (113.26 ms | 36165 tok/s)\n",
      "step  965/1000 | train loss 0.065609 | norm 0.0161 | lr 9.88e-08 | (115.64 ms | 35420 tok/s)\n",
      "step  966/1000 | train loss 0.065608 | norm 0.0161 | lr 9.36e-08 | (114.99 ms | 35621 tok/s)\n",
      "step  967/1000 | train loss 0.065608 | norm 0.0161 | lr 8.85e-08 | (115.49 ms | 35466 tok/s)\n",
      "step  968/1000 | train loss 0.065608 | norm 0.0161 | lr 8.35e-08 | (115.33 ms | 35515 tok/s)\n",
      "step  969/1000 | train loss 0.065608 | norm 0.0161 | lr 7.87e-08 | (115.41 ms | 35490 tok/s)\n",
      "step  970/1000 | train loss 0.065608 | norm 0.0161 | lr 7.41e-08 | (114.89 ms | 35651 tok/s)\n",
      "step  971/1000 | train loss 0.065608 | norm 0.0161 | lr 6.96e-08 | (116.22 ms | 35242 tok/s)\n",
      "step  972/1000 | train loss 0.065608 | norm 0.0161 | lr 6.52e-08 | (112.50 ms | 36409 tok/s)\n",
      "step  973/1000 | train loss 0.065607 | norm 0.0161 | lr 6.10e-08 | (116.15 ms | 35263 tok/s)\n",
      "step  974/1000 | train loss 0.065607 | norm 0.0161 | lr 5.69e-08 | (116.18 ms | 35256 tok/s)\n",
      "step  975/1000 | train loss 0.065607 | norm 0.0161 | lr 5.30e-08 | (115.57 ms | 35441 tok/s)\n",
      "step  976/1000 | train loss 0.065607 | norm 0.0161 | lr 4.92e-08 | (117.00 ms | 35007 tok/s)\n",
      "step  977/1000 | train loss 0.065607 | norm 0.0161 | lr 4.56e-08 | (114.55 ms | 35757 tok/s)\n",
      "step  978/1000 | train loss 0.065607 | norm 0.0161 | lr 4.21e-08 | (114.74 ms | 35698 tok/s)\n",
      "step  979/1000 | train loss 0.065607 | norm 0.0161 | lr 3.88e-08 | (115.40 ms | 35493 tok/s)\n",
      "step  980/1000 | train loss 0.065607 | norm 0.0161 | lr 3.56e-08 | (115.70 ms | 35403 tok/s)\n",
      "step  981/1000 | train loss 0.065607 | norm 0.0161 | lr 3.26e-08 | (115.18 ms | 35561 tok/s)\n",
      "step  982/1000 | train loss 0.065607 | norm 0.0161 | lr 2.97e-08 | (115.43 ms | 35485 tok/s)\n",
      "step  983/1000 | train loss 0.065607 | norm 0.0161 | lr 2.70e-08 | (116.02 ms | 35304 tok/s)\n",
      "step  984/1000 | train loss 0.065607 | norm 0.0161 | lr 2.44e-08 | (115.23 ms | 35548 tok/s)\n",
      "step  985/1000 | train loss 0.065607 | norm 0.0161 | lr 2.19e-08 | (116.07 ms | 35289 tok/s)\n",
      "step  986/1000 | train loss 0.065606 | norm 0.0161 | lr 1.97e-08 | (113.91 ms | 35959 tok/s)\n",
      "step  987/1000 | train loss 0.065606 | norm 0.0161 | lr 1.75e-08 | (117.34 ms | 34908 tok/s)\n",
      "step  988/1000 | train loss 0.065606 | norm 0.0161 | lr 1.55e-08 | (116.10 ms | 35281 tok/s)\n",
      "step  989/1000 | train loss 0.065606 | norm 0.0161 | lr 1.37e-08 | (117.10 ms | 34979 tok/s)\n",
      "step  990/1000 | train loss 0.065606 | norm 0.0161 | lr 1.20e-08 | (115.52 ms | 35458 tok/s)\n",
      "step  991/1000 | train loss 0.065606 | norm 0.0161 | lr 1.04e-08 | (115.43 ms | 35486 tok/s)\n",
      "step  992/1000 | train loss 0.065606 | norm 0.0161 | lr 8.99e-09 | (115.71 ms | 35400 tok/s)\n",
      "step  993/1000 | train loss 0.065606 | norm 0.0161 | lr 7.74e-09 | (114.10 ms | 35898 tok/s)\n",
      "step  994/1000 | train loss 0.065606 | norm 0.0161 | lr 6.63e-09 | (113.48 ms | 36094 tok/s)\n",
      "step  995/1000 | train loss 0.065606 | norm 0.0161 | lr 5.66e-09 | (116.70 ms | 35099 tok/s)\n",
      "step  996/1000 | train loss 0.065606 | norm 0.0161 | lr 4.85e-09 | (114.41 ms | 35800 tok/s)\n",
      "step  997/1000 | train loss 0.065606 | norm 0.0161 | lr 4.18e-09 | (116.60 ms | 35129 tok/s)\n",
      "step  998/1000 | train loss 0.065606 | norm 0.0161 | lr 3.67e-09 | (116.08 ms | 35288 tok/s)\n",
      "step  999/1000 | train loss 0.065606 | norm 0.0161 | lr 3.30e-09 | (113.53 ms | 36077 tok/s)\n",
      "step 1000/1000 | train loss 0.065606 | norm 0.0161 | lr 3.07e-09 | (114.42 ms | 35797 tok/s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "timings = []\n",
    "norm = -1.0   # dummy value to print in inference-only mode\n",
    "for step in range(num_iterations + 1):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == num_iterations)\n",
    "\n",
    "    # once in a while evaluate the validation dataset\n",
    "    if (val_loss_every > 0 \\\n",
    "        and (step % val_loss_every == 0 or last_step)) \\\n",
    "        and (val_loader is not None):\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for _ in range(val_max_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, loss = model(x, y, return_logits=False)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= val_max_steps\n",
    "        # log to console and to file\n",
    "        print0(f\"val loss {val_loss}\")\n",
    "\n",
    "\n",
    "    # once in a while perform model inference on the master process\n",
    "    if (sample_every > 0 \\\n",
    "        and (step % sample_every == 0 or last_step)) \\\n",
    "        and master_process:\n",
    "        model.eval()\n",
    "        # before we end, let's also do one round of inference\n",
    "        # we'll kick off the generation with \"<|endoftext|>\", which designates the start of a new sequence\n",
    "        start_ids = [63]\n",
    "        xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "        max_new_tokens = 32\n",
    "        temperature = 1.0\n",
    "        top_k = 40\n",
    "        yg = raw_model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        print0('---------------')\n",
    "        print0(decode(yg[0].tolist()))\n",
    "        print0('---------------')\n",
    "\n",
    "    # bit confusing: we want to make sure to eval and sample on 0th iteration\n",
    "    # but also after the very last iteration. so we loop for step <= num_iterations\n",
    "    # instead of just < num_iterations (one extra due to <=), only to do\n",
    "    # the validation/sampling one last time, and then we break right here as we're done.\n",
    "    if last_step:\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION BEGIN -----------------\n",
    "    model.train()\n",
    "    # micro-batch loop where we do gradient accumulation to reach desired total batch size\n",
    "    lossf = 0.0 # for getting the mean loss (as simple float) over the accumulation steps\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # fetch a batch\n",
    "        if not overfit_single_batch \\\n",
    "            or (overfit_single_batch and step == 0 and micro_step == 0):\n",
    "            x, y = train_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        with ctx:\n",
    "            _, loss = model(x, y, return_logits=False)\n",
    "            # we have to scale the loss to account for gradient accumulation,\n",
    "            # because the gradients just add on each successive backward().\n",
    "            # addition of gradients corresponds to a SUM in the objective, but\n",
    "            # instead of a SUM we want MEAN, so we scale the loss here\n",
    "            loss = loss / grad_accum_steps\n",
    "            lossf += loss.detach() # keep track of the mean loss\n",
    "        # backward pass\n",
    "        if not inference_only:\n",
    "            loss.backward()\n",
    "\n",
    "    lossf = lossf.item()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    # step the optimizer\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # --------------- TRAINING SECTION END -------------------\n",
    "    # everything that follows now is just diagnostics, prints, logging, etc.\n",
    "\n",
    "    # wait on the CPU for all device work to end so we get accurate per-iteration timings below\n",
    "    if device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "    elif device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    # time and print\n",
    "    t1 = time.time()\n",
    "    # the 0th iteration is often an outlier (much slower) => skip logging it\n",
    "    tokens_per_second = grad_accum_steps * ddp_world_size * B * T / (t1-t0)\n",
    "    print0(f\"step {step+1:4d}/{num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)\")\n",
    "\n",
    "\n",
    "    # keep track of smooth timings, last 20 iterations\n",
    "    if step > 0 and step > num_iterations - 20:\n",
    "        timings.append(t1-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0839,  0.0716,  0.0449, -0.0960,  0.0551],\n",
       "        [-0.0607,  0.0496,  0.0738,  0.0693,  0.0313],\n",
       "        [ 0.0485, -0.0739, -0.0587, -0.0393,  0.0301],\n",
       "        [-0.0583,  0.0514, -0.0589,  0.0752,  0.0701],\n",
       "        [-0.0523, -0.0578, -0.0065, -0.0553, -0.0861]], device='mps:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final 19 iters avg: 115.429ms\n",
      "peak memory consumption: 0 MiB\n"
     ]
    }
   ],
   "source": [
    "# print the average of the last 20 timings, to get something smooth-ish\n",
    "timings = timings[-20:]\n",
    "print0(f\"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
    "print0(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Example input:  \n",
      "Generated output:  gezegeni bile benimle aynı  düşü\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# write an example for generating text\n",
    "sample_text = \" \"\n",
    "sample_tokens = encode(sample_text)\n",
    "sample_tokens = torch.tensor(sample_tokens, dtype=torch.long, device=device)[None, ...]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_out = model.generate(sample_tokens, max_new_tokens=32, temperature=0.95, top_k=10)\n",
    "\n",
    "# print the generated text\n",
    "print0('---------------')\n",
    "print0(f\"Example input: {sample_text}\")\n",
    "print0(f\"Generated output: {decode(sample_out[0].tolist())}\")\n",
    "print0('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
