{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import struct\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch._inductor.config as config\n",
    "import torch.nn as nn\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"Careful there are a few versions of GeLU, this one is the exact one used by OpenAI\"\"\"\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        if FLASH:\n",
    "            # flashattention\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            # this materializes the large (T,T) matrix for all the queries and keys\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = NewGELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1 # special flag for residual scaling.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" @dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768 \"\"\"\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 256\n",
    "    vocab_size: int = 64\n",
    "    n_layer: int = 8\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print0(*args, **kwargs):\n",
    "    # modified print that only prints from the master process\n",
    "    # if this is not a distributed run, it's just a print\n",
    "    if int(os.environ.get(\"RANK\", 0)) == 0:\n",
    "        print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # wte: word token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # wpe: positional embeddings\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # h: transformer blocks\n",
    "            ln_f = nn.LayerNorm(config.n_embd), # ln_f: final layer norm before output\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # lm_head: head for language modeling\n",
    "        self.lm_head.LLMC_SKIP_INIT = 1 # don't init this one, we will tie weights\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights, use a torch rng object to be very careful\n",
    "        self.init_rng = torch.Generator()\n",
    "        self.init_rng.manual_seed(42)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02/math.sqrt(2 * self.config.n_layer)\n",
    "            # we want to skip initializing lm_head, which shares parameters with wte\n",
    "            # and wte was already initialized down below during the Embedding init\n",
    "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "\n",
    "    def forward(self, idx, targets=None, return_logits=True):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        # there are performance reasons why not returning logits is prudent, if not needed\n",
    "        if not return_logits:\n",
    "            logits = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type, zero_stage):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print0(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print0(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        print0(f\"using fused AdamW: {use_fused}\")\n",
    "        if zero_stage == 1:\n",
    "            print0(\"using ZeroRedundancyOptimizer\")\n",
    "            optimizer = ZeroRedundancyOptimizer(**optim_groups[0], optimizer_class=torch.optim.AdamW,\n",
    "                                                lr=learning_rate, betas=betas, fused=use_fused)\n",
    "            optimizer.add_param_group(optim_groups[1])\n",
    "        else:\n",
    "            print0(\"using regular AdamW\")\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peek_data_shard(filename):\n",
    "    # only reads the header, returns header data\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()\n",
    "        tokens = [x for x in tokens]\n",
    "    return len(tokens)\n",
    "\n",
    "def _load_data_shard(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()\n",
    "        tokens = [x for x in tokens]\n",
    "    return tokens\n",
    "\n",
    "class DistributedDataLoader:\n",
    "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # glob files that match the pattern\n",
    "        self.files = sorted(glob.glob(filename_pattern))\n",
    "        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n",
    "\n",
    "        # load and validate all data shards, count number of tokens in total\n",
    "        ntok_total = 0\n",
    "        for fname in self.files:\n",
    "            shard_ntok = _peek_data_shard(fname)\n",
    "            assert shard_ntok >= num_processes * B * T + 1, f\"dataset shard {fname} is too small for the current setting\"\n",
    "            ntok_total += shard_ntok\n",
    "        self.ntok_total = ntok_total\n",
    "        print0(f\"DataLoader: total number of tokens: {ntok_total:,} across {len(self.files)} files\")\n",
    "\n",
    "        # kick things off\n",
    "        self.current_shard = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # we're being a bit clever here: if we already had shard 0 loaded,\n",
    "        # then don't do the work to reload it, just reset the pointer\n",
    "        if self.current_shard != 0:\n",
    "            self.current_shard = 0\n",
    "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "\n",
    "    def advance(self): # advance to next data shard\n",
    "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        # buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n",
    "        buf = torch.tensor(buf, dtype=torch.long)\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the start pointer in current shard\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds advance the shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.advance()\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fp16(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.float16)\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)\n",
    "\n",
    "def write_fp32(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.float32)\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)\n",
    "\n",
    "def write_bf16(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.bfloat16)\n",
    "    # numpy doesn't have bf16 datatype so we have to trick it\n",
    "    t = t.view(torch.int16) # trick: reinterpret as int16\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tensors(model_tensors, L, file, dtype):\n",
    "    # writes the GPT-2 model's weights to a binary file\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"}\n",
    "    write_fun = write_fp16 if dtype == \"float16\" else write_fp32 if dtype == \"float32\" else write_bf16\n",
    "    write_fun(model_tensors[\"transformer.wte.weight\"], file) # (V, C)\n",
    "    write_fun(model_tensors[\"transformer.wpe.weight\"], file) # (T, C)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
    "    for i in range(L): # (L, 3C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
    "    for i in range(L): # (L, 3C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
    "    for i in range(L): # (L, C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
    "    for i in range(L): # (L, 4C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
    "    for i in range(L): # (L, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
    "    for i in range(L): # (L, C, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
    "    write_fun(model_tensors[\"transformer.ln_f.weight\"], file) # (C, )\n",
    "    write_fun(model_tensors[\"transformer.ln_f.bias\"], file) # (C, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pad_vocab(tensor, multiple=128, value=0):\n",
    "    \"\"\"\n",
    "    The dimension of the vocab size in GPT-2 is 50,257\n",
    "    which is unfortunately a very unfriendly number for a lot of\n",
    "    matrix operations on the GPU. So we pad it to the nearest\n",
    "    friendlier multiple, e.g. 50,304 if multiple=128 when we\n",
    "    export the weights into C land. This is a NOOP algorithmically\n",
    "    and is only done to make the tensor operations more efficient.\n",
    "    \"\"\"\n",
    "    assert tensor.ndim == 2\n",
    "    V, C = tensor.shape\n",
    "    # assert V == 50257, \"just being defensive here\"\n",
    "    # calculate padded vocab size by rounding up to nearest multiple\n",
    "    Vp = ((V + multiple - 1) // multiple) * multiple\n",
    "    # pad the tensor\n",
    "    pad_rows = Vp - V\n",
    "    padded = tensor if pad_rows == 0 else F.pad(tensor, (0, 0, 0, pad_rows), value=value)\n",
    "    assert padded.shape == (Vp, C)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(model, filename, dtype):\n",
    "    # everything we need to instantiate the model\n",
    "    # 1) header is: version int, GPTConfig ints, padding to 1024 bytes\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"} # float16 todo maybe later\n",
    "    version = {\n",
    "        \"float16\": 2, # 2: all tensors are fp16, padded vocab\n",
    "        \"float32\": 3, # 3: all tensors are fp32, padded vocab\n",
    "        \"bfloat16\": 5, # 5: all tensors are bf16, padded vocab\n",
    "    }[dtype]\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240326 # magic\n",
    "    header[1] = version # checkpoint version\n",
    "    header[2] = model.config.block_size\n",
    "    header[3] = model.config.vocab_size\n",
    "    header[4] = model.config.n_layer\n",
    "    header[5] = model.config.n_head\n",
    "    header[6] = model.config.n_embd\n",
    "    # 2) the parameters follow the header\n",
    "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
    "    # pad the vocab to a multiple of 128 here at export, for efficiency in C\n",
    "    wte = params[\"transformer.wte.weight\"] # (V, C)\n",
    "    wte_padded = pad_vocab(wte) # (Vp, C)\n",
    "    params[\"transformer.wte.weight\"] = wte_padded # (Vp, C)\n",
    "    print(f\"padded vocab size from {wte.size(0)} to {wte_padded.size(0)}\")\n",
    "    header[7] = wte_padded.size(0) # padded vocab size store in header\n",
    "    # now write to file\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes()) # header\n",
    "        write_tensors(params, model.config.n_layer, file, dtype) # params\n",
    "    print(f\"wrote {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_state(model, x, y, logits, loss, filename, dtype=\"float32\"):\n",
    "    # the state is used for debugging.\n",
    "    # it contains information about the input, logits, loss, and the parameter gradients\n",
    "    # this can be used for checking the computation correctness in C\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240327 # magic\n",
    "    header[1] = 2 # run state version = 2 (1 -> 2 for padded vocab changes)\n",
    "    header[2] = x.size(0) # batch size of the batch, B\n",
    "    header[3] = x.size(1) # temporal extent of the batch, T\n",
    "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
    "    # pad the vocab grads here as well, to mirror write_model\n",
    "    wte_grad = grads[\"transformer.wte.weight\"] # (V, C)\n",
    "    wte_grad_padded = pad_vocab(wte_grad, value=0) # (Vp, C) # TODO later maybe pad with nan?\n",
    "    grads[\"transformer.wte.weight\"] = wte_grad_padded # (Vp, C)\n",
    "    print(f\"padded vocab size in reference grads from {wte_grad.size(0)} to {wte_grad_padded.size(0)}\")\n",
    "    with open(filename, \"wb\") as file:\n",
    "        # header\n",
    "        file.write(header.numpy().tobytes())\n",
    "        # input x\n",
    "        file.write(x.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # targets y\n",
    "        file.write(y.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # logits (result of the model forward pass)\n",
    "        write_fp32(logits.cpu(), file)\n",
    "        # loss (single float, result of the cross entropy loss)\n",
    "        write_fp32(loss.cpu(), file)\n",
    "        # gradients\n",
    "        write_tensors(grads, model.config.n_layer, file, dtype)\n",
    "    print(f\"wrote {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokenizer(enc, filename):\n",
    "    n = enc.max_token_value + 1\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240328 # magic\n",
    "    header[1] = 2 # tokenizer version = 2 (1 -> 2: includes EOT token)\n",
    "    header[2] = n # number of tokens\n",
    "    header[3] = enc.eot_token # EOT token\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes())\n",
    "        for i in range(n):\n",
    "            b = enc.decode_bytes([i])\n",
    "            length = len(b)\n",
    "            assert length < 256, f\"Token length exceeds 255: {length}\"\n",
    "            file.write(struct.pack(\"<B\", length))  # Write the length as a 1-byte unsigned integer\n",
    "            file.write(b)  # Write the actual bytes\n",
    "    print(f\"wrote {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps (cpu)\n"
     ]
    }
   ],
   "source": [
    "# B, T = 2, 4\n",
    "\"\"\" \n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128\n",
    "    vocab_size: int = 64\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 16 \"\"\"\n",
    "\n",
    "B, T = 2, 32 # batch size, sequence length\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "print(f\"using device: {device} ({device_type})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp_rank = 0\n",
    "ddp_local_rank = 0\n",
    "zero_stage = 0\n",
    "ddp_world_size = 1\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "total_batch_size = 128\n",
    "tokens_per_fwdbwd = B * T\n",
    "tokens_per_fwdbwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 128\n",
      "=> calculated gradient accumulation steps: 2\n"
     ]
    }
   ],
   "source": [
    "assert total_batch_size % tokens_per_fwdbwd == 0\n",
    "grad_accum_steps = total_batch_size // tokens_per_fwdbwd\n",
    "print0(f\"total desired batch size: {total_batch_size}\")\n",
    "print0(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a context manager following the desired dtype and device\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16, 'float8': torch.float8_e4m3fn}['float16']\n",
    "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
    "# rng / reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(64, 32)\n",
       "    (wpe): Embedding(256, 32)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=32, out_features=96, bias=True)\n",
       "          (c_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (gelu): NewGELU()\n",
       "          (c_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "if False: # args.compile:\n",
    "    if hasattr(config, \"coordinate_descent_tuning\"):\n",
    "        config.coordinate_descent_tuning = True # suggested by @Chillee\n",
    "    print0(\"compiling the model...\")\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader: total number of tokens: 892,316 across 1 files\n",
      "DataLoader: total number of tokens: 111,539 across 1 files\n"
     ]
    }
   ],
   "source": [
    "train_loader = DistributedDataLoader(\"tokenizer/train_tokens.bin\", B, T, ddp_rank, ddp_world_size)\n",
    "val_loader = DistributedDataLoader(\"tokenizer/val_tokens.bin\", B, T, ddp_rank, ddp_world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 32]), torch.Size([2, 32]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = train_loader.next_batch()\n",
    "x, y = x.to(device), y.to(device)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded vocab size from 64 to 128\n",
      "wrote gpt2_12K.bin\n",
      "padded vocab size from 64 to 128\n",
      "wrote gpt2_12K_bf16.bin\n",
      "padded vocab size in reference grads from 64 to 128\n",
      "wrote gpt2_12K_debug_state.bin\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model(x, y)\n",
    "loss.backward()\n",
    "# save model params, in both float32 and bfloat16\n",
    "model_to_size = {\"gpt1Letter\": \"12K\",  \"gpt2\": \"124M\", \"gpt2-medium\": \"355M\", \"gpt2-large\": \"774M\", \"gpt2-xl\": \"1558M\"}\n",
    "model_to_size.update({f\"d{d}\": f\"d{d}\" for d in [12, 24, 36, 48]})\n",
    "model_size_str = model_to_size[\"gpt1Letter\"] # e.g. \"124M\", or \"d12\"\n",
    "write_model(model, f\"gpt2_{model_size_str}.bin\", dtype=\"float16\")\n",
    "write_model(model, f\"gpt2_{model_size_str}_bf16.bin\", dtype=\"bfloat16\")\n",
    "# save x, y, logits, loss, and parameter gradients, for debugging C\n",
    "# always store these in fp32 to have an accurate reference (?)\n",
    "write_state(model, x, y, logits, loss, f\"gpt2_{model_size_str}_debug_state.bin\")\n",
    "# reset the train_loader for the optimization below\n",
    "train_loader.reset()\n",
    "# clear the grads here explicitly because otherwise we'd have a duplicate grad accumulation\n",
    "# since in the training loop we do a backward() and then zero_grad() at the end of the loop\n",
    "# this would cause an incorrect first training step\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0\n",
    "learning_rate = 3e-4\n",
    "learning_rate_decay_frac = 0.0\n",
    "warmup_iters = 0\n",
    "num_iterations = 1000\n",
    "val_loss_every = 0\n",
    "val_max_steps = 20\n",
    "sample_every = 0\n",
    "overfit_single_batch = 1\n",
    "inference_only = 0\n",
    "grad_clip = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 34, with 108,544 parameters\n",
      "num non-decayed parameter tensors: 66, with 3,392 parameters\n",
      "using fused AdamW: False\n",
      "using regular AdamW\n"
     ]
    }
   ],
   "source": [
    "raw_model = model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "# init the optimizer\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=weight_decay,\n",
    "                                            learning_rate=learning_rate, betas=(0.9, 0.95),\n",
    "                                            device_type=device, zero_stage=zero_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    min_lr = learning_rate * learning_rate_decay_frac\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it+1) / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > num_iterations:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (num_iterations - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "              'ı', 'ğ', 'ü', 'ş', 'ö', 'ç',\n",
    "              '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "              ' ', ',', '.', '!', '?', ';', ':', '-', '(', ')', '\"', \"'\", '+', '*', '/', '=', '@', '<', '>', '\\\\', '_',\n",
    "              '\\n',\n",
    "              ]\n",
    "\n",
    "def encode(text):\n",
    "  return [letter_list.index(c) for c in text.lower()]\n",
    "\n",
    "def decode(ids):\n",
    "  return ''.join([letter_list[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/1000 | train loss 4.147572 | norm 3.9352 | lr 3.00e-04 | (850.34 ms | 151 tok/s)\n",
      "step    2/1000 | train loss 4.061966 | norm 3.0696 | lr 3.00e-04 | (64.10 ms | 1997 tok/s)\n",
      "step    3/1000 | train loss 4.000878 | norm 1.8897 | lr 3.00e-04 | (64.90 ms | 1972 tok/s)\n",
      "step    4/1000 | train loss 3.962283 | norm 1.4610 | lr 3.00e-04 | (63.47 ms | 2017 tok/s)\n",
      "step    5/1000 | train loss 3.934070 | norm 1.4428 | lr 3.00e-04 | (65.29 ms | 1961 tok/s)\n",
      "step    6/1000 | train loss 3.908884 | norm 1.5155 | lr 3.00e-04 | (63.57 ms | 2014 tok/s)\n",
      "step    7/1000 | train loss 3.883916 | norm 1.5605 | lr 3.00e-04 | (66.46 ms | 1926 tok/s)\n",
      "step    8/1000 | train loss 3.859229 | norm 1.4854 | lr 3.00e-04 | (65.01 ms | 1969 tok/s)\n",
      "step    9/1000 | train loss 3.836764 | norm 1.3438 | lr 3.00e-04 | (64.29 ms | 1991 tok/s)\n",
      "step   10/1000 | train loss 3.817059 | norm 1.2818 | lr 3.00e-04 | (63.57 ms | 2014 tok/s)\n",
      "step   11/1000 | train loss 3.798239 | norm 1.2798 | lr 3.00e-04 | (64.13 ms | 1996 tok/s)\n",
      "step   12/1000 | train loss 3.779037 | norm 1.2904 | lr 3.00e-04 | (66.21 ms | 1933 tok/s)\n",
      "step   13/1000 | train loss 3.759562 | norm 1.2883 | lr 3.00e-04 | (68.32 ms | 1874 tok/s)\n",
      "step   14/1000 | train loss 3.740544 | norm 1.2688 | lr 3.00e-04 | (68.57 ms | 1867 tok/s)\n",
      "step   15/1000 | train loss 3.722635 | norm 1.2815 | lr 3.00e-04 | (64.15 ms | 1995 tok/s)\n",
      "step   16/1000 | train loss 3.705767 | norm 1.3107 | lr 3.00e-04 | (63.50 ms | 2016 tok/s)\n",
      "step   17/1000 | train loss 3.689604 | norm 1.3548 | lr 3.00e-04 | (64.61 ms | 1981 tok/s)\n",
      "step   18/1000 | train loss 3.673574 | norm 1.3696 | lr 3.00e-04 | (66.03 ms | 1939 tok/s)\n",
      "step   19/1000 | train loss 3.657400 | norm 1.3557 | lr 3.00e-04 | (64.90 ms | 1972 tok/s)\n",
      "step   20/1000 | train loss 3.641167 | norm 1.3552 | lr 3.00e-04 | (62.75 ms | 2040 tok/s)\n",
      "step   21/1000 | train loss 3.625070 | norm 1.3490 | lr 3.00e-04 | (65.07 ms | 1967 tok/s)\n",
      "step   22/1000 | train loss 3.609541 | norm 1.3412 | lr 3.00e-04 | (65.58 ms | 1952 tok/s)\n",
      "step   23/1000 | train loss 3.594787 | norm 1.3664 | lr 3.00e-04 | (63.50 ms | 2016 tok/s)\n",
      "step   24/1000 | train loss 3.580495 | norm 1.3657 | lr 3.00e-04 | (113.63 ms | 1126 tok/s)\n",
      "step   25/1000 | train loss 3.566446 | norm 1.3601 | lr 3.00e-04 | (67.67 ms | 1892 tok/s)\n",
      "step   26/1000 | train loss 3.552765 | norm 1.3946 | lr 3.00e-04 | (66.94 ms | 1912 tok/s)\n",
      "step   27/1000 | train loss 3.539281 | norm 1.4795 | lr 2.99e-04 | (64.77 ms | 1976 tok/s)\n",
      "step   28/1000 | train loss 3.525871 | norm 1.6772 | lr 2.99e-04 | (65.30 ms | 1960 tok/s)\n",
      "step   29/1000 | train loss 3.512690 | norm 1.7626 | lr 2.99e-04 | (66.84 ms | 1915 tok/s)\n",
      "step   30/1000 | train loss 3.499448 | norm 1.5804 | lr 2.99e-04 | (70.67 ms | 1811 tok/s)\n",
      "step   31/1000 | train loss 3.485919 | norm 1.4602 | lr 2.99e-04 | (67.24 ms | 1904 tok/s)\n",
      "step   32/1000 | train loss 3.472304 | norm 1.3713 | lr 2.99e-04 | (67.28 ms | 1903 tok/s)\n",
      "step   33/1000 | train loss 3.458914 | norm 1.3993 | lr 2.99e-04 | (66.31 ms | 1930 tok/s)\n",
      "step   34/1000 | train loss 3.445725 | norm 1.4332 | lr 2.99e-04 | (65.60 ms | 1951 tok/s)\n",
      "step   35/1000 | train loss 3.432967 | norm 2.0172 | lr 2.99e-04 | (64.74 ms | 1977 tok/s)\n",
      "step   36/1000 | train loss 3.421213 | norm 2.9011 | lr 2.99e-04 | (64.71 ms | 1978 tok/s)\n",
      "step   37/1000 | train loss 3.408494 | norm 1.7594 | lr 2.99e-04 | (62.49 ms | 2048 tok/s)\n",
      "step   38/1000 | train loss 3.396523 | norm 1.4436 | lr 2.99e-04 | (60.53 ms | 2115 tok/s)\n",
      "step   39/1000 | train loss 3.384717 | norm 1.9676 | lr 2.99e-04 | (61.44 ms | 2083 tok/s)\n",
      "step   40/1000 | train loss 3.373411 | norm 3.0144 | lr 2.99e-04 | (61.41 ms | 2084 tok/s)\n",
      "step   41/1000 | train loss 3.361800 | norm 2.4595 | lr 2.99e-04 | (68.69 ms | 1863 tok/s)\n",
      "step   42/1000 | train loss 3.351133 | norm 2.5870 | lr 2.99e-04 | (68.34 ms | 1873 tok/s)\n",
      "step   43/1000 | train loss 3.339564 | norm 1.7172 | lr 2.99e-04 | (115.41 ms | 1109 tok/s)\n",
      "step   44/1000 | train loss 3.328242 | norm 1.4513 | lr 2.99e-04 | (72.00 ms | 1778 tok/s)\n",
      "step   45/1000 | train loss 3.317114 | norm 2.0782 | lr 2.99e-04 | (65.88 ms | 1943 tok/s)\n",
      "step   46/1000 | train loss 3.307426 | norm 4.2779 | lr 2.99e-04 | (66.67 ms | 1920 tok/s)\n",
      "step   47/1000 | train loss 3.295456 | norm 1.5551 | lr 2.98e-04 | (66.05 ms | 1938 tok/s)\n",
      "step   48/1000 | train loss 3.285419 | norm 2.9723 | lr 2.98e-04 | (63.29 ms | 2022 tok/s)\n",
      "step   49/1000 | train loss 3.274576 | norm 2.8559 | lr 2.98e-04 | (62.06 ms | 2063 tok/s)\n",
      "step   50/1000 | train loss 3.264776 | norm 3.3409 | lr 2.98e-04 | (61.13 ms | 2094 tok/s)\n",
      "step   51/1000 | train loss 3.254381 | norm 2.4497 | lr 2.98e-04 | (60.53 ms | 2115 tok/s)\n",
      "step   52/1000 | train loss 3.243764 | norm 1.5832 | lr 2.98e-04 | (60.57 ms | 2113 tok/s)\n",
      "step   53/1000 | train loss 3.233407 | norm 2.3787 | lr 2.98e-04 | (60.08 ms | 2131 tok/s)\n",
      "step   54/1000 | train loss 3.225140 | norm 5.4833 | lr 2.98e-04 | (63.15 ms | 2027 tok/s)\n",
      "step   55/1000 | train loss 3.213158 | norm 1.6013 | lr 2.98e-04 | (61.75 ms | 2073 tok/s)\n",
      "step   56/1000 | train loss 3.208163 | norm 8.3949 | lr 2.98e-04 | (61.34 ms | 2087 tok/s)\n",
      "step   57/1000 | train loss 3.195298 | norm 4.6153 | lr 2.98e-04 | (61.16 ms | 2093 tok/s)\n",
      "step   58/1000 | train loss 3.191641 | norm 9.3667 | lr 2.98e-04 | (61.90 ms | 2068 tok/s)\n",
      "step   59/1000 | train loss 3.184249 | norm 8.6796 | lr 2.98e-04 | (61.97 ms | 2066 tok/s)\n",
      "step   60/1000 | train loss 3.171241 | norm 4.7660 | lr 2.97e-04 | (61.27 ms | 2089 tok/s)\n",
      "step   61/1000 | train loss 3.164225 | norm 6.0763 | lr 2.97e-04 | (63.88 ms | 2004 tok/s)\n",
      "step   62/1000 | train loss 3.158083 | norm 6.3791 | lr 2.97e-04 | (66.75 ms | 1917 tok/s)\n",
      "step   63/1000 | train loss 3.146347 | norm 3.5774 | lr 2.97e-04 | (68.51 ms | 1868 tok/s)\n",
      "step   64/1000 | train loss 3.144007 | norm 6.7530 | lr 2.97e-04 | (67.20 ms | 1905 tok/s)\n",
      "step   65/1000 | train loss 3.136063 | norm 6.0041 | lr 2.97e-04 | (64.93 ms | 1971 tok/s)\n",
      "step   66/1000 | train loss 3.124966 | norm 3.5936 | lr 2.97e-04 | (68.79 ms | 1861 tok/s)\n",
      "step   67/1000 | train loss 3.118630 | norm 3.9695 | lr 2.97e-04 | (65.26 ms | 1962 tok/s)\n",
      "step   68/1000 | train loss 3.110877 | norm 4.5190 | lr 2.97e-04 | (62.78 ms | 2039 tok/s)\n",
      "step   69/1000 | train loss 3.102647 | norm 2.4009 | lr 2.97e-04 | (64.34 ms | 1989 tok/s)\n",
      "step   70/1000 | train loss 3.094366 | norm 2.9098 | lr 2.96e-04 | (63.58 ms | 2013 tok/s)\n",
      "step   71/1000 | train loss 3.088207 | norm 4.1908 | lr 2.96e-04 | (63.79 ms | 2007 tok/s)\n",
      "step   72/1000 | train loss 3.078388 | norm 2.2761 | lr 2.96e-04 | (61.47 ms | 2082 tok/s)\n",
      "step   73/1000 | train loss 3.072722 | norm 5.4070 | lr 2.96e-04 | (62.44 ms | 2050 tok/s)\n",
      "step   74/1000 | train loss 3.065884 | norm 3.7000 | lr 2.96e-04 | (60.74 ms | 2107 tok/s)\n",
      "step   75/1000 | train loss 3.055187 | norm 4.2810 | lr 2.96e-04 | (62.66 ms | 2043 tok/s)\n",
      "step   76/1000 | train loss 3.047509 | norm 3.1566 | lr 2.96e-04 | (60.64 ms | 2111 tok/s)\n",
      "step   77/1000 | train loss 3.039460 | norm 3.2022 | lr 2.96e-04 | (61.04 ms | 2097 tok/s)\n",
      "step   78/1000 | train loss 3.030220 | norm 2.9211 | lr 2.96e-04 | (60.68 ms | 2109 tok/s)\n",
      "step   79/1000 | train loss 3.021923 | norm 2.6268 | lr 2.96e-04 | (60.76 ms | 2107 tok/s)\n",
      "step   80/1000 | train loss 3.013059 | norm 3.3974 | lr 2.95e-04 | (59.18 ms | 2163 tok/s)\n",
      "step   81/1000 | train loss 3.004293 | norm 2.5300 | lr 2.95e-04 | (62.24 ms | 2057 tok/s)\n",
      "step   82/1000 | train loss 2.994447 | norm 2.1015 | lr 2.95e-04 | (61.86 ms | 2069 tok/s)\n",
      "step   83/1000 | train loss 2.986683 | norm 3.7977 | lr 2.95e-04 | (57.80 ms | 2214 tok/s)\n",
      "step   84/1000 | train loss 2.976464 | norm 2.0324 | lr 2.95e-04 | (59.38 ms | 2155 tok/s)\n",
      "step   85/1000 | train loss 2.968023 | norm 3.9953 | lr 2.95e-04 | (60.04 ms | 2132 tok/s)\n",
      "step   86/1000 | train loss 2.958913 | norm 2.9010 | lr 2.95e-04 | (60.56 ms | 2113 tok/s)\n",
      "step   87/1000 | train loss 2.948592 | norm 3.1886 | lr 2.95e-04 | (62.19 ms | 2058 tok/s)\n",
      "step   88/1000 | train loss 2.939868 | norm 3.5259 | lr 2.94e-04 | (61.30 ms | 2088 tok/s)\n",
      "step   89/1000 | train loss 2.930153 | norm 2.8258 | lr 2.94e-04 | (61.43 ms | 2084 tok/s)\n",
      "step   90/1000 | train loss 2.920628 | norm 3.2088 | lr 2.94e-04 | (60.24 ms | 2125 tok/s)\n",
      "step   91/1000 | train loss 2.911060 | norm 3.0047 | lr 2.94e-04 | (63.10 ms | 2028 tok/s)\n",
      "step   92/1000 | train loss 2.902011 | norm 3.2147 | lr 2.94e-04 | (62.73 ms | 2041 tok/s)\n",
      "step   93/1000 | train loss 2.892154 | norm 2.4490 | lr 2.94e-04 | (63.08 ms | 2029 tok/s)\n",
      "step   94/1000 | train loss 2.882159 | norm 2.6322 | lr 2.94e-04 | (61.59 ms | 2078 tok/s)\n",
      "step   95/1000 | train loss 2.873377 | norm 3.0117 | lr 2.94e-04 | (62.91 ms | 2035 tok/s)\n",
      "step   96/1000 | train loss 2.862536 | norm 2.2254 | lr 2.93e-04 | (64.39 ms | 1988 tok/s)\n",
      "step   97/1000 | train loss 2.854734 | norm 4.3499 | lr 2.93e-04 | (104.02 ms | 1230 tok/s)\n",
      "step   98/1000 | train loss 2.844027 | norm 2.6391 | lr 2.93e-04 | (64.70 ms | 1978 tok/s)\n",
      "step   99/1000 | train loss 2.834271 | norm 3.1368 | lr 2.93e-04 | (63.65 ms | 2011 tok/s)\n",
      "step  100/1000 | train loss 2.824340 | norm 2.3073 | lr 2.93e-04 | (63.42 ms | 2018 tok/s)\n",
      "step  101/1000 | train loss 2.814889 | norm 3.8634 | lr 2.93e-04 | (64.56 ms | 1983 tok/s)\n",
      "step  102/1000 | train loss 2.805892 | norm 3.3375 | lr 2.93e-04 | (64.61 ms | 1981 tok/s)\n",
      "step  103/1000 | train loss 2.794950 | norm 2.3305 | lr 2.92e-04 | (65.64 ms | 1950 tok/s)\n",
      "step  104/1000 | train loss 2.786199 | norm 3.3626 | lr 2.92e-04 | (63.37 ms | 2020 tok/s)\n",
      "step  105/1000 | train loss 2.776322 | norm 2.7975 | lr 2.92e-04 | (63.66 ms | 2011 tok/s)\n",
      "step  106/1000 | train loss 2.766294 | norm 3.1370 | lr 2.92e-04 | (64.71 ms | 1978 tok/s)\n",
      "step  107/1000 | train loss 2.757099 | norm 3.1652 | lr 2.92e-04 | (63.06 ms | 2030 tok/s)\n",
      "step  108/1000 | train loss 2.746884 | norm 2.2532 | lr 2.92e-04 | (60.52 ms | 2115 tok/s)\n",
      "step  109/1000 | train loss 2.737542 | norm 3.0181 | lr 2.91e-04 | (63.69 ms | 2010 tok/s)\n",
      "step  110/1000 | train loss 2.728402 | norm 3.2382 | lr 2.91e-04 | (63.07 ms | 2029 tok/s)\n",
      "step  111/1000 | train loss 2.717712 | norm 3.1664 | lr 2.91e-04 | (63.40 ms | 2019 tok/s)\n",
      "step  112/1000 | train loss 2.708628 | norm 3.0436 | lr 2.91e-04 | (67.71 ms | 1891 tok/s)\n",
      "step  113/1000 | train loss 2.698292 | norm 1.8659 | lr 2.91e-04 | (66.85 ms | 1915 tok/s)\n",
      "step  114/1000 | train loss 2.688876 | norm 2.5807 | lr 2.91e-04 | (61.12 ms | 2094 tok/s)\n",
      "step  115/1000 | train loss 2.679825 | norm 3.4653 | lr 2.90e-04 | (63.09 ms | 2029 tok/s)\n",
      "step  116/1000 | train loss 2.669369 | norm 3.2949 | lr 2.90e-04 | (62.32 ms | 2054 tok/s)\n",
      "step  117/1000 | train loss 2.659466 | norm 2.9114 | lr 2.90e-04 | (61.50 ms | 2081 tok/s)\n",
      "step  118/1000 | train loss 2.649927 | norm 2.3438 | lr 2.90e-04 | (60.54 ms | 2114 tok/s)\n",
      "step  119/1000 | train loss 2.639728 | norm 2.2138 | lr 2.90e-04 | (60.65 ms | 2111 tok/s)\n",
      "step  120/1000 | train loss 2.630129 | norm 2.4517 | lr 2.90e-04 | (59.18 ms | 2163 tok/s)\n",
      "step  121/1000 | train loss 2.620420 | norm 3.1695 | lr 2.89e-04 | (133.05 ms | 962 tok/s)\n",
      "step  122/1000 | train loss 2.610847 | norm 3.6834 | lr 2.89e-04 | (69.18 ms | 1850 tok/s)\n",
      "step  123/1000 | train loss 2.601614 | norm 3.4162 | lr 2.89e-04 | (68.21 ms | 1876 tok/s)\n",
      "step  124/1000 | train loss 2.591043 | norm 2.1500 | lr 2.89e-04 | (115.94 ms | 1104 tok/s)\n",
      "step  125/1000 | train loss 2.581554 | norm 2.2827 | lr 2.89e-04 | (67.74 ms | 1890 tok/s)\n",
      "step  126/1000 | train loss 2.572735 | norm 4.0612 | lr 2.89e-04 | (63.31 ms | 2022 tok/s)\n",
      "step  127/1000 | train loss 2.562366 | norm 2.6787 | lr 2.88e-04 | (63.09 ms | 2029 tok/s)\n",
      "step  128/1000 | train loss 2.552761 | norm 2.5102 | lr 2.88e-04 | (62.18 ms | 2058 tok/s)\n",
      "step  129/1000 | train loss 2.543782 | norm 3.4815 | lr 2.88e-04 | (63.38 ms | 2019 tok/s)\n",
      "step  130/1000 | train loss 2.533887 | norm 2.8163 | lr 2.88e-04 | (69.04 ms | 1854 tok/s)\n",
      "step  131/1000 | train loss 2.523973 | norm 2.1409 | lr 2.88e-04 | (72.67 ms | 1761 tok/s)\n",
      "step  132/1000 | train loss 2.514824 | norm 3.0744 | lr 2.87e-04 | (69.75 ms | 1835 tok/s)\n",
      "step  133/1000 | train loss 2.505307 | norm 3.2389 | lr 2.87e-04 | (68.90 ms | 1858 tok/s)\n",
      "step  134/1000 | train loss 2.495572 | norm 2.3914 | lr 2.87e-04 | (64.45 ms | 1986 tok/s)\n",
      "step  135/1000 | train loss 2.486682 | norm 2.7752 | lr 2.87e-04 | (69.84 ms | 1833 tok/s)\n",
      "step  136/1000 | train loss 2.478248 | norm 3.9562 | lr 2.87e-04 | (65.49 ms | 1954 tok/s)\n",
      "step  137/1000 | train loss 2.467473 | norm 2.1061 | lr 2.87e-04 | (64.57 ms | 1982 tok/s)\n",
      "step  138/1000 | train loss 2.459305 | norm 2.8075 | lr 2.86e-04 | (61.93 ms | 2067 tok/s)\n",
      "step  139/1000 | train loss 2.449550 | norm 2.7983 | lr 2.86e-04 | (65.04 ms | 1968 tok/s)\n",
      "step  140/1000 | train loss 2.440039 | norm 2.6640 | lr 2.86e-04 | (65.89 ms | 1943 tok/s)\n",
      "step  141/1000 | train loss 2.430541 | norm 2.7570 | lr 2.86e-04 | (64.89 ms | 1973 tok/s)\n",
      "step  142/1000 | train loss 2.421334 | norm 2.6700 | lr 2.86e-04 | (119.66 ms | 1070 tok/s)\n",
      "step  143/1000 | train loss 2.411457 | norm 2.3757 | lr 2.85e-04 | (68.54 ms | 1867 tok/s)\n",
      "step  144/1000 | train loss 2.402361 | norm 2.9186 | lr 2.85e-04 | (61.92 ms | 2067 tok/s)\n",
      "step  145/1000 | train loss 2.393018 | norm 2.8451 | lr 2.85e-04 | (61.66 ms | 2076 tok/s)\n",
      "step  146/1000 | train loss 2.383369 | norm 2.2369 | lr 2.85e-04 | (62.12 ms | 2061 tok/s)\n",
      "step  147/1000 | train loss 2.375059 | norm 2.9976 | lr 2.84e-04 | (61.05 ms | 2097 tok/s)\n",
      "step  148/1000 | train loss 2.364775 | norm 2.5573 | lr 2.84e-04 | (61.16 ms | 2093 tok/s)\n",
      "step  149/1000 | train loss 2.356046 | norm 2.3214 | lr 2.84e-04 | (62.04 ms | 2063 tok/s)\n",
      "step  150/1000 | train loss 2.345857 | norm 1.7942 | lr 2.84e-04 | (64.26 ms | 1992 tok/s)\n",
      "step  151/1000 | train loss 2.336571 | norm 2.2556 | lr 2.84e-04 | (67.19 ms | 1905 tok/s)\n",
      "step  152/1000 | train loss 2.328271 | norm 4.2929 | lr 2.83e-04 | (66.82 ms | 1916 tok/s)\n",
      "step  153/1000 | train loss 2.317996 | norm 1.8012 | lr 2.83e-04 | (66.86 ms | 1914 tok/s)\n",
      "step  154/1000 | train loss 2.311337 | norm 3.2599 | lr 2.83e-04 | (65.26 ms | 1961 tok/s)\n",
      "step  155/1000 | train loss 2.300445 | norm 2.9691 | lr 2.83e-04 | (67.65 ms | 1892 tok/s)\n",
      "step  156/1000 | train loss 2.291633 | norm 2.6008 | lr 2.83e-04 | (64.92 ms | 1972 tok/s)\n",
      "step  157/1000 | train loss 2.282290 | norm 2.6841 | lr 2.82e-04 | (63.83 ms | 2005 tok/s)\n",
      "step  158/1000 | train loss 2.272787 | norm 2.6504 | lr 2.82e-04 | (62.59 ms | 2045 tok/s)\n",
      "step  159/1000 | train loss 2.264404 | norm 2.6964 | lr 2.82e-04 | (61.41 ms | 2084 tok/s)\n",
      "step  160/1000 | train loss 2.254131 | norm 1.8498 | lr 2.82e-04 | (62.85 ms | 2037 tok/s)\n",
      "step  161/1000 | train loss 2.245826 | norm 1.9757 | lr 2.81e-04 | (65.48 ms | 1955 tok/s)\n",
      "step  162/1000 | train loss 2.236199 | norm 2.5189 | lr 2.81e-04 | (66.34 ms | 1929 tok/s)\n",
      "step  163/1000 | train loss 2.227008 | norm 1.9369 | lr 2.81e-04 | (67.27 ms | 1903 tok/s)\n",
      "step  164/1000 | train loss 2.218390 | norm 3.2301 | lr 2.81e-04 | (69.16 ms | 1851 tok/s)\n",
      "step  165/1000 | train loss 2.209015 | norm 2.2086 | lr 2.81e-04 | (65.64 ms | 1950 tok/s)\n",
      "step  166/1000 | train loss 2.200297 | norm 3.3424 | lr 2.80e-04 | (66.37 ms | 1928 tok/s)\n",
      "step  167/1000 | train loss 2.191174 | norm 2.1407 | lr 2.80e-04 | (64.60 ms | 1981 tok/s)\n",
      "step  168/1000 | train loss 2.182325 | norm 2.7561 | lr 2.80e-04 | (66.77 ms | 1917 tok/s)\n",
      "step  169/1000 | train loss 2.173121 | norm 1.8730 | lr 2.80e-04 | (67.87 ms | 1886 tok/s)\n",
      "step  170/1000 | train loss 2.164902 | norm 3.4384 | lr 2.79e-04 | (68.96 ms | 1856 tok/s)\n",
      "step  171/1000 | train loss 2.156338 | norm 2.5647 | lr 2.79e-04 | (70.51 ms | 1815 tok/s)\n",
      "step  172/1000 | train loss 2.146494 | norm 2.6151 | lr 2.79e-04 | (71.89 ms | 1780 tok/s)\n",
      "step  173/1000 | train loss 2.137972 | norm 1.9887 | lr 2.79e-04 | (65.68 ms | 1949 tok/s)\n",
      "step  174/1000 | train loss 2.129005 | norm 2.5624 | lr 2.78e-04 | (63.49 ms | 2016 tok/s)\n",
      "step  175/1000 | train loss 2.120125 | norm 1.9971 | lr 2.78e-04 | (63.63 ms | 2012 tok/s)\n",
      "step  176/1000 | train loss 2.111379 | norm 2.8240 | lr 2.78e-04 | (62.68 ms | 2042 tok/s)\n",
      "step  177/1000 | train loss 2.102839 | norm 2.9774 | lr 2.78e-04 | (61.64 ms | 2076 tok/s)\n",
      "step  178/1000 | train loss 2.093876 | norm 2.3586 | lr 2.77e-04 | (60.72 ms | 2108 tok/s)\n",
      "step  179/1000 | train loss 2.085190 | norm 2.7100 | lr 2.77e-04 | (60.42 ms | 2118 tok/s)\n",
      "step  180/1000 | train loss 2.076728 | norm 2.6135 | lr 2.77e-04 | (60.43 ms | 2118 tok/s)\n",
      "step  181/1000 | train loss 2.067734 | norm 2.2861 | lr 2.77e-04 | (59.26 ms | 2160 tok/s)\n",
      "step  182/1000 | train loss 2.059374 | norm 2.5742 | lr 2.76e-04 | (59.35 ms | 2157 tok/s)\n",
      "step  183/1000 | train loss 2.050960 | norm 2.9942 | lr 2.76e-04 | (58.86 ms | 2175 tok/s)\n",
      "step  184/1000 | train loss 2.041703 | norm 1.7580 | lr 2.76e-04 | (57.62 ms | 2222 tok/s)\n",
      "step  185/1000 | train loss 2.033422 | norm 1.9044 | lr 2.76e-04 | (58.32 ms | 2195 tok/s)\n",
      "step  186/1000 | train loss 2.024966 | norm 2.8343 | lr 2.75e-04 | (60.25 ms | 2125 tok/s)\n",
      "step  187/1000 | train loss 2.016483 | norm 2.8256 | lr 2.75e-04 | (60.39 ms | 2120 tok/s)\n",
      "step  188/1000 | train loss 2.007252 | norm 1.7266 | lr 2.75e-04 | (58.78 ms | 2178 tok/s)\n",
      "step  189/1000 | train loss 1.999446 | norm 2.5715 | lr 2.75e-04 | (59.58 ms | 2148 tok/s)\n",
      "step  190/1000 | train loss 1.991294 | norm 3.4515 | lr 2.74e-04 | (59.20 ms | 2162 tok/s)\n",
      "step  191/1000 | train loss 1.981681 | norm 1.6581 | lr 2.74e-04 | (61.10 ms | 2095 tok/s)\n",
      "step  192/1000 | train loss 1.973768 | norm 2.2073 | lr 2.74e-04 | (60.71 ms | 2108 tok/s)\n",
      "step  193/1000 | train loss 1.966366 | norm 3.4486 | lr 2.74e-04 | (60.62 ms | 2112 tok/s)\n",
      "step  194/1000 | train loss 1.956310 | norm 1.5505 | lr 2.73e-04 | (59.38 ms | 2155 tok/s)\n",
      "step  195/1000 | train loss 1.951133 | norm 3.4690 | lr 2.73e-04 | (60.78 ms | 2106 tok/s)\n",
      "step  196/1000 | train loss 1.940654 | norm 2.2157 | lr 2.73e-04 | (61.03 ms | 2097 tok/s)\n",
      "step  197/1000 | train loss 1.934181 | norm 2.8184 | lr 2.72e-04 | (60.59 ms | 2112 tok/s)\n",
      "step  198/1000 | train loss 1.924774 | norm 2.4138 | lr 2.72e-04 | (61.50 ms | 2081 tok/s)\n",
      "step  199/1000 | train loss 1.917096 | norm 2.5936 | lr 2.72e-04 | (60.20 ms | 2126 tok/s)\n",
      "step  200/1000 | train loss 1.908214 | norm 1.9224 | lr 2.72e-04 | (59.64 ms | 2146 tok/s)\n",
      "step  201/1000 | train loss 1.900535 | norm 2.2402 | lr 2.71e-04 | (59.02 ms | 2169 tok/s)\n",
      "step  202/1000 | train loss 1.892376 | norm 2.3461 | lr 2.71e-04 | (59.93 ms | 2136 tok/s)\n",
      "step  203/1000 | train loss 1.883669 | norm 2.6009 | lr 2.71e-04 | (60.64 ms | 2111 tok/s)\n",
      "step  204/1000 | train loss 1.876828 | norm 2.9221 | lr 2.71e-04 | (59.72 ms | 2143 tok/s)\n",
      "step  205/1000 | train loss 1.867417 | norm 2.0978 | lr 2.70e-04 | (60.44 ms | 2118 tok/s)\n",
      "step  206/1000 | train loss 1.860071 | norm 2.5329 | lr 2.70e-04 | (60.41 ms | 2119 tok/s)\n",
      "step  207/1000 | train loss 1.852292 | norm 2.4389 | lr 2.70e-04 | (60.82 ms | 2104 tok/s)\n",
      "step  208/1000 | train loss 1.843261 | norm 1.7532 | lr 2.69e-04 | (59.30 ms | 2159 tok/s)\n",
      "step  209/1000 | train loss 1.836273 | norm 2.6256 | lr 2.69e-04 | (59.27 ms | 2160 tok/s)\n",
      "step  210/1000 | train loss 1.828162 | norm 2.7624 | lr 2.69e-04 | (59.93 ms | 2136 tok/s)\n",
      "step  211/1000 | train loss 1.819515 | norm 1.8867 | lr 2.69e-04 | (60.05 ms | 2131 tok/s)\n",
      "step  212/1000 | train loss 1.811966 | norm 2.1442 | lr 2.68e-04 | (60.69 ms | 2109 tok/s)\n",
      "step  213/1000 | train loss 1.804270 | norm 2.5998 | lr 2.68e-04 | (59.41 ms | 2154 tok/s)\n",
      "step  214/1000 | train loss 1.795611 | norm 1.6849 | lr 2.68e-04 | (58.84 ms | 2175 tok/s)\n",
      "step  215/1000 | train loss 1.788059 | norm 2.1812 | lr 2.67e-04 | (58.86 ms | 2175 tok/s)\n",
      "step  216/1000 | train loss 1.780463 | norm 2.6364 | lr 2.67e-04 | (59.92 ms | 2136 tok/s)\n",
      "step  217/1000 | train loss 1.771858 | norm 2.2111 | lr 2.67e-04 | (58.47 ms | 2189 tok/s)\n",
      "step  218/1000 | train loss 1.764319 | norm 1.9790 | lr 2.66e-04 | (58.60 ms | 2184 tok/s)\n",
      "step  219/1000 | train loss 1.756295 | norm 1.9293 | lr 2.66e-04 | (60.45 ms | 2117 tok/s)\n",
      "step  220/1000 | train loss 1.748442 | norm 2.4239 | lr 2.66e-04 | (58.58 ms | 2185 tok/s)\n",
      "step  221/1000 | train loss 1.740393 | norm 2.2498 | lr 2.66e-04 | (59.03 ms | 2168 tok/s)\n",
      "step  222/1000 | train loss 1.732575 | norm 1.9050 | lr 2.65e-04 | (59.95 ms | 2135 tok/s)\n",
      "step  223/1000 | train loss 1.724238 | norm 1.6865 | lr 2.65e-04 | (59.87 ms | 2138 tok/s)\n",
      "step  224/1000 | train loss 1.716615 | norm 2.3439 | lr 2.65e-04 | (59.69 ms | 2145 tok/s)\n",
      "step  225/1000 | train loss 1.708675 | norm 2.2420 | lr 2.64e-04 | (59.04 ms | 2168 tok/s)\n",
      "step  226/1000 | train loss 1.700638 | norm 1.9914 | lr 2.64e-04 | (60.04 ms | 2132 tok/s)\n",
      "step  227/1000 | train loss 1.692813 | norm 2.1723 | lr 2.64e-04 | (59.63 ms | 2147 tok/s)\n",
      "step  228/1000 | train loss 1.684916 | norm 1.8808 | lr 2.63e-04 | (60.77 ms | 2106 tok/s)\n",
      "step  229/1000 | train loss 1.677265 | norm 2.2216 | lr 2.63e-04 | (59.53 ms | 2150 tok/s)\n",
      "step  230/1000 | train loss 1.669455 | norm 2.1929 | lr 2.63e-04 | (60.53 ms | 2115 tok/s)\n",
      "step  231/1000 | train loss 1.661415 | norm 1.8169 | lr 2.63e-04 | (60.48 ms | 2117 tok/s)\n",
      "step  232/1000 | train loss 1.653672 | norm 1.9092 | lr 2.62e-04 | (59.71 ms | 2144 tok/s)\n",
      "step  233/1000 | train loss 1.645981 | norm 2.0769 | lr 2.62e-04 | (59.52 ms | 2150 tok/s)\n",
      "step  234/1000 | train loss 1.638543 | norm 2.7773 | lr 2.62e-04 | (60.13 ms | 2129 tok/s)\n",
      "step  235/1000 | train loss 1.630379 | norm 1.9937 | lr 2.61e-04 | (59.58 ms | 2148 tok/s)\n",
      "step  236/1000 | train loss 1.622895 | norm 2.0095 | lr 2.61e-04 | (59.02 ms | 2169 tok/s)\n",
      "step  237/1000 | train loss 1.615420 | norm 2.0927 | lr 2.61e-04 | (60.37 ms | 2120 tok/s)\n",
      "step  238/1000 | train loss 1.607714 | norm 2.0862 | lr 2.60e-04 | (60.13 ms | 2129 tok/s)\n",
      "step  239/1000 | train loss 1.600180 | norm 2.0149 | lr 2.60e-04 | (60.30 ms | 2123 tok/s)\n",
      "step  240/1000 | train loss 1.592492 | norm 1.7778 | lr 2.60e-04 | (58.74 ms | 2179 tok/s)\n",
      "step  241/1000 | train loss 1.584729 | norm 1.5356 | lr 2.59e-04 | (61.68 ms | 2075 tok/s)\n",
      "step  242/1000 | train loss 1.577247 | norm 1.7644 | lr 2.59e-04 | (60.95 ms | 2100 tok/s)\n",
      "step  243/1000 | train loss 1.569922 | norm 2.2862 | lr 2.59e-04 | (62.03 ms | 2064 tok/s)\n",
      "step  244/1000 | train loss 1.562232 | norm 2.3445 | lr 2.58e-04 | (59.23 ms | 2161 tok/s)\n",
      "step  245/1000 | train loss 1.554844 | norm 2.5297 | lr 2.58e-04 | (60.89 ms | 2102 tok/s)\n",
      "step  246/1000 | train loss 1.547245 | norm 1.9883 | lr 2.58e-04 | (60.76 ms | 2107 tok/s)\n",
      "step  247/1000 | train loss 1.540181 | norm 2.0023 | lr 2.57e-04 | (60.75 ms | 2107 tok/s)\n",
      "step  248/1000 | train loss 1.533064 | norm 2.3483 | lr 2.57e-04 | (60.11 ms | 2129 tok/s)\n",
      "step  249/1000 | train loss 1.525273 | norm 1.9398 | lr 2.57e-04 | (61.23 ms | 2090 tok/s)\n",
      "step  250/1000 | train loss 1.518339 | norm 2.1174 | lr 2.56e-04 | (61.02 ms | 2098 tok/s)\n",
      "step  251/1000 | train loss 1.511408 | norm 2.6110 | lr 2.56e-04 | (59.00 ms | 2169 tok/s)\n",
      "step  252/1000 | train loss 1.503832 | norm 1.8122 | lr 2.56e-04 | (59.04 ms | 2168 tok/s)\n",
      "step  253/1000 | train loss 1.497063 | norm 1.7847 | lr 2.55e-04 | (60.62 ms | 2112 tok/s)\n",
      "step  254/1000 | train loss 1.489975 | norm 2.1160 | lr 2.55e-04 | (60.06 ms | 2131 tok/s)\n",
      "step  255/1000 | train loss 1.482998 | norm 2.5577 | lr 2.55e-04 | (59.40 ms | 2155 tok/s)\n",
      "step  256/1000 | train loss 1.475537 | norm 1.7586 | lr 2.54e-04 | (60.51 ms | 2115 tok/s)\n",
      "step  257/1000 | train loss 1.469397 | norm 2.1237 | lr 2.54e-04 | (58.53 ms | 2187 tok/s)\n",
      "step  258/1000 | train loss 1.461536 | norm 1.8462 | lr 2.54e-04 | (59.46 ms | 2153 tok/s)\n",
      "step  259/1000 | train loss 1.455076 | norm 2.0145 | lr 2.53e-04 | (59.91 ms | 2137 tok/s)\n",
      "step  260/1000 | train loss 1.447790 | norm 1.7478 | lr 2.53e-04 | (58.57 ms | 2185 tok/s)\n",
      "step  261/1000 | train loss 1.441457 | norm 2.5119 | lr 2.53e-04 | (59.95 ms | 2135 tok/s)\n",
      "step  262/1000 | train loss 1.433876 | norm 1.8980 | lr 2.52e-04 | (61.29 ms | 2088 tok/s)\n",
      "step  263/1000 | train loss 1.427300 | norm 1.7792 | lr 2.52e-04 | (60.47 ms | 2117 tok/s)\n",
      "step  264/1000 | train loss 1.420323 | norm 1.8458 | lr 2.52e-04 | (59.37 ms | 2156 tok/s)\n",
      "step  265/1000 | train loss 1.413637 | norm 2.0897 | lr 2.51e-04 | (58.30 ms | 2195 tok/s)\n",
      "step  266/1000 | train loss 1.406654 | norm 1.9442 | lr 2.51e-04 | (59.36 ms | 2156 tok/s)\n",
      "step  267/1000 | train loss 1.399754 | norm 1.7168 | lr 2.51e-04 | (60.14 ms | 2128 tok/s)\n",
      "step  268/1000 | train loss 1.393140 | norm 2.0410 | lr 2.50e-04 | (59.84 ms | 2139 tok/s)\n",
      "step  269/1000 | train loss 1.386269 | norm 2.1942 | lr 2.50e-04 | (60.39 ms | 2119 tok/s)\n",
      "step  270/1000 | train loss 1.379669 | norm 2.0633 | lr 2.50e-04 | (59.49 ms | 2151 tok/s)\n",
      "step  271/1000 | train loss 1.372883 | norm 2.0755 | lr 2.49e-04 | (58.92 ms | 2173 tok/s)\n",
      "step  272/1000 | train loss 1.366229 | norm 1.6364 | lr 2.49e-04 | (59.11 ms | 2166 tok/s)\n",
      "step  273/1000 | train loss 1.359531 | norm 1.5901 | lr 2.48e-04 | (96.48 ms | 1327 tok/s)\n",
      "step  274/1000 | train loss 1.352882 | norm 2.0478 | lr 2.48e-04 | (65.68 ms | 1949 tok/s)\n",
      "step  275/1000 | train loss 1.346386 | norm 1.9476 | lr 2.48e-04 | (62.16 ms | 2059 tok/s)\n",
      "step  276/1000 | train loss 1.339580 | norm 1.9897 | lr 2.47e-04 | (61.04 ms | 2097 tok/s)\n",
      "step  277/1000 | train loss 1.333292 | norm 2.2526 | lr 2.47e-04 | (59.64 ms | 2146 tok/s)\n",
      "step  278/1000 | train loss 1.326362 | norm 1.5454 | lr 2.47e-04 | (58.72 ms | 2180 tok/s)\n",
      "step  279/1000 | train loss 1.320134 | norm 1.6863 | lr 2.46e-04 | (59.55 ms | 2149 tok/s)\n",
      "step  280/1000 | train loss 1.313454 | norm 1.7427 | lr 2.46e-04 | (60.90 ms | 2102 tok/s)\n",
      "step  281/1000 | train loss 1.307445 | norm 2.0855 | lr 2.46e-04 | (59.95 ms | 2135 tok/s)\n",
      "step  282/1000 | train loss 1.300719 | norm 1.8432 | lr 2.45e-04 | (60.03 ms | 2132 tok/s)\n",
      "step  283/1000 | train loss 1.293949 | norm 1.6361 | lr 2.45e-04 | (58.58 ms | 2185 tok/s)\n",
      "step  284/1000 | train loss 1.287425 | norm 1.4335 | lr 2.45e-04 | (58.41 ms | 2191 tok/s)\n",
      "step  285/1000 | train loss 1.281353 | norm 1.6517 | lr 2.44e-04 | (101.43 ms | 1262 tok/s)\n",
      "step  286/1000 | train loss 1.275540 | norm 2.2572 | lr 2.44e-04 | (61.61 ms | 2077 tok/s)\n",
      "step  287/1000 | train loss 1.268149 | norm 1.6291 | lr 2.43e-04 | (59.59 ms | 2148 tok/s)\n",
      "step  288/1000 | train loss 1.262251 | norm 2.0672 | lr 2.43e-04 | (58.90 ms | 2173 tok/s)\n",
      "step  289/1000 | train loss 1.255994 | norm 2.0776 | lr 2.43e-04 | (58.73 ms | 2180 tok/s)\n",
      "step  290/1000 | train loss 1.249347 | norm 1.6595 | lr 2.42e-04 | (60.44 ms | 2118 tok/s)\n",
      "step  291/1000 | train loss 1.243598 | norm 1.9533 | lr 2.42e-04 | (59.47 ms | 2152 tok/s)\n",
      "step  292/1000 | train loss 1.237253 | norm 1.9003 | lr 2.42e-04 | (58.16 ms | 2201 tok/s)\n",
      "step  293/1000 | train loss 1.231051 | norm 1.8548 | lr 2.41e-04 | (60.85 ms | 2103 tok/s)\n",
      "step  294/1000 | train loss 1.225085 | norm 1.8571 | lr 2.41e-04 | (61.07 ms | 2096 tok/s)\n",
      "step  295/1000 | train loss 1.218807 | norm 1.6481 | lr 2.40e-04 | (97.17 ms | 1317 tok/s)\n",
      "step  296/1000 | train loss 1.213019 | norm 1.6102 | lr 2.40e-04 | (64.45 ms | 1986 tok/s)\n",
      "step  297/1000 | train loss 1.206798 | norm 1.8591 | lr 2.40e-04 | (60.60 ms | 2112 tok/s)\n",
      "step  298/1000 | train loss 1.200929 | norm 2.0320 | lr 2.39e-04 | (63.76 ms | 2008 tok/s)\n",
      "step  299/1000 | train loss 1.194819 | norm 1.7294 | lr 2.39e-04 | (118.80 ms | 1077 tok/s)\n",
      "step  300/1000 | train loss 1.188614 | norm 1.4809 | lr 2.39e-04 | (68.70 ms | 1863 tok/s)\n",
      "step  301/1000 | train loss 1.182819 | norm 1.6487 | lr 2.38e-04 | (69.50 ms | 1842 tok/s)\n",
      "step  302/1000 | train loss 1.177072 | norm 1.9587 | lr 2.38e-04 | (70.45 ms | 1817 tok/s)\n",
      "step  303/1000 | train loss 1.170811 | norm 1.6612 | lr 2.37e-04 | (68.46 ms | 1870 tok/s)\n",
      "step  304/1000 | train loss 1.165099 | norm 1.5818 | lr 2.37e-04 | (67.15 ms | 1906 tok/s)\n",
      "step  305/1000 | train loss 1.159580 | norm 1.8062 | lr 2.37e-04 | (63.12 ms | 2028 tok/s)\n",
      "step  306/1000 | train loss 1.153615 | norm 1.8254 | lr 2.36e-04 | (62.09 ms | 2062 tok/s)\n",
      "step  307/1000 | train loss 1.147613 | norm 1.8455 | lr 2.36e-04 | (62.23 ms | 2057 tok/s)\n",
      "step  308/1000 | train loss 1.142032 | norm 1.7501 | lr 2.35e-04 | (62.27 ms | 2056 tok/s)\n",
      "step  309/1000 | train loss 1.136065 | norm 1.6065 | lr 2.35e-04 | (63.15 ms | 2027 tok/s)\n",
      "step  310/1000 | train loss 1.130339 | norm 1.6194 | lr 2.35e-04 | (68.75 ms | 1862 tok/s)\n",
      "step  311/1000 | train loss 1.124834 | norm 1.7858 | lr 2.34e-04 | (68.89 ms | 1858 tok/s)\n",
      "step  312/1000 | train loss 1.119214 | norm 1.9838 | lr 2.34e-04 | (66.95 ms | 1912 tok/s)\n",
      "step  313/1000 | train loss 1.113273 | norm 1.4183 | lr 2.34e-04 | (68.82 ms | 1860 tok/s)\n",
      "step  314/1000 | train loss 1.107981 | norm 1.5568 | lr 2.33e-04 | (66.14 ms | 1935 tok/s)\n",
      "step  315/1000 | train loss 1.102356 | norm 1.6996 | lr 2.33e-04 | (65.38 ms | 1958 tok/s)\n",
      "step  316/1000 | train loss 1.096756 | norm 1.7658 | lr 2.32e-04 | (63.88 ms | 2004 tok/s)\n",
      "step  317/1000 | train loss 1.091036 | norm 1.7166 | lr 2.32e-04 | (63.79 ms | 2007 tok/s)\n",
      "step  318/1000 | train loss 1.085663 | norm 1.6025 | lr 2.32e-04 | (62.31 ms | 2054 tok/s)\n",
      "step  319/1000 | train loss 1.080438 | norm 2.0085 | lr 2.31e-04 | (65.78 ms | 1946 tok/s)\n",
      "step  320/1000 | train loss 1.074874 | norm 1.8850 | lr 2.31e-04 | (68.17 ms | 1878 tok/s)\n",
      "step  321/1000 | train loss 1.069185 | norm 1.3733 | lr 2.30e-04 | (69.31 ms | 1847 tok/s)\n",
      "step  322/1000 | train loss 1.063913 | norm 1.5574 | lr 2.30e-04 | (68.31 ms | 1874 tok/s)\n",
      "step  323/1000 | train loss 1.058884 | norm 2.1199 | lr 2.30e-04 | (67.35 ms | 1900 tok/s)\n",
      "step  324/1000 | train loss 1.053042 | norm 1.3598 | lr 2.29e-04 | (64.18 ms | 1994 tok/s)\n",
      "step  325/1000 | train loss 1.048502 | norm 1.9754 | lr 2.29e-04 | (65.54 ms | 1953 tok/s)\n",
      "step  326/1000 | train loss 1.042561 | norm 1.4670 | lr 2.28e-04 | (63.27 ms | 2023 tok/s)\n",
      "step  327/1000 | train loss 1.037736 | norm 1.6995 | lr 2.28e-04 | (63.84 ms | 2005 tok/s)\n",
      "step  328/1000 | train loss 1.032104 | norm 1.5186 | lr 2.28e-04 | (66.62 ms | 1921 tok/s)\n",
      "step  329/1000 | train loss 1.027075 | norm 1.5803 | lr 2.27e-04 | (69.14 ms | 1851 tok/s)\n",
      "step  330/1000 | train loss 1.021894 | norm 1.6113 | lr 2.27e-04 | (67.71 ms | 1891 tok/s)\n",
      "step  331/1000 | train loss 1.016827 | norm 1.8274 | lr 2.26e-04 | (65.19 ms | 1963 tok/s)\n",
      "step  332/1000 | train loss 1.011465 | norm 1.5776 | lr 2.26e-04 | (65.80 ms | 1945 tok/s)\n",
      "step  333/1000 | train loss 1.006464 | norm 1.6231 | lr 2.26e-04 | (66.55 ms | 1923 tok/s)\n",
      "step  334/1000 | train loss 1.001137 | norm 1.3154 | lr 2.25e-04 | (63.32 ms | 2021 tok/s)\n",
      "step  335/1000 | train loss 0.996226 | norm 1.3043 | lr 2.25e-04 | (65.14 ms | 1965 tok/s)\n",
      "step  336/1000 | train loss 0.990985 | norm 1.4524 | lr 2.24e-04 | (63.64 ms | 2011 tok/s)\n",
      "step  337/1000 | train loss 0.985953 | norm 1.6298 | lr 2.24e-04 | (62.06 ms | 2063 tok/s)\n",
      "step  338/1000 | train loss 0.980830 | norm 1.7022 | lr 2.23e-04 | (61.85 ms | 2070 tok/s)\n",
      "step  339/1000 | train loss 0.975904 | norm 1.7111 | lr 2.23e-04 | (62.21 ms | 2058 tok/s)\n",
      "step  340/1000 | train loss 0.970699 | norm 1.4125 | lr 2.23e-04 | (62.07 ms | 2062 tok/s)\n",
      "step  341/1000 | train loss 0.966149 | norm 1.9167 | lr 2.22e-04 | (62.28 ms | 2055 tok/s)\n",
      "step  342/1000 | train loss 0.961087 | norm 1.7520 | lr 2.22e-04 | (63.39 ms | 2019 tok/s)\n",
      "step  343/1000 | train loss 0.956054 | norm 1.4099 | lr 2.21e-04 | (60.60 ms | 2112 tok/s)\n",
      "step  344/1000 | train loss 0.951191 | norm 1.3416 | lr 2.21e-04 | (61.64 ms | 2076 tok/s)\n",
      "step  345/1000 | train loss 0.946498 | norm 1.5420 | lr 2.21e-04 | (61.94 ms | 2067 tok/s)\n",
      "step  346/1000 | train loss 0.941926 | norm 1.7758 | lr 2.20e-04 | (62.39 ms | 2052 tok/s)\n",
      "step  347/1000 | train loss 0.936566 | norm 1.1012 | lr 2.20e-04 | (62.06 ms | 2063 tok/s)\n",
      "step  348/1000 | train loss 0.931944 | norm 1.2444 | lr 2.19e-04 | (60.56 ms | 2114 tok/s)\n",
      "step  349/1000 | train loss 0.927499 | norm 1.8437 | lr 2.19e-04 | (62.73 ms | 2041 tok/s)\n",
      "step  350/1000 | train loss 0.922552 | norm 1.7159 | lr 2.19e-04 | (62.16 ms | 2059 tok/s)\n",
      "step  351/1000 | train loss 0.917654 | norm 1.2901 | lr 2.18e-04 | (65.04 ms | 1968 tok/s)\n",
      "step  352/1000 | train loss 0.913134 | norm 1.3338 | lr 2.18e-04 | (67.58 ms | 1894 tok/s)\n",
      "step  353/1000 | train loss 0.909133 | norm 1.8517 | lr 2.17e-04 | (68.04 ms | 1881 tok/s)\n",
      "step  354/1000 | train loss 0.903500 | norm 1.2107 | lr 2.17e-04 | (62.63 ms | 2044 tok/s)\n",
      "step  355/1000 | train loss 0.899992 | norm 2.1160 | lr 2.16e-04 | (61.21 ms | 2091 tok/s)\n",
      "step  356/1000 | train loss 0.894499 | norm 1.2016 | lr 2.16e-04 | (59.97 ms | 2134 tok/s)\n",
      "step  357/1000 | train loss 0.890841 | norm 1.9975 | lr 2.16e-04 | (61.38 ms | 2085 tok/s)\n",
      "step  358/1000 | train loss 0.885844 | norm 1.3951 | lr 2.15e-04 | (63.39 ms | 2019 tok/s)\n",
      "step  359/1000 | train loss 0.881621 | norm 1.5291 | lr 2.15e-04 | (60.67 ms | 2110 tok/s)\n",
      "step  360/1000 | train loss 0.877077 | norm 1.3414 | lr 2.14e-04 | (60.66 ms | 2110 tok/s)\n",
      "step  361/1000 | train loss 0.872684 | norm 1.4762 | lr 2.14e-04 | (61.92 ms | 2067 tok/s)\n",
      "step  362/1000 | train loss 0.868447 | norm 1.6448 | lr 2.13e-04 | (61.46 ms | 2083 tok/s)\n",
      "step  363/1000 | train loss 0.863848 | norm 1.4594 | lr 2.13e-04 | (60.81 ms | 2105 tok/s)\n",
      "step  364/1000 | train loss 0.859636 | norm 1.4557 | lr 2.13e-04 | (59.92 ms | 2136 tok/s)\n",
      "step  365/1000 | train loss 0.855134 | norm 1.2386 | lr 2.12e-04 | (61.79 ms | 2072 tok/s)\n",
      "step  366/1000 | train loss 0.850835 | norm 1.2558 | lr 2.12e-04 | (61.49 ms | 2082 tok/s)\n",
      "step  367/1000 | train loss 0.846542 | norm 1.3818 | lr 2.11e-04 | (60.25 ms | 2124 tok/s)\n",
      "step  368/1000 | train loss 0.842348 | norm 1.5701 | lr 2.11e-04 | (61.74 ms | 2073 tok/s)\n",
      "step  369/1000 | train loss 0.837918 | norm 1.3288 | lr 2.10e-04 | (61.98 ms | 2065 tok/s)\n",
      "step  370/1000 | train loss 0.833723 | norm 1.2843 | lr 2.10e-04 | (62.44 ms | 2050 tok/s)\n",
      "step  371/1000 | train loss 0.829419 | norm 1.2558 | lr 2.10e-04 | (60.00 ms | 2133 tok/s)\n",
      "step  372/1000 | train loss 0.825323 | norm 1.3946 | lr 2.09e-04 | (60.38 ms | 2120 tok/s)\n",
      "step  373/1000 | train loss 0.820899 | norm 1.3770 | lr 2.09e-04 | (61.70 ms | 2074 tok/s)\n",
      "step  374/1000 | train loss 0.816875 | norm 1.4688 | lr 2.08e-04 | (60.96 ms | 2100 tok/s)\n",
      "step  375/1000 | train loss 0.812470 | norm 1.0517 | lr 2.08e-04 | (59.71 ms | 2144 tok/s)\n",
      "step  376/1000 | train loss 0.808545 | norm 1.2214 | lr 2.07e-04 | (60.09 ms | 2130 tok/s)\n",
      "step  377/1000 | train loss 0.804244 | norm 1.2656 | lr 2.07e-04 | (60.17 ms | 2127 tok/s)\n",
      "step  378/1000 | train loss 0.800122 | norm 1.2846 | lr 2.07e-04 | (62.83 ms | 2037 tok/s)\n",
      "step  379/1000 | train loss 0.796012 | norm 1.2477 | lr 2.06e-04 | (60.28 ms | 2123 tok/s)\n",
      "step  380/1000 | train loss 0.792170 | norm 1.6401 | lr 2.06e-04 | (59.34 ms | 2157 tok/s)\n",
      "step  381/1000 | train loss 0.787792 | norm 1.0640 | lr 2.05e-04 | (62.97 ms | 2033 tok/s)\n",
      "step  382/1000 | train loss 0.783686 | norm 1.0180 | lr 2.05e-04 | (62.48 ms | 2049 tok/s)\n",
      "step  383/1000 | train loss 0.780060 | norm 1.7341 | lr 2.04e-04 | (60.64 ms | 2111 tok/s)\n",
      "step  384/1000 | train loss 0.775678 | norm 1.2565 | lr 2.04e-04 | (60.08 ms | 2131 tok/s)\n",
      "step  385/1000 | train loss 0.771785 | norm 1.3247 | lr 2.03e-04 | (60.86 ms | 2103 tok/s)\n",
      "step  386/1000 | train loss 0.768031 | norm 1.6605 | lr 2.03e-04 | (67.02 ms | 1910 tok/s)\n",
      "step  387/1000 | train loss 0.763834 | norm 1.0282 | lr 2.03e-04 | (66.83 ms | 1915 tok/s)\n",
      "step  388/1000 | train loss 0.760251 | norm 1.2772 | lr 2.02e-04 | (67.67 ms | 1892 tok/s)\n",
      "step  389/1000 | train loss 0.756098 | norm 1.0231 | lr 2.02e-04 | (66.95 ms | 1912 tok/s)\n",
      "step  390/1000 | train loss 0.752447 | norm 1.2425 | lr 2.01e-04 | (64.09 ms | 1997 tok/s)\n",
      "step  391/1000 | train loss 0.748707 | norm 1.5832 | lr 2.01e-04 | (64.46 ms | 1986 tok/s)\n",
      "step  392/1000 | train loss 0.744609 | norm 1.1164 | lr 2.00e-04 | (64.61 ms | 1981 tok/s)\n",
      "step  393/1000 | train loss 0.741119 | norm 1.2991 | lr 2.00e-04 | (62.26 ms | 2056 tok/s)\n",
      "step  394/1000 | train loss 0.737283 | norm 1.4531 | lr 1.99e-04 | (60.76 ms | 2107 tok/s)\n",
      "step  395/1000 | train loss 0.733534 | norm 1.1578 | lr 1.99e-04 | (60.69 ms | 2109 tok/s)\n",
      "step  396/1000 | train loss 0.729721 | norm 1.0767 | lr 1.99e-04 | (61.65 ms | 2076 tok/s)\n",
      "step  397/1000 | train loss 0.726258 | norm 1.5777 | lr 1.98e-04 | (62.43 ms | 2050 tok/s)\n",
      "step  398/1000 | train loss 0.722358 | norm 1.0518 | lr 1.98e-04 | (62.83 ms | 2037 tok/s)\n",
      "step  399/1000 | train loss 0.718843 | norm 1.1831 | lr 1.97e-04 | (61.16 ms | 2093 tok/s)\n",
      "step  400/1000 | train loss 0.715469 | norm 1.3447 | lr 1.97e-04 | (63.67 ms | 2011 tok/s)\n",
      "step  401/1000 | train loss 0.711559 | norm 1.0634 | lr 1.96e-04 | (65.69 ms | 1948 tok/s)\n",
      "step  402/1000 | train loss 0.708456 | norm 1.5893 | lr 1.96e-04 | (65.73 ms | 1947 tok/s)\n",
      "step  403/1000 | train loss 0.704382 | norm 0.9883 | lr 1.95e-04 | (68.76 ms | 1861 tok/s)\n",
      "step  404/1000 | train loss 0.701153 | norm 1.2759 | lr 1.95e-04 | (69.24 ms | 1849 tok/s)\n",
      "step  405/1000 | train loss 0.697488 | norm 1.1682 | lr 1.95e-04 | (68.99 ms | 1855 tok/s)\n",
      "step  406/1000 | train loss 0.694042 | norm 1.3031 | lr 1.94e-04 | (65.84 ms | 1944 tok/s)\n",
      "step  407/1000 | train loss 0.690656 | norm 1.2351 | lr 1.94e-04 | (65.80 ms | 1945 tok/s)\n",
      "step  408/1000 | train loss 0.687037 | norm 1.0658 | lr 1.93e-04 | (63.72 ms | 2009 tok/s)\n",
      "step  409/1000 | train loss 0.684154 | norm 1.5545 | lr 1.93e-04 | (61.76 ms | 2072 tok/s)\n",
      "step  410/1000 | train loss 0.680308 | norm 1.0451 | lr 1.92e-04 | (59.32 ms | 2158 tok/s)\n",
      "step  411/1000 | train loss 0.677026 | norm 1.1834 | lr 1.92e-04 | (60.76 ms | 2107 tok/s)\n",
      "step  412/1000 | train loss 0.673637 | norm 1.1143 | lr 1.91e-04 | (61.54 ms | 2080 tok/s)\n",
      "step  413/1000 | train loss 0.670491 | norm 1.4470 | lr 1.91e-04 | (59.93 ms | 2136 tok/s)\n",
      "step  414/1000 | train loss 0.666899 | norm 0.8644 | lr 1.90e-04 | (60.65 ms | 2111 tok/s)\n",
      "step  415/1000 | train loss 0.663862 | norm 1.2678 | lr 1.90e-04 | (61.20 ms | 2091 tok/s)\n",
      "step  416/1000 | train loss 0.660481 | norm 1.1744 | lr 1.90e-04 | (60.73 ms | 2108 tok/s)\n",
      "step  417/1000 | train loss 0.657417 | norm 1.3043 | lr 1.89e-04 | (59.56 ms | 2149 tok/s)\n",
      "step  418/1000 | train loss 0.653805 | norm 0.8882 | lr 1.89e-04 | (61.06 ms | 2096 tok/s)\n",
      "step  419/1000 | train loss 0.650888 | norm 1.2610 | lr 1.88e-04 | (62.02 ms | 2064 tok/s)\n",
      "step  420/1000 | train loss 0.647540 | norm 1.1923 | lr 1.88e-04 | (63.79 ms | 2007 tok/s)\n",
      "step  421/1000 | train loss 0.644292 | norm 0.9760 | lr 1.87e-04 | (60.18 ms | 2127 tok/s)\n",
      "step  422/1000 | train loss 0.641143 | norm 0.9552 | lr 1.87e-04 | (59.11 ms | 2166 tok/s)\n",
      "step  423/1000 | train loss 0.638133 | norm 1.2878 | lr 1.86e-04 | (59.67 ms | 2145 tok/s)\n",
      "step  424/1000 | train loss 0.635001 | norm 1.2207 | lr 1.86e-04 | (62.36 ms | 2053 tok/s)\n",
      "step  425/1000 | train loss 0.631814 | norm 1.1988 | lr 1.85e-04 | (60.44 ms | 2118 tok/s)\n",
      "step  426/1000 | train loss 0.628721 | norm 1.2060 | lr 1.85e-04 | (61.20 ms | 2091 tok/s)\n",
      "step  427/1000 | train loss 0.625638 | norm 1.1592 | lr 1.85e-04 | (65.36 ms | 1959 tok/s)\n",
      "step  428/1000 | train loss 0.622693 | norm 1.1716 | lr 1.84e-04 | (69.08 ms | 1853 tok/s)\n",
      "step  429/1000 | train loss 0.619599 | norm 0.9680 | lr 1.84e-04 | (67.08 ms | 1908 tok/s)\n",
      "step  430/1000 | train loss 0.616972 | norm 1.3953 | lr 1.83e-04 | (66.43 ms | 1927 tok/s)\n",
      "step  431/1000 | train loss 0.613555 | norm 0.9426 | lr 1.83e-04 | (64.93 ms | 1971 tok/s)\n",
      "step  432/1000 | train loss 0.610720 | norm 1.2183 | lr 1.82e-04 | (63.39 ms | 2019 tok/s)\n",
      "step  433/1000 | train loss 0.607814 | norm 1.2294 | lr 1.82e-04 | (61.91 ms | 2067 tok/s)\n",
      "step  434/1000 | train loss 0.604644 | norm 0.8621 | lr 1.81e-04 | (63.96 ms | 2001 tok/s)\n",
      "step  435/1000 | train loss 0.601925 | norm 1.1614 | lr 1.81e-04 | (62.31 ms | 2054 tok/s)\n",
      "step  436/1000 | train loss 0.598966 | norm 1.1184 | lr 1.80e-04 | (61.97 ms | 2066 tok/s)\n",
      "step  437/1000 | train loss 0.596062 | norm 0.9654 | lr 1.80e-04 | (60.69 ms | 2109 tok/s)\n",
      "step  438/1000 | train loss 0.593251 | norm 1.0395 | lr 1.79e-04 | (61.57 ms | 2079 tok/s)\n",
      "step  439/1000 | train loss 0.590344 | norm 0.9757 | lr 1.79e-04 | (61.14 ms | 2094 tok/s)\n",
      "step  440/1000 | train loss 0.587554 | norm 0.9715 | lr 1.79e-04 | (58.73 ms | 2180 tok/s)\n",
      "step  441/1000 | train loss 0.584754 | norm 1.1907 | lr 1.78e-04 | (59.51 ms | 2151 tok/s)\n",
      "step  442/1000 | train loss 0.581835 | norm 1.1656 | lr 1.78e-04 | (61.07 ms | 2096 tok/s)\n",
      "step  443/1000 | train loss 0.579126 | norm 1.0986 | lr 1.77e-04 | (60.73 ms | 2108 tok/s)\n",
      "step  444/1000 | train loss 0.576196 | norm 0.9620 | lr 1.77e-04 | (60.13 ms | 2129 tok/s)\n",
      "step  445/1000 | train loss 0.573734 | norm 1.2355 | lr 1.76e-04 | (61.75 ms | 2073 tok/s)\n",
      "step  446/1000 | train loss 0.570809 | norm 1.0507 | lr 1.76e-04 | (61.67 ms | 2076 tok/s)\n",
      "step  447/1000 | train loss 0.567904 | norm 0.7919 | lr 1.75e-04 | (60.70 ms | 2109 tok/s)\n",
      "step  448/1000 | train loss 0.565302 | norm 0.9327 | lr 1.75e-04 | (60.78 ms | 2106 tok/s)\n",
      "step  449/1000 | train loss 0.562611 | norm 1.0127 | lr 1.74e-04 | (61.21 ms | 2091 tok/s)\n",
      "step  450/1000 | train loss 0.559959 | norm 0.9873 | lr 1.74e-04 | (63.72 ms | 2009 tok/s)\n",
      "step  451/1000 | train loss 0.557277 | norm 1.0043 | lr 1.73e-04 | (63.07 ms | 2030 tok/s)\n",
      "step  452/1000 | train loss 0.554586 | norm 1.0710 | lr 1.73e-04 | (60.48 ms | 2116 tok/s)\n",
      "step  453/1000 | train loss 0.551943 | norm 1.0367 | lr 1.73e-04 | (60.43 ms | 2118 tok/s)\n",
      "step  454/1000 | train loss 0.549248 | norm 0.9027 | lr 1.72e-04 | (62.05 ms | 2063 tok/s)\n",
      "step  455/1000 | train loss 0.546728 | norm 0.9603 | lr 1.72e-04 | (65.36 ms | 1959 tok/s)\n",
      "step  456/1000 | train loss 0.544356 | norm 1.2691 | lr 1.71e-04 | (67.65 ms | 1892 tok/s)\n",
      "step  457/1000 | train loss 0.541497 | norm 0.9925 | lr 1.71e-04 | (65.78 ms | 1946 tok/s)\n",
      "step  458/1000 | train loss 0.539080 | norm 1.0387 | lr 1.70e-04 | (65.71 ms | 1948 tok/s)\n",
      "step  459/1000 | train loss 0.536513 | norm 0.9951 | lr 1.70e-04 | (65.29 ms | 1961 tok/s)\n",
      "step  460/1000 | train loss 0.533919 | norm 0.8724 | lr 1.69e-04 | (63.01 ms | 2031 tok/s)\n",
      "step  461/1000 | train loss 0.531407 | norm 0.8642 | lr 1.69e-04 | (76.30 ms | 1678 tok/s)\n",
      "step  462/1000 | train loss 0.528876 | norm 0.8560 | lr 1.68e-04 | (64.66 ms | 1980 tok/s)\n",
      "step  463/1000 | train loss 0.526457 | norm 0.8754 | lr 1.68e-04 | (61.52 ms | 2081 tok/s)\n",
      "step  464/1000 | train loss 0.523957 | norm 0.8851 | lr 1.67e-04 | (60.49 ms | 2116 tok/s)\n",
      "step  465/1000 | train loss 0.521492 | norm 0.8468 | lr 1.67e-04 | (62.04 ms | 2063 tok/s)\n",
      "step  466/1000 | train loss 0.519085 | norm 0.8338 | lr 1.66e-04 | (61.19 ms | 2092 tok/s)\n",
      "step  467/1000 | train loss 0.516624 | norm 0.8403 | lr 1.66e-04 | (62.34 ms | 2053 tok/s)\n",
      "step  468/1000 | train loss 0.514339 | norm 0.9295 | lr 1.66e-04 | (60.15 ms | 2128 tok/s)\n",
      "step  469/1000 | train loss 0.512063 | norm 1.0729 | lr 1.65e-04 | (62.30 ms | 2055 tok/s)\n",
      "step  470/1000 | train loss 0.509627 | norm 1.0869 | lr 1.65e-04 | (62.17 ms | 2059 tok/s)\n",
      "step  471/1000 | train loss 0.507291 | norm 1.0668 | lr 1.64e-04 | (60.98 ms | 2099 tok/s)\n",
      "step  472/1000 | train loss 0.504854 | norm 0.8689 | lr 1.64e-04 | (59.55 ms | 2150 tok/s)\n",
      "step  473/1000 | train loss 0.502530 | norm 0.7370 | lr 1.63e-04 | (62.16 ms | 2059 tok/s)\n",
      "step  474/1000 | train loss 0.500269 | norm 0.8644 | lr 1.63e-04 | (62.04 ms | 2063 tok/s)\n",
      "step  475/1000 | train loss 0.498051 | norm 1.0494 | lr 1.62e-04 | (59.51 ms | 2151 tok/s)\n",
      "step  476/1000 | train loss 0.495781 | norm 0.9730 | lr 1.62e-04 | (60.20 ms | 2126 tok/s)\n",
      "step  477/1000 | train loss 0.493512 | norm 0.9362 | lr 1.61e-04 | (61.64 ms | 2077 tok/s)\n",
      "step  478/1000 | train loss 0.491278 | norm 0.9218 | lr 1.61e-04 | (63.77 ms | 2007 tok/s)\n",
      "step  479/1000 | train loss 0.488966 | norm 0.6875 | lr 1.60e-04 | (62.02 ms | 2064 tok/s)\n",
      "step  480/1000 | train loss 0.486730 | norm 0.6752 | lr 1.60e-04 | (61.65 ms | 2076 tok/s)\n",
      "step  481/1000 | train loss 0.484663 | norm 0.9033 | lr 1.59e-04 | (63.61 ms | 2012 tok/s)\n",
      "step  482/1000 | train loss 0.482441 | norm 0.8574 | lr 1.59e-04 | (61.67 ms | 2076 tok/s)\n",
      "step  483/1000 | train loss 0.480214 | norm 0.7314 | lr 1.58e-04 | (60.98 ms | 2099 tok/s)\n",
      "step  484/1000 | train loss 0.478126 | norm 0.7815 | lr 1.58e-04 | (60.06 ms | 2131 tok/s)\n",
      "step  485/1000 | train loss 0.476027 | norm 0.8351 | lr 1.58e-04 | (61.03 ms | 2097 tok/s)\n",
      "step  486/1000 | train loss 0.474064 | norm 0.9393 | lr 1.57e-04 | (61.82 ms | 2070 tok/s)\n",
      "step  487/1000 | train loss 0.472241 | norm 1.3090 | lr 1.57e-04 | (59.67 ms | 2145 tok/s)\n",
      "step  488/1000 | train loss 0.469686 | norm 0.8248 | lr 1.56e-04 | (59.27 ms | 2159 tok/s)\n",
      "step  489/1000 | train loss 0.467902 | norm 0.9907 | lr 1.56e-04 | (61.36 ms | 2086 tok/s)\n",
      "step  490/1000 | train loss 0.465987 | norm 1.1418 | lr 1.55e-04 | (61.89 ms | 2068 tok/s)\n",
      "step  491/1000 | train loss 0.463612 | norm 0.8190 | lr 1.55e-04 | (60.27 ms | 2124 tok/s)\n",
      "step  492/1000 | train loss 0.461848 | norm 1.0206 | lr 1.54e-04 | (59.55 ms | 2149 tok/s)\n",
      "step  493/1000 | train loss 0.459789 | norm 0.9422 | lr 1.54e-04 | (97.64 ms | 1311 tok/s)\n",
      "step  494/1000 | train loss 0.457657 | norm 0.7459 | lr 1.53e-04 | (63.85 ms | 2005 tok/s)\n",
      "step  495/1000 | train loss 0.455801 | norm 0.8874 | lr 1.53e-04 | (60.63 ms | 2111 tok/s)\n",
      "step  496/1000 | train loss 0.453824 | norm 0.8494 | lr 1.52e-04 | (60.10 ms | 2130 tok/s)\n",
      "step  497/1000 | train loss 0.451735 | norm 0.6696 | lr 1.52e-04 | (63.43 ms | 2018 tok/s)\n",
      "step  498/1000 | train loss 0.449956 | norm 0.8037 | lr 1.51e-04 | (62.04 ms | 2063 tok/s)\n",
      "step  499/1000 | train loss 0.447902 | norm 0.6810 | lr 1.51e-04 | (60.14 ms | 2128 tok/s)\n",
      "step  500/1000 | train loss 0.446003 | norm 0.6829 | lr 1.50e-04 | (60.49 ms | 2116 tok/s)\n",
      "step  501/1000 | train loss 0.444157 | norm 0.7180 | lr 1.50e-04 | (61.56 ms | 2079 tok/s)\n",
      "step  502/1000 | train loss 0.442148 | norm 0.6122 | lr 1.50e-04 | (62.20 ms | 2058 tok/s)\n",
      "step  503/1000 | train loss 0.440376 | norm 0.7280 | lr 1.49e-04 | (59.38 ms | 2156 tok/s)\n",
      "step  504/1000 | train loss 0.438454 | norm 0.6751 | lr 1.49e-04 | (59.60 ms | 2148 tok/s)\n",
      "step  505/1000 | train loss 0.436624 | norm 0.7494 | lr 1.48e-04 | (61.84 ms | 2070 tok/s)\n",
      "step  506/1000 | train loss 0.434835 | norm 0.8879 | lr 1.48e-04 | (61.88 ms | 2069 tok/s)\n",
      "step  507/1000 | train loss 0.433088 | norm 1.0363 | lr 1.47e-04 | (61.26 ms | 2090 tok/s)\n",
      "step  508/1000 | train loss 0.431336 | norm 1.0820 | lr 1.47e-04 | (61.61 ms | 2078 tok/s)\n",
      "step  509/1000 | train loss 0.429399 | norm 0.8138 | lr 1.46e-04 | (64.63 ms | 1980 tok/s)\n",
      "step  510/1000 | train loss 0.427566 | norm 0.6770 | lr 1.46e-04 | (63.24 ms | 2024 tok/s)\n",
      "step  511/1000 | train loss 0.425741 | norm 0.6995 | lr 1.45e-04 | (61.71 ms | 2074 tok/s)\n",
      "step  512/1000 | train loss 0.424108 | norm 0.8916 | lr 1.45e-04 | (60.79 ms | 2106 tok/s)\n",
      "step  513/1000 | train loss 0.422307 | norm 0.8569 | lr 1.44e-04 | (64.03 ms | 1999 tok/s)\n",
      "step  514/1000 | train loss 0.420498 | norm 0.6550 | lr 1.44e-04 | (62.95 ms | 2034 tok/s)\n",
      "step  515/1000 | train loss 0.418753 | norm 0.6638 | lr 1.43e-04 | (59.67 ms | 2145 tok/s)\n",
      "step  516/1000 | train loss 0.417047 | norm 0.7583 | lr 1.43e-04 | (59.66 ms | 2146 tok/s)\n",
      "step  517/1000 | train loss 0.415349 | norm 0.7018 | lr 1.42e-04 | (59.96 ms | 2135 tok/s)\n",
      "step  518/1000 | train loss 0.413609 | norm 0.6420 | lr 1.42e-04 | (60.37 ms | 2120 tok/s)\n",
      "step  519/1000 | train loss 0.411920 | norm 0.6309 | lr 1.42e-04 | (61.50 ms | 2081 tok/s)\n",
      "step  520/1000 | train loss 0.410218 | norm 0.6360 | lr 1.41e-04 | (60.86 ms | 2103 tok/s)\n",
      "step  521/1000 | train loss 0.408551 | norm 0.6638 | lr 1.41e-04 | (61.03 ms | 2097 tok/s)\n",
      "step  522/1000 | train loss 0.406888 | norm 0.6470 | lr 1.40e-04 | (60.66 ms | 2110 tok/s)\n",
      "step  523/1000 | train loss 0.405267 | norm 0.6793 | lr 1.40e-04 | (59.07 ms | 2167 tok/s)\n",
      "step  524/1000 | train loss 0.403616 | norm 0.6730 | lr 1.39e-04 | (60.40 ms | 2119 tok/s)\n",
      "step  525/1000 | train loss 0.401951 | norm 0.6327 | lr 1.39e-04 | (60.51 ms | 2115 tok/s)\n",
      "step  526/1000 | train loss 0.400413 | norm 0.7709 | lr 1.38e-04 | (62.55 ms | 2046 tok/s)\n",
      "step  527/1000 | train loss 0.398850 | norm 0.8648 | lr 1.38e-04 | (60.16 ms | 2128 tok/s)\n",
      "step  528/1000 | train loss 0.397313 | norm 0.9625 | lr 1.37e-04 | (59.34 ms | 2157 tok/s)\n",
      "step  529/1000 | train loss 0.395638 | norm 0.8090 | lr 1.37e-04 | (62.34 ms | 2053 tok/s)\n",
      "step  530/1000 | train loss 0.393944 | norm 0.5698 | lr 1.36e-04 | (61.91 ms | 2068 tok/s)\n",
      "step  531/1000 | train loss 0.392396 | norm 0.5888 | lr 1.36e-04 | (59.52 ms | 2150 tok/s)\n",
      "step  532/1000 | train loss 0.390913 | norm 0.7216 | lr 1.35e-04 | (58.16 ms | 2201 tok/s)\n",
      "step  533/1000 | train loss 0.389364 | norm 0.7545 | lr 1.35e-04 | (59.44 ms | 2154 tok/s)\n",
      "step  534/1000 | train loss 0.387757 | norm 0.6010 | lr 1.34e-04 | (59.96 ms | 2135 tok/s)\n",
      "step  535/1000 | train loss 0.386231 | norm 0.5745 | lr 1.34e-04 | (59.43 ms | 2154 tok/s)\n",
      "step  536/1000 | train loss 0.384737 | norm 0.6568 | lr 1.34e-04 | (60.58 ms | 2113 tok/s)\n",
      "step  537/1000 | train loss 0.383243 | norm 0.6224 | lr 1.33e-04 | (60.58 ms | 2113 tok/s)\n",
      "step  538/1000 | train loss 0.381703 | norm 0.5906 | lr 1.33e-04 | (60.33 ms | 2122 tok/s)\n",
      "step  539/1000 | train loss 0.380226 | norm 0.6054 | lr 1.32e-04 | (60.30 ms | 2123 tok/s)\n",
      "step  540/1000 | train loss 0.378777 | norm 0.6885 | lr 1.32e-04 | (59.11 ms | 2165 tok/s)\n",
      "step  541/1000 | train loss 0.377290 | norm 0.6701 | lr 1.31e-04 | (60.60 ms | 2112 tok/s)\n",
      "step  542/1000 | train loss 0.375804 | norm 0.5539 | lr 1.31e-04 | (61.85 ms | 2069 tok/s)\n",
      "step  543/1000 | train loss 0.374395 | norm 0.6046 | lr 1.30e-04 | (59.28 ms | 2159 tok/s)\n",
      "step  544/1000 | train loss 0.373033 | norm 0.7284 | lr 1.30e-04 | (59.33 ms | 2157 tok/s)\n",
      "step  545/1000 | train loss 0.371635 | norm 0.7649 | lr 1.29e-04 | (59.92 ms | 2136 tok/s)\n",
      "step  546/1000 | train loss 0.370207 | norm 0.7844 | lr 1.29e-04 | (63.21 ms | 2025 tok/s)\n",
      "step  547/1000 | train loss 0.368771 | norm 0.8276 | lr 1.28e-04 | (61.75 ms | 2073 tok/s)\n",
      "step  548/1000 | train loss 0.367348 | norm 0.7385 | lr 1.28e-04 | (60.11 ms | 2130 tok/s)\n",
      "step  549/1000 | train loss 0.365888 | norm 0.5390 | lr 1.27e-04 | (62.30 ms | 2055 tok/s)\n",
      "step  550/1000 | train loss 0.364566 | norm 0.7403 | lr 1.27e-04 | (60.95 ms | 2100 tok/s)\n",
      "step  551/1000 | train loss 0.363231 | norm 0.8127 | lr 1.27e-04 | (59.52 ms | 2150 tok/s)\n",
      "step  552/1000 | train loss 0.361770 | norm 0.5960 | lr 1.26e-04 | (58.85 ms | 2175 tok/s)\n",
      "step  553/1000 | train loss 0.360421 | norm 0.5870 | lr 1.26e-04 | (59.90 ms | 2137 tok/s)\n",
      "step  554/1000 | train loss 0.359106 | norm 0.6524 | lr 1.25e-04 | (60.46 ms | 2117 tok/s)\n",
      "step  555/1000 | train loss 0.357740 | norm 0.5734 | lr 1.25e-04 | (60.19 ms | 2127 tok/s)\n",
      "step  556/1000 | train loss 0.356432 | norm 0.5829 | lr 1.24e-04 | (58.39 ms | 2192 tok/s)\n",
      "step  557/1000 | train loss 0.355122 | norm 0.6000 | lr 1.24e-04 | (61.56 ms | 2079 tok/s)\n",
      "step  558/1000 | train loss 0.353765 | norm 0.5058 | lr 1.23e-04 | (63.04 ms | 2031 tok/s)\n",
      "step  559/1000 | train loss 0.352505 | norm 0.6011 | lr 1.23e-04 | (60.07 ms | 2131 tok/s)\n",
      "step  560/1000 | train loss 0.351248 | norm 0.6439 | lr 1.22e-04 | (59.05 ms | 2168 tok/s)\n",
      "step  561/1000 | train loss 0.350015 | norm 0.6870 | lr 1.22e-04 | (63.50 ms | 2016 tok/s)\n",
      "step  562/1000 | train loss 0.348781 | norm 0.7354 | lr 1.21e-04 | (62.60 ms | 2045 tok/s)\n",
      "step  563/1000 | train loss 0.347468 | norm 0.7176 | lr 1.21e-04 | (60.06 ms | 2131 tok/s)\n",
      "step  564/1000 | train loss 0.346135 | norm 0.5628 | lr 1.21e-04 | (60.36 ms | 2121 tok/s)\n",
      "step  565/1000 | train loss 0.344903 | norm 0.6093 | lr 1.20e-04 | (60.70 ms | 2109 tok/s)\n",
      "step  566/1000 | train loss 0.343705 | norm 0.7126 | lr 1.20e-04 | (62.99 ms | 2032 tok/s)\n",
      "step  567/1000 | train loss 0.342462 | norm 0.6446 | lr 1.19e-04 | (67.61 ms | 1893 tok/s)\n",
      "step  568/1000 | train loss 0.341166 | norm 0.5032 | lr 1.19e-04 | (66.71 ms | 1919 tok/s)\n",
      "step  569/1000 | train loss 0.339986 | norm 0.5935 | lr 1.18e-04 | (66.79 ms | 1916 tok/s)\n",
      "step  570/1000 | train loss 0.338802 | norm 0.6227 | lr 1.18e-04 | (64.41 ms | 1987 tok/s)\n",
      "step  571/1000 | train loss 0.337581 | norm 0.5552 | lr 1.17e-04 | (61.69 ms | 2075 tok/s)\n",
      "step  572/1000 | train loss 0.336369 | norm 0.4976 | lr 1.17e-04 | (61.61 ms | 2078 tok/s)\n",
      "step  573/1000 | train loss 0.335191 | norm 0.5062 | lr 1.16e-04 | (60.93 ms | 2101 tok/s)\n",
      "step  574/1000 | train loss 0.334040 | norm 0.5531 | lr 1.16e-04 | (65.38 ms | 1958 tok/s)\n",
      "step  575/1000 | train loss 0.332866 | norm 0.4967 | lr 1.15e-04 | (64.68 ms | 1979 tok/s)\n",
      "step  576/1000 | train loss 0.331676 | norm 0.4795 | lr 1.15e-04 | (64.28 ms | 1991 tok/s)\n",
      "step  577/1000 | train loss 0.330524 | norm 0.4862 | lr 1.15e-04 | (66.66 ms | 1920 tok/s)\n",
      "step  578/1000 | train loss 0.329419 | norm 0.5393 | lr 1.14e-04 | (62.45 ms | 2050 tok/s)\n",
      "step  579/1000 | train loss 0.328276 | norm 0.5823 | lr 1.14e-04 | (60.70 ms | 2109 tok/s)\n",
      "step  580/1000 | train loss 0.327179 | norm 0.6268 | lr 1.13e-04 | (59.57 ms | 2149 tok/s)\n",
      "step  581/1000 | train loss 0.326094 | norm 0.6666 | lr 1.13e-04 | (61.25 ms | 2090 tok/s)\n",
      "step  582/1000 | train loss 0.325021 | norm 0.7619 | lr 1.12e-04 | (64.40 ms | 1988 tok/s)\n",
      "step  583/1000 | train loss 0.323941 | norm 0.7256 | lr 1.12e-04 | (66.19 ms | 1934 tok/s)\n",
      "step  584/1000 | train loss 0.322713 | norm 0.5599 | lr 1.11e-04 | (65.69 ms | 1949 tok/s)\n",
      "step  585/1000 | train loss 0.321606 | norm 0.4916 | lr 1.11e-04 | (65.82 ms | 1945 tok/s)\n",
      "step  586/1000 | train loss 0.320593 | norm 0.5938 | lr 1.10e-04 | (61.62 ms | 2077 tok/s)\n",
      "step  587/1000 | train loss 0.319520 | norm 0.6166 | lr 1.10e-04 | (61.21 ms | 2091 tok/s)\n",
      "step  588/1000 | train loss 0.318390 | norm 0.4850 | lr 1.10e-04 | (61.55 ms | 2079 tok/s)\n",
      "step  589/1000 | train loss 0.317330 | norm 0.4800 | lr 1.09e-04 | (61.90 ms | 2068 tok/s)\n",
      "step  590/1000 | train loss 0.316318 | norm 0.5624 | lr 1.09e-04 | (63.28 ms | 2023 tok/s)\n",
      "step  591/1000 | train loss 0.315248 | norm 0.4991 | lr 1.08e-04 | (65.07 ms | 1967 tok/s)\n",
      "step  592/1000 | train loss 0.314178 | norm 0.4539 | lr 1.08e-04 | (64.57 ms | 1982 tok/s)\n",
      "step  593/1000 | train loss 0.313174 | norm 0.5013 | lr 1.07e-04 | (65.41 ms | 1957 tok/s)\n",
      "step  594/1000 | train loss 0.312159 | norm 0.4842 | lr 1.07e-04 | (62.10 ms | 2061 tok/s)\n",
      "step  595/1000 | train loss 0.311111 | norm 0.4462 | lr 1.06e-04 | (60.20 ms | 2126 tok/s)\n",
      "step  596/1000 | train loss 0.310090 | norm 0.4426 | lr 1.06e-04 | (59.82 ms | 2140 tok/s)\n",
      "step  597/1000 | train loss 0.309114 | norm 0.4581 | lr 1.05e-04 | (62.44 ms | 2050 tok/s)\n",
      "step  598/1000 | train loss 0.308110 | norm 0.4657 | lr 1.05e-04 | (62.39 ms | 2052 tok/s)\n",
      "step  599/1000 | train loss 0.307095 | norm 0.4267 | lr 1.05e-04 | (62.58 ms | 2045 tok/s)\n",
      "step  600/1000 | train loss 0.306121 | norm 0.4456 | lr 1.04e-04 | (59.89 ms | 2137 tok/s)\n",
      "step  601/1000 | train loss 0.305154 | norm 0.4861 | lr 1.04e-04 | (61.84 ms | 2070 tok/s)\n",
      "step  602/1000 | train loss 0.304181 | norm 0.4778 | lr 1.03e-04 | (63.72 ms | 2009 tok/s)\n",
      "step  603/1000 | train loss 0.303216 | norm 0.4809 | lr 1.03e-04 | (62.21 ms | 2058 tok/s)\n",
      "step  604/1000 | train loss 0.302268 | norm 0.5073 | lr 1.02e-04 | (62.28 ms | 2055 tok/s)\n",
      "step  605/1000 | train loss 0.301343 | norm 0.5673 | lr 1.02e-04 | (63.84 ms | 2005 tok/s)\n",
      "step  606/1000 | train loss 0.300436 | norm 0.6446 | lr 1.01e-04 | (60.31 ms | 2122 tok/s)\n",
      "step  607/1000 | train loss 0.299494 | norm 0.6286 | lr 1.01e-04 | (60.55 ms | 2114 tok/s)\n",
      "step  608/1000 | train loss 0.298541 | norm 0.5574 | lr 1.01e-04 | (60.23 ms | 2125 tok/s)\n",
      "step  609/1000 | train loss 0.297591 | norm 0.5196 | lr 1.00e-04 | (62.81 ms | 2038 tok/s)\n",
      "step  610/1000 | train loss 0.296680 | norm 0.5661 | lr 9.96e-05 | (61.79 ms | 2072 tok/s)\n",
      "step  611/1000 | train loss 0.295780 | norm 0.5821 | lr 9.92e-05 | (60.99 ms | 2099 tok/s)\n",
      "step  612/1000 | train loss 0.294863 | norm 0.5271 | lr 9.87e-05 | (59.85 ms | 2139 tok/s)\n",
      "step  613/1000 | train loss 0.293947 | norm 0.4845 | lr 9.83e-05 | (62.67 ms | 2043 tok/s)\n",
      "step  614/1000 | train loss 0.293049 | norm 0.4766 | lr 9.79e-05 | (65.03 ms | 1968 tok/s)\n",
      "step  615/1000 | train loss 0.292156 | norm 0.4546 | lr 9.74e-05 | (67.74 ms | 1889 tok/s)\n",
      "step  616/1000 | train loss 0.291263 | norm 0.4522 | lr 9.70e-05 | (67.46 ms | 1897 tok/s)\n",
      "step  617/1000 | train loss 0.290403 | norm 0.4618 | lr 9.65e-05 | (65.84 ms | 1944 tok/s)\n",
      "step  618/1000 | train loss 0.289528 | norm 0.4363 | lr 9.61e-05 | (66.58 ms | 1923 tok/s)\n",
      "step  619/1000 | train loss 0.288652 | norm 0.4404 | lr 9.57e-05 | (63.37 ms | 2020 tok/s)\n",
      "step  620/1000 | train loss 0.287802 | norm 0.4365 | lr 9.52e-05 | (62.06 ms | 2063 tok/s)\n",
      "step  621/1000 | train loss 0.286945 | norm 0.4363 | lr 9.48e-05 | (62.55 ms | 2046 tok/s)\n",
      "step  622/1000 | train loss 0.286093 | norm 0.4348 | lr 9.43e-05 | (65.66 ms | 1949 tok/s)\n",
      "step  623/1000 | train loss 0.285267 | norm 0.4288 | lr 9.39e-05 | (62.12 ms | 2061 tok/s)\n",
      "step  624/1000 | train loss 0.284414 | norm 0.4143 | lr 9.35e-05 | (61.67 ms | 2076 tok/s)\n",
      "step  625/1000 | train loss 0.283583 | norm 0.4195 | lr 9.30e-05 | (62.62 ms | 2044 tok/s)\n",
      "step  626/1000 | train loss 0.282776 | norm 0.4324 | lr 9.26e-05 | (62.42 ms | 2051 tok/s)\n",
      "step  627/1000 | train loss 0.281947 | norm 0.4215 | lr 9.22e-05 | (60.40 ms | 2119 tok/s)\n",
      "step  628/1000 | train loss 0.281129 | norm 0.3947 | lr 9.17e-05 | (62.84 ms | 2037 tok/s)\n",
      "step  629/1000 | train loss 0.280340 | norm 0.4307 | lr 9.13e-05 | (63.60 ms | 2012 tok/s)\n",
      "step  630/1000 | train loss 0.279553 | norm 0.4825 | lr 9.09e-05 | (61.06 ms | 2096 tok/s)\n",
      "step  631/1000 | train loss 0.278798 | norm 0.5959 | lr 9.04e-05 | (61.34 ms | 2087 tok/s)\n",
      "step  632/1000 | train loss 0.278046 | norm 0.6559 | lr 9.00e-05 | (60.75 ms | 2107 tok/s)\n",
      "step  633/1000 | train loss 0.277231 | norm 0.5800 | lr 8.96e-05 | (61.02 ms | 2098 tok/s)\n",
      "step  634/1000 | train loss 0.276414 | norm 0.4408 | lr 8.91e-05 | (61.62 ms | 2077 tok/s)\n",
      "step  635/1000 | train loss 0.275648 | norm 0.5178 | lr 8.87e-05 | (60.81 ms | 2105 tok/s)\n",
      "step  636/1000 | train loss 0.274895 | norm 0.5402 | lr 8.83e-05 | (60.52 ms | 2115 tok/s)\n",
      "step  637/1000 | train loss 0.274116 | norm 0.4734 | lr 8.78e-05 | (63.65 ms | 2011 tok/s)\n",
      "step  638/1000 | train loss 0.273353 | norm 0.4794 | lr 8.74e-05 | (62.47 ms | 2049 tok/s)\n",
      "step  639/1000 | train loss 0.272605 | norm 0.4727 | lr 8.70e-05 | (61.53 ms | 2080 tok/s)\n",
      "step  640/1000 | train loss 0.271838 | norm 0.4031 | lr 8.66e-05 | (59.89 ms | 2137 tok/s)\n",
      "step  641/1000 | train loss 0.271111 | norm 0.4512 | lr 8.61e-05 | (61.53 ms | 2080 tok/s)\n",
      "step  642/1000 | train loss 0.270382 | norm 0.4631 | lr 8.57e-05 | (62.31 ms | 2054 tok/s)\n",
      "step  643/1000 | train loss 0.269631 | norm 0.4104 | lr 8.53e-05 | (60.83 ms | 2104 tok/s)\n",
      "step  644/1000 | train loss 0.268910 | norm 0.4205 | lr 8.49e-05 | (60.52 ms | 2115 tok/s)\n",
      "step  645/1000 | train loss 0.268200 | norm 0.4171 | lr 8.44e-05 | (61.64 ms | 2077 tok/s)\n",
      "step  646/1000 | train loss 0.267478 | norm 0.4082 | lr 8.40e-05 | (61.77 ms | 2072 tok/s)\n",
      "step  647/1000 | train loss 0.266767 | norm 0.3991 | lr 8.36e-05 | (63.34 ms | 2021 tok/s)\n",
      "step  648/1000 | train loss 0.266063 | norm 0.4200 | lr 8.32e-05 | (59.83 ms | 2139 tok/s)\n",
      "step  649/1000 | train loss 0.265360 | norm 0.4082 | lr 8.27e-05 | (61.52 ms | 2080 tok/s)\n",
      "step  650/1000 | train loss 0.264671 | norm 0.4152 | lr 8.23e-05 | (62.47 ms | 2049 tok/s)\n",
      "step  651/1000 | train loss 0.263985 | norm 0.4380 | lr 8.19e-05 | (59.30 ms | 2159 tok/s)\n",
      "step  652/1000 | train loss 0.263308 | norm 0.4534 | lr 8.15e-05 | (59.37 ms | 2156 tok/s)\n",
      "step  653/1000 | train loss 0.262630 | norm 0.4316 | lr 8.11e-05 | (61.12 ms | 2094 tok/s)\n",
      "step  654/1000 | train loss 0.261959 | norm 0.4536 | lr 8.06e-05 | (62.11 ms | 2061 tok/s)\n",
      "step  655/1000 | train loss 0.261307 | norm 0.4579 | lr 8.02e-05 | (60.06 ms | 2131 tok/s)\n",
      "step  656/1000 | train loss 0.260616 | norm 0.4358 | lr 7.98e-05 | (60.16 ms | 2128 tok/s)\n",
      "step  657/1000 | train loss 0.259942 | norm 0.3794 | lr 7.94e-05 | (62.10 ms | 2061 tok/s)\n",
      "step  658/1000 | train loss 0.259284 | norm 0.3876 | lr 7.90e-05 | (62.59 ms | 2045 tok/s)\n",
      "step  659/1000 | train loss 0.258636 | norm 0.4048 | lr 7.86e-05 | (61.17 ms | 2092 tok/s)\n",
      "step  660/1000 | train loss 0.257998 | norm 0.4115 | lr 7.82e-05 | (59.47 ms | 2152 tok/s)\n",
      "step  661/1000 | train loss 0.257351 | norm 0.4275 | lr 7.77e-05 | (61.15 ms | 2093 tok/s)\n",
      "step  662/1000 | train loss 0.256717 | norm 0.4126 | lr 7.73e-05 | (61.80 ms | 2071 tok/s)\n",
      "step  663/1000 | train loss 0.256069 | norm 0.3781 | lr 7.69e-05 | (60.32 ms | 2122 tok/s)\n",
      "step  664/1000 | train loss 0.255437 | norm 0.3620 | lr 7.65e-05 | (58.76 ms | 2178 tok/s)\n",
      "step  665/1000 | train loss 0.254827 | norm 0.4040 | lr 7.61e-05 | (60.05 ms | 2132 tok/s)\n",
      "step  666/1000 | train loss 0.254209 | norm 0.4167 | lr 7.57e-05 | (63.35 ms | 2021 tok/s)\n",
      "step  667/1000 | train loss 0.253593 | norm 0.4082 | lr 7.53e-05 | (61.15 ms | 2093 tok/s)\n",
      "step  668/1000 | train loss 0.252979 | norm 0.4039 | lr 7.49e-05 | (59.46 ms | 2153 tok/s)\n",
      "step  669/1000 | train loss 0.252369 | norm 0.3831 | lr 7.45e-05 | (61.60 ms | 2078 tok/s)\n",
      "step  670/1000 | train loss 0.251763 | norm 0.3530 | lr 7.40e-05 | (62.68 ms | 2042 tok/s)\n",
      "step  671/1000 | train loss 0.251165 | norm 0.3692 | lr 7.36e-05 | (59.97 ms | 2134 tok/s)\n",
      "step  672/1000 | train loss 0.250585 | norm 0.3811 | lr 7.32e-05 | (59.84 ms | 2139 tok/s)\n",
      "step  673/1000 | train loss 0.250010 | norm 0.4233 | lr 7.28e-05 | (63.82 ms | 2006 tok/s)\n",
      "step  674/1000 | train loss 0.249439 | norm 0.4282 | lr 7.24e-05 | (63.83 ms | 2005 tok/s)\n",
      "step  675/1000 | train loss 0.248856 | norm 0.4439 | lr 7.20e-05 | (60.66 ms | 2110 tok/s)\n",
      "step  676/1000 | train loss 0.248258 | norm 0.3936 | lr 7.16e-05 | (58.70 ms | 2181 tok/s)\n",
      "step  677/1000 | train loss 0.247675 | norm 0.3749 | lr 7.12e-05 | (61.64 ms | 2077 tok/s)\n",
      "step  678/1000 | train loss 0.247117 | norm 0.3955 | lr 7.08e-05 | (61.21 ms | 2091 tok/s)\n",
      "step  679/1000 | train loss 0.246570 | norm 0.4110 | lr 7.04e-05 | (59.64 ms | 2146 tok/s)\n",
      "step  680/1000 | train loss 0.245998 | norm 0.4046 | lr 7.00e-05 | (60.48 ms | 2116 tok/s)\n",
      "step  681/1000 | train loss 0.245431 | norm 0.3617 | lr 6.96e-05 | (60.51 ms | 2115 tok/s)\n",
      "step  682/1000 | train loss 0.244884 | norm 0.3580 | lr 6.92e-05 | (61.64 ms | 2076 tok/s)\n",
      "step  683/1000 | train loss 0.244350 | norm 0.3892 | lr 6.88e-05 | (62.51 ms | 2048 tok/s)\n",
      "step  684/1000 | train loss 0.243800 | norm 0.3610 | lr 6.84e-05 | (59.48 ms | 2152 tok/s)\n",
      "step  685/1000 | train loss 0.243248 | norm 0.3390 | lr 6.80e-05 | (62.41 ms | 2051 tok/s)\n",
      "step  686/1000 | train loss 0.242723 | norm 0.3562 | lr 6.76e-05 | (63.30 ms | 2022 tok/s)\n",
      "step  687/1000 | train loss 0.242199 | norm 0.3658 | lr 6.73e-05 | (106.26 ms | 1205 tok/s)\n",
      "step  688/1000 | train loss 0.241667 | norm 0.3618 | lr 6.69e-05 | (61.79 ms | 2072 tok/s)\n",
      "step  689/1000 | train loss 0.241140 | norm 0.3372 | lr 6.65e-05 | (63.46 ms | 2017 tok/s)\n",
      "step  690/1000 | train loss 0.240623 | norm 0.3504 | lr 6.61e-05 | (60.57 ms | 2113 tok/s)\n",
      "step  691/1000 | train loss 0.240115 | norm 0.3704 | lr 6.57e-05 | (59.48 ms | 2152 tok/s)\n",
      "step  692/1000 | train loss 0.239614 | norm 0.3769 | lr 6.53e-05 | (61.27 ms | 2089 tok/s)\n",
      "step  693/1000 | train loss 0.239121 | norm 0.4268 | lr 6.49e-05 | (61.93 ms | 2067 tok/s)\n",
      "step  694/1000 | train loss 0.238642 | norm 0.4987 | lr 6.45e-05 | (59.73 ms | 2143 tok/s)\n",
      "step  695/1000 | train loss 0.238146 | norm 0.5001 | lr 6.41e-05 | (60.62 ms | 2112 tok/s)\n",
      "step  696/1000 | train loss 0.237628 | norm 0.4248 | lr 6.37e-05 | (61.32 ms | 2088 tok/s)\n",
      "step  697/1000 | train loss 0.237120 | norm 0.3821 | lr 6.34e-05 | (62.55 ms | 2046 tok/s)\n",
      "step  698/1000 | train loss 0.236644 | norm 0.4249 | lr 6.30e-05 | (60.05 ms | 2131 tok/s)\n",
      "step  699/1000 | train loss 0.236168 | norm 0.4267 | lr 6.26e-05 | (60.29 ms | 2123 tok/s)\n",
      "step  700/1000 | train loss 0.235666 | norm 0.3708 | lr 6.22e-05 | (61.18 ms | 2092 tok/s)\n",
      "step  701/1000 | train loss 0.235195 | norm 0.3817 | lr 6.18e-05 | (62.27 ms | 2056 tok/s)\n",
      "step  702/1000 | train loss 0.234731 | norm 0.4066 | lr 6.15e-05 | (60.07 ms | 2131 tok/s)\n",
      "step  703/1000 | train loss 0.234248 | norm 0.3610 | lr 6.11e-05 | (59.93 ms | 2136 tok/s)\n",
      "step  704/1000 | train loss 0.233786 | norm 0.3631 | lr 6.07e-05 | (59.88 ms | 2138 tok/s)\n",
      "step  705/1000 | train loss 0.233326 | norm 0.3774 | lr 6.03e-05 | (63.23 ms | 2024 tok/s)\n",
      "step  706/1000 | train loss 0.232865 | norm 0.3557 | lr 5.99e-05 | (61.99 ms | 2065 tok/s)\n",
      "step  707/1000 | train loss 0.232406 | norm 0.3409 | lr 5.96e-05 | (60.41 ms | 2119 tok/s)\n",
      "step  708/1000 | train loss 0.231957 | norm 0.3576 | lr 5.92e-05 | (61.14 ms | 2094 tok/s)\n",
      "step  709/1000 | train loss 0.231513 | norm 0.3492 | lr 5.88e-05 | (61.42 ms | 2084 tok/s)\n",
      "step  710/1000 | train loss 0.231058 | norm 0.3250 | lr 5.84e-05 | (60.05 ms | 2132 tok/s)\n",
      "step  711/1000 | train loss 0.230624 | norm 0.3457 | lr 5.81e-05 | (62.47 ms | 2049 tok/s)\n",
      "step  712/1000 | train loss 0.230187 | norm 0.3376 | lr 5.77e-05 | (62.96 ms | 2033 tok/s)\n",
      "step  713/1000 | train loss 0.229747 | norm 0.3233 | lr 5.73e-05 | (62.10 ms | 2061 tok/s)\n",
      "step  714/1000 | train loss 0.229322 | norm 0.3372 | lr 5.70e-05 | (62.87 ms | 2036 tok/s)\n",
      "step  715/1000 | train loss 0.228892 | norm 0.3254 | lr 5.66e-05 | (61.83 ms | 2070 tok/s)\n",
      "step  716/1000 | train loss 0.228465 | norm 0.3201 | lr 5.62e-05 | (62.54 ms | 2047 tok/s)\n",
      "step  717/1000 | train loss 0.228050 | norm 0.3337 | lr 5.58e-05 | (62.83 ms | 2037 tok/s)\n",
      "step  718/1000 | train loss 0.227629 | norm 0.3197 | lr 5.55e-05 | (61.14 ms | 2094 tok/s)\n",
      "step  719/1000 | train loss 0.227214 | norm 0.3182 | lr 5.51e-05 | (61.10 ms | 2095 tok/s)\n",
      "step  720/1000 | train loss 0.226807 | norm 0.3279 | lr 5.47e-05 | (60.65 ms | 2110 tok/s)\n",
      "step  721/1000 | train loss 0.226396 | norm 0.3220 | lr 5.44e-05 | (64.03 ms | 1999 tok/s)\n",
      "step  722/1000 | train loss 0.225996 | norm 0.3195 | lr 5.40e-05 | (66.14 ms | 1935 tok/s)\n",
      "step  723/1000 | train loss 0.225609 | norm 0.3673 | lr 5.37e-05 | (67.56 ms | 1894 tok/s)\n",
      "step  724/1000 | train loss 0.225237 | norm 0.4161 | lr 5.33e-05 | (65.63 ms | 1950 tok/s)\n",
      "step  725/1000 | train loss 0.224876 | norm 0.5103 | lr 5.29e-05 | (62.44 ms | 2050 tok/s)\n",
      "step  726/1000 | train loss 0.224450 | norm 0.4283 | lr 5.26e-05 | (60.74 ms | 2107 tok/s)\n",
      "step  727/1000 | train loss 0.224027 | norm 0.3192 | lr 5.22e-05 | (61.24 ms | 2090 tok/s)\n",
      "step  728/1000 | train loss 0.223681 | norm 0.4289 | lr 5.19e-05 | (63.19 ms | 2026 tok/s)\n",
      "step  729/1000 | train loss 0.223285 | norm 0.3823 | lr 5.15e-05 | (62.18 ms | 2059 tok/s)\n",
      "step  730/1000 | train loss 0.222887 | norm 0.3223 | lr 5.12e-05 | (60.95 ms | 2100 tok/s)\n",
      "step  731/1000 | train loss 0.222539 | norm 0.3997 | lr 5.08e-05 | (62.39 ms | 2052 tok/s)\n",
      "step  732/1000 | train loss 0.222147 | norm 0.3369 | lr 5.05e-05 | (64.44 ms | 1986 tok/s)\n",
      "step  733/1000 | train loss 0.221778 | norm 0.3276 | lr 5.01e-05 | (67.38 ms | 1900 tok/s)\n",
      "step  734/1000 | train loss 0.221426 | norm 0.3738 | lr 4.97e-05 | (68.88 ms | 1858 tok/s)\n",
      "step  735/1000 | train loss 0.221045 | norm 0.3151 | lr 4.94e-05 | (64.80 ms | 1975 tok/s)\n",
      "step  736/1000 | train loss 0.220695 | norm 0.3342 | lr 4.90e-05 | (63.72 ms | 2009 tok/s)\n",
      "step  737/1000 | train loss 0.220339 | norm 0.3455 | lr 4.87e-05 | (62.37 ms | 2052 tok/s)\n",
      "step  738/1000 | train loss 0.219975 | norm 0.3139 | lr 4.84e-05 | (62.31 ms | 2054 tok/s)\n",
      "step  739/1000 | train loss 0.219636 | norm 0.3338 | lr 4.80e-05 | (62.95 ms | 2034 tok/s)\n",
      "step  740/1000 | train loss 0.219281 | norm 0.3212 | lr 4.77e-05 | (62.19 ms | 2058 tok/s)\n",
      "step  741/1000 | train loss 0.218935 | norm 0.3195 | lr 4.73e-05 | (60.05 ms | 2131 tok/s)\n",
      "step  742/1000 | train loss 0.218598 | norm 0.3202 | lr 4.70e-05 | (61.56 ms | 2079 tok/s)\n",
      "step  743/1000 | train loss 0.218255 | norm 0.3102 | lr 4.66e-05 | (65.27 ms | 1961 tok/s)\n",
      "step  744/1000 | train loss 0.217917 | norm 0.3152 | lr 4.63e-05 | (66.88 ms | 1914 tok/s)\n",
      "step  745/1000 | train loss 0.217589 | norm 0.3130 | lr 4.60e-05 | (67.54 ms | 1895 tok/s)\n",
      "step  746/1000 | train loss 0.217252 | norm 0.3028 | lr 4.56e-05 | (65.16 ms | 1964 tok/s)\n",
      "step  747/1000 | train loss 0.216926 | norm 0.3112 | lr 4.53e-05 | (65.09 ms | 1966 tok/s)\n",
      "step  748/1000 | train loss 0.216604 | norm 0.3089 | lr 4.49e-05 | (62.19 ms | 2058 tok/s)\n",
      "step  749/1000 | train loss 0.216275 | norm 0.2998 | lr 4.46e-05 | (60.81 ms | 2105 tok/s)\n",
      "step  750/1000 | train loss 0.215960 | norm 0.3090 | lr 4.43e-05 | (63.02 ms | 2031 tok/s)\n",
      "step  751/1000 | train loss 0.215643 | norm 0.3038 | lr 4.39e-05 | (61.08 ms | 2096 tok/s)\n",
      "step  752/1000 | train loss 0.215326 | norm 0.3010 | lr 4.36e-05 | (59.90 ms | 2137 tok/s)\n",
      "step  753/1000 | train loss 0.215017 | norm 0.3047 | lr 4.33e-05 | (60.48 ms | 2116 tok/s)\n",
      "step  754/1000 | train loss 0.214708 | norm 0.3002 | lr 4.29e-05 | (61.87 ms | 2069 tok/s)\n",
      "step  755/1000 | train loss 0.214401 | norm 0.3012 | lr 4.26e-05 | (60.92 ms | 2101 tok/s)\n",
      "step  756/1000 | train loss 0.214099 | norm 0.3065 | lr 4.23e-05 | (61.78 ms | 2072 tok/s)\n",
      "step  757/1000 | train loss 0.213800 | norm 0.3114 | lr 4.20e-05 | (60.07 ms | 2131 tok/s)\n",
      "step  758/1000 | train loss 0.213506 | norm 0.3254 | lr 4.16e-05 | (62.71 ms | 2041 tok/s)\n",
      "step  759/1000 | train loss 0.213220 | norm 0.3442 | lr 4.13e-05 | (61.98 ms | 2065 tok/s)\n",
      "step  760/1000 | train loss 0.212935 | norm 0.3686 | lr 4.10e-05 | (60.16 ms | 2128 tok/s)\n",
      "step  761/1000 | train loss 0.212635 | norm 0.3508 | lr 4.07e-05 | (59.97 ms | 2135 tok/s)\n",
      "step  762/1000 | train loss 0.212334 | norm 0.3002 | lr 4.03e-05 | (60.38 ms | 2120 tok/s)\n",
      "step  763/1000 | train loss 0.212056 | norm 0.3204 | lr 4.00e-05 | (61.21 ms | 2091 tok/s)\n",
      "step  764/1000 | train loss 0.211781 | norm 0.3472 | lr 3.97e-05 | (59.98 ms | 2134 tok/s)\n",
      "step  765/1000 | train loss 0.211489 | norm 0.3098 | lr 3.94e-05 | (58.83 ms | 2176 tok/s)\n",
      "step  766/1000 | train loss 0.211212 | norm 0.3068 | lr 3.91e-05 | (60.10 ms | 2130 tok/s)\n",
      "step  767/1000 | train loss 0.210944 | norm 0.3295 | lr 3.87e-05 | (60.33 ms | 2122 tok/s)\n",
      "step  768/1000 | train loss 0.210664 | norm 0.3075 | lr 3.84e-05 | (63.09 ms | 2029 tok/s)\n",
      "step  769/1000 | train loss 0.210395 | norm 0.3053 | lr 3.81e-05 | (67.06 ms | 1909 tok/s)\n",
      "step  770/1000 | train loss 0.210130 | norm 0.3119 | lr 3.78e-05 | (66.82 ms | 1916 tok/s)\n",
      "step  771/1000 | train loss 0.209862 | norm 0.3023 | lr 3.75e-05 | (65.09 ms | 1967 tok/s)\n",
      "step  772/1000 | train loss 0.209599 | norm 0.3005 | lr 3.72e-05 | (63.95 ms | 2002 tok/s)\n",
      "step  773/1000 | train loss 0.209340 | norm 0.3011 | lr 3.69e-05 | (61.59 ms | 2078 tok/s)\n",
      "step  774/1000 | train loss 0.209080 | norm 0.2968 | lr 3.66e-05 | (62.21 ms | 2058 tok/s)\n",
      "step  775/1000 | train loss 0.208825 | norm 0.2965 | lr 3.62e-05 | (60.96 ms | 2100 tok/s)\n",
      "step  776/1000 | train loss 0.208573 | norm 0.2976 | lr 3.59e-05 | (60.90 ms | 2102 tok/s)\n",
      "step  777/1000 | train loss 0.208320 | norm 0.2924 | lr 3.56e-05 | (58.99 ms | 2170 tok/s)\n",
      "step  778/1000 | train loss 0.208072 | norm 0.2957 | lr 3.53e-05 | (61.75 ms | 2073 tok/s)\n",
      "step  779/1000 | train loss 0.207826 | norm 0.2942 | lr 3.50e-05 | (62.95 ms | 2033 tok/s)\n",
      "step  780/1000 | train loss 0.207582 | norm 0.2904 | lr 3.47e-05 | (60.60 ms | 2112 tok/s)\n",
      "step  781/1000 | train loss 0.207340 | norm 0.2921 | lr 3.44e-05 | (59.26 ms | 2160 tok/s)\n",
      "step  782/1000 | train loss 0.207101 | norm 0.2929 | lr 3.41e-05 | (60.08 ms | 2130 tok/s)\n",
      "step  783/1000 | train loss 0.206864 | norm 0.2905 | lr 3.38e-05 | (63.12 ms | 2028 tok/s)\n",
      "step  784/1000 | train loss 0.206628 | norm 0.2884 | lr 3.35e-05 | (61.76 ms | 2072 tok/s)\n",
      "step  785/1000 | train loss 0.206397 | norm 0.2925 | lr 3.32e-05 | (59.99 ms | 2134 tok/s)\n",
      "step  786/1000 | train loss 0.206167 | norm 0.2885 | lr 3.29e-05 | (62.45 ms | 2050 tok/s)\n",
      "step  787/1000 | train loss 0.205937 | norm 0.2901 | lr 3.26e-05 | (61.92 ms | 2067 tok/s)\n",
      "step  788/1000 | train loss 0.205713 | norm 0.2899 | lr 3.23e-05 | (59.98 ms | 2134 tok/s)\n",
      "step  789/1000 | train loss 0.205491 | norm 0.2980 | lr 3.21e-05 | (61.53 ms | 2080 tok/s)\n",
      "step  790/1000 | train loss 0.205272 | norm 0.3024 | lr 3.18e-05 | (61.81 ms | 2071 tok/s)\n",
      "step  791/1000 | train loss 0.205058 | norm 0.3275 | lr 3.15e-05 | (62.86 ms | 2036 tok/s)\n",
      "step  792/1000 | train loss 0.204845 | norm 0.3291 | lr 3.12e-05 | (59.98 ms | 2134 tok/s)\n",
      "step  793/1000 | train loss 0.204623 | norm 0.3165 | lr 3.09e-05 | (59.68 ms | 2145 tok/s)\n",
      "step  794/1000 | train loss 0.204403 | norm 0.2851 | lr 3.06e-05 | (62.64 ms | 2043 tok/s)\n",
      "step  795/1000 | train loss 0.204198 | norm 0.3048 | lr 3.03e-05 | (61.88 ms | 2069 tok/s)\n",
      "step  796/1000 | train loss 0.203991 | norm 0.3173 | lr 3.00e-05 | (59.35 ms | 2157 tok/s)\n",
      "step  797/1000 | train loss 0.203779 | norm 0.2867 | lr 2.98e-05 | (59.38 ms | 2155 tok/s)\n",
      "step  798/1000 | train loss 0.203577 | norm 0.2965 | lr 2.95e-05 | (62.11 ms | 2061 tok/s)\n",
      "step  799/1000 | train loss 0.203376 | norm 0.3059 | lr 2.92e-05 | (62.43 ms | 2050 tok/s)\n",
      "step  800/1000 | train loss 0.203172 | norm 0.2872 | lr 2.89e-05 | (60.40 ms | 2119 tok/s)\n",
      "step  801/1000 | train loss 0.202976 | norm 0.2936 | lr 2.86e-05 | (59.81 ms | 2140 tok/s)\n",
      "step  802/1000 | train loss 0.202780 | norm 0.2968 | lr 2.84e-05 | (64.21 ms | 1994 tok/s)\n",
      "step  803/1000 | train loss 0.202584 | norm 0.2889 | lr 2.81e-05 | (62.12 ms | 2060 tok/s)\n",
      "step  804/1000 | train loss 0.202394 | norm 0.2916 | lr 2.78e-05 | (61.73 ms | 2074 tok/s)\n",
      "step  805/1000 | train loss 0.202202 | norm 0.2883 | lr 2.75e-05 | (61.51 ms | 2081 tok/s)\n",
      "step  806/1000 | train loss 0.202015 | norm 0.2887 | lr 2.73e-05 | (63.41 ms | 2018 tok/s)\n",
      "step  807/1000 | train loss 0.201829 | norm 0.2882 | lr 2.70e-05 | (64.48 ms | 1985 tok/s)\n",
      "step  808/1000 | train loss 0.201643 | norm 0.2846 | lr 2.67e-05 | (65.72 ms | 1948 tok/s)\n",
      "step  809/1000 | train loss 0.201463 | norm 0.2867 | lr 2.65e-05 | (68.28 ms | 1875 tok/s)\n",
      "step  810/1000 | train loss 0.201281 | norm 0.2836 | lr 2.62e-05 | (67.15 ms | 1906 tok/s)\n",
      "step  811/1000 | train loss 0.201103 | norm 0.2818 | lr 2.59e-05 | (64.64 ms | 1980 tok/s)\n",
      "step  812/1000 | train loss 0.200927 | norm 0.2855 | lr 2.57e-05 | (61.38 ms | 2085 tok/s)\n",
      "step  813/1000 | train loss 0.200752 | norm 0.2794 | lr 2.54e-05 | (60.01 ms | 2133 tok/s)\n",
      "step  814/1000 | train loss 0.200579 | norm 0.2824 | lr 2.51e-05 | (62.61 ms | 2044 tok/s)\n",
      "step  815/1000 | train loss 0.200409 | norm 0.2845 | lr 2.49e-05 | (63.05 ms | 2030 tok/s)\n",
      "step  816/1000 | train loss 0.200241 | norm 0.2880 | lr 2.46e-05 | (59.51 ms | 2151 tok/s)\n",
      "step  817/1000 | train loss 0.200075 | norm 0.2867 | lr 2.44e-05 | (59.98 ms | 2134 tok/s)\n",
      "step  818/1000 | train loss 0.199913 | norm 0.2957 | lr 2.41e-05 | (64.31 ms | 1990 tok/s)\n",
      "step  819/1000 | train loss 0.199749 | norm 0.2870 | lr 2.39e-05 | (66.60 ms | 1922 tok/s)\n",
      "step  820/1000 | train loss 0.199584 | norm 0.2854 | lr 2.36e-05 | (67.56 ms | 1895 tok/s)\n",
      "step  821/1000 | train loss 0.199423 | norm 0.2805 | lr 2.34e-05 | (68.41 ms | 1871 tok/s)\n",
      "step  822/1000 | train loss 0.199267 | norm 0.2826 | lr 2.31e-05 | (65.75 ms | 1947 tok/s)\n",
      "step  823/1000 | train loss 0.199112 | norm 0.2908 | lr 2.28e-05 | (63.38 ms | 2020 tok/s)\n",
      "step  824/1000 | train loss 0.198956 | norm 0.2835 | lr 2.26e-05 | (63.06 ms | 2030 tok/s)\n",
      "step  825/1000 | train loss 0.198803 | norm 0.2777 | lr 2.24e-05 | (63.25 ms | 2024 tok/s)\n",
      "step  826/1000 | train loss 0.198653 | norm 0.2841 | lr 2.21e-05 | (61.03 ms | 2097 tok/s)\n",
      "step  827/1000 | train loss 0.198505 | norm 0.2826 | lr 2.19e-05 | (61.22 ms | 2091 tok/s)\n",
      "step  828/1000 | train loss 0.198356 | norm 0.2800 | lr 2.16e-05 | (60.84 ms | 2104 tok/s)\n",
      "step  829/1000 | train loss 0.198211 | norm 0.2791 | lr 2.14e-05 | (60.66 ms | 2110 tok/s)\n",
      "step  830/1000 | train loss 0.198067 | norm 0.2819 | lr 2.11e-05 | (61.68 ms | 2075 tok/s)\n",
      "step  831/1000 | train loss 0.197925 | norm 0.2820 | lr 2.09e-05 | (60.31 ms | 2122 tok/s)\n",
      "step  832/1000 | train loss 0.197785 | norm 0.2813 | lr 2.06e-05 | (60.08 ms | 2131 tok/s)\n",
      "step  833/1000 | train loss 0.197646 | norm 0.2770 | lr 2.04e-05 | (60.79 ms | 2106 tok/s)\n",
      "step  834/1000 | train loss 0.197510 | norm 0.2848 | lr 2.02e-05 | (61.69 ms | 2075 tok/s)\n",
      "step  835/1000 | train loss 0.197374 | norm 0.2779 | lr 1.99e-05 | (62.25 ms | 2056 tok/s)\n",
      "step  836/1000 | train loss 0.197239 | norm 0.2810 | lr 1.97e-05 | (67.38 ms | 1900 tok/s)\n",
      "step  837/1000 | train loss 0.197107 | norm 0.2788 | lr 1.95e-05 | (67.58 ms | 1894 tok/s)\n",
      "step  838/1000 | train loss 0.196976 | norm 0.2756 | lr 1.92e-05 | (63.71 ms | 2009 tok/s)\n",
      "step  839/1000 | train loss 0.196847 | norm 0.2784 | lr 1.90e-05 | (61.82 ms | 2071 tok/s)\n",
      "step  840/1000 | train loss 0.196720 | norm 0.2765 | lr 1.88e-05 | (61.57 ms | 2079 tok/s)\n",
      "step  841/1000 | train loss 0.196595 | norm 0.2785 | lr 1.86e-05 | (62.38 ms | 2052 tok/s)\n",
      "step  842/1000 | train loss 0.196470 | norm 0.2772 | lr 1.83e-05 | (62.92 ms | 2034 tok/s)\n",
      "step  843/1000 | train loss 0.196347 | norm 0.2767 | lr 1.81e-05 | (60.64 ms | 2111 tok/s)\n",
      "step  844/1000 | train loss 0.196226 | norm 0.2753 | lr 1.79e-05 | (60.94 ms | 2101 tok/s)\n",
      "step  845/1000 | train loss 0.196107 | norm 0.2756 | lr 1.77e-05 | (61.86 ms | 2069 tok/s)\n",
      "step  846/1000 | train loss 0.195989 | norm 0.2761 | lr 1.74e-05 | (61.89 ms | 2068 tok/s)\n",
      "step  847/1000 | train loss 0.195872 | norm 0.2748 | lr 1.72e-05 | (60.80 ms | 2105 tok/s)\n",
      "step  848/1000 | train loss 0.195757 | norm 0.2760 | lr 1.70e-05 | (60.03 ms | 2132 tok/s)\n",
      "step  849/1000 | train loss 0.195644 | norm 0.2737 | lr 1.68e-05 | (61.44 ms | 2083 tok/s)\n",
      "step  850/1000 | train loss 0.195532 | norm 0.2750 | lr 1.66e-05 | (63.51 ms | 2015 tok/s)\n",
      "step  851/1000 | train loss 0.195421 | norm 0.2730 | lr 1.63e-05 | (62.61 ms | 2044 tok/s)\n",
      "step  852/1000 | train loss 0.195312 | norm 0.2742 | lr 1.61e-05 | (61.50 ms | 2081 tok/s)\n",
      "step  853/1000 | train loss 0.195204 | norm 0.2737 | lr 1.59e-05 | (62.43 ms | 2050 tok/s)\n",
      "step  854/1000 | train loss 0.195098 | norm 0.2729 | lr 1.57e-05 | (61.96 ms | 2066 tok/s)\n",
      "step  855/1000 | train loss 0.194994 | norm 0.2737 | lr 1.55e-05 | (60.88 ms | 2102 tok/s)\n",
      "step  856/1000 | train loss 0.194890 | norm 0.2723 | lr 1.53e-05 | (61.38 ms | 2085 tok/s)\n",
      "step  857/1000 | train loss 0.194789 | norm 0.2747 | lr 1.51e-05 | (64.20 ms | 1994 tok/s)\n",
      "step  858/1000 | train loss 0.194688 | norm 0.2729 | lr 1.49e-05 | (64.03 ms | 1999 tok/s)\n",
      "step  859/1000 | train loss 0.194589 | norm 0.2746 | lr 1.47e-05 | (62.54 ms | 2047 tok/s)\n",
      "step  860/1000 | train loss 0.194492 | norm 0.2725 | lr 1.45e-05 | (61.65 ms | 2076 tok/s)\n",
      "step  861/1000 | train loss 0.194396 | norm 0.2759 | lr 1.43e-05 | (61.70 ms | 2075 tok/s)\n",
      "step  862/1000 | train loss 0.194301 | norm 0.2715 | lr 1.41e-05 | (63.48 ms | 2016 tok/s)\n",
      "step  863/1000 | train loss 0.194207 | norm 0.2735 | lr 1.39e-05 | (62.83 ms | 2037 tok/s)\n",
      "step  864/1000 | train loss 0.194114 | norm 0.2714 | lr 1.37e-05 | (62.58 ms | 2045 tok/s)\n",
      "step  865/1000 | train loss 0.194024 | norm 0.2705 | lr 1.35e-05 | (62.05 ms | 2063 tok/s)\n",
      "step  866/1000 | train loss 0.193934 | norm 0.2738 | lr 1.33e-05 | (63.22 ms | 2025 tok/s)\n",
      "step  867/1000 | train loss 0.193846 | norm 0.2709 | lr 1.31e-05 | (61.09 ms | 2095 tok/s)\n",
      "step  868/1000 | train loss 0.193759 | norm 0.2726 | lr 1.29e-05 | (61.37 ms | 2086 tok/s)\n",
      "step  869/1000 | train loss 0.193673 | norm 0.2713 | lr 1.27e-05 | (61.77 ms | 2072 tok/s)\n",
      "step  870/1000 | train loss 0.193589 | norm 0.2713 | lr 1.25e-05 | (62.41 ms | 2051 tok/s)\n",
      "step  871/1000 | train loss 0.193506 | norm 0.2717 | lr 1.23e-05 | (59.85 ms | 2139 tok/s)\n",
      "step  872/1000 | train loss 0.193424 | norm 0.2703 | lr 1.22e-05 | (59.44 ms | 2153 tok/s)\n",
      "step  873/1000 | train loss 0.193343 | norm 0.2714 | lr 1.20e-05 | (62.37 ms | 2052 tok/s)\n",
      "step  874/1000 | train loss 0.193264 | norm 0.2704 | lr 1.18e-05 | (61.37 ms | 2086 tok/s)\n",
      "step  875/1000 | train loss 0.193186 | norm 0.2713 | lr 1.16e-05 | (62.35 ms | 2053 tok/s)\n",
      "step  876/1000 | train loss 0.193109 | norm 0.2713 | lr 1.14e-05 | (67.61 ms | 1893 tok/s)\n",
      "step  877/1000 | train loss 0.193034 | norm 0.2707 | lr 1.12e-05 | (68.19 ms | 1877 tok/s)\n",
      "step  878/1000 | train loss 0.192959 | norm 0.2699 | lr 1.11e-05 | (68.98 ms | 1856 tok/s)\n",
      "step  879/1000 | train loss 0.192886 | norm 0.2701 | lr 1.09e-05 | (67.53 ms | 1896 tok/s)\n",
      "step  880/1000 | train loss 0.192814 | norm 0.2704 | lr 1.07e-05 | (62.53 ms | 2047 tok/s)\n",
      "step  881/1000 | train loss 0.192743 | norm 0.2703 | lr 1.05e-05 | (64.41 ms | 1987 tok/s)\n",
      "step  882/1000 | train loss 0.192674 | norm 0.2715 | lr 1.04e-05 | (62.35 ms | 2053 tok/s)\n",
      "step  883/1000 | train loss 0.192605 | norm 0.2718 | lr 1.02e-05 | (63.43 ms | 2018 tok/s)\n",
      "step  884/1000 | train loss 0.192538 | norm 0.2742 | lr 1.00e-05 | (61.04 ms | 2097 tok/s)\n",
      "step  885/1000 | train loss 0.192472 | norm 0.2722 | lr 9.85e-06 | (63.27 ms | 2023 tok/s)\n",
      "step  886/1000 | train loss 0.192406 | norm 0.2705 | lr 9.68e-06 | (64.69 ms | 1979 tok/s)\n",
      "step  887/1000 | train loss 0.192342 | norm 0.2705 | lr 9.52e-06 | (61.60 ms | 2078 tok/s)\n",
      "step  888/1000 | train loss 0.192280 | norm 0.2702 | lr 9.35e-06 | (60.96 ms | 2100 tok/s)\n",
      "step  889/1000 | train loss 0.192218 | norm 0.2714 | lr 9.19e-06 | (62.67 ms | 2042 tok/s)\n",
      "step  890/1000 | train loss 0.192157 | norm 0.2692 | lr 9.03e-06 | (60.70 ms | 2109 tok/s)\n",
      "step  891/1000 | train loss 0.192097 | norm 0.2693 | lr 8.87e-06 | (60.03 ms | 2132 tok/s)\n",
      "step  892/1000 | train loss 0.192039 | norm 0.2710 | lr 8.71e-06 | (59.22 ms | 2162 tok/s)\n",
      "step  893/1000 | train loss 0.191981 | norm 0.2687 | lr 8.55e-06 | (60.23 ms | 2125 tok/s)\n",
      "step  894/1000 | train loss 0.191925 | norm 0.2691 | lr 8.40e-06 | (64.62 ms | 1981 tok/s)\n",
      "step  895/1000 | train loss 0.191869 | norm 0.2701 | lr 8.24e-06 | (60.77 ms | 2106 tok/s)\n",
      "step  896/1000 | train loss 0.191815 | norm 0.2686 | lr 8.09e-06 | (59.63 ms | 2147 tok/s)\n",
      "step  897/1000 | train loss 0.191762 | norm 0.2688 | lr 7.94e-06 | (62.89 ms | 2035 tok/s)\n",
      "step  898/1000 | train loss 0.191709 | norm 0.2693 | lr 7.78e-06 | (61.36 ms | 2086 tok/s)\n",
      "step  899/1000 | train loss 0.191658 | norm 0.2687 | lr 7.64e-06 | (63.05 ms | 2030 tok/s)\n",
      "step  900/1000 | train loss 0.191607 | norm 0.2684 | lr 7.49e-06 | (61.25 ms | 2090 tok/s)\n",
      "step  901/1000 | train loss 0.191558 | norm 0.2688 | lr 7.34e-06 | (63.76 ms | 2008 tok/s)\n",
      "step  902/1000 | train loss 0.191510 | norm 0.2686 | lr 7.20e-06 | (62.56 ms | 2046 tok/s)\n",
      "step  903/1000 | train loss 0.191462 | norm 0.2681 | lr 7.05e-06 | (61.64 ms | 2077 tok/s)\n",
      "step  904/1000 | train loss 0.191416 | norm 0.2685 | lr 6.91e-06 | (60.95 ms | 2100 tok/s)\n",
      "step  905/1000 | train loss 0.191370 | norm 0.2682 | lr 6.77e-06 | (62.36 ms | 2053 tok/s)\n",
      "step  906/1000 | train loss 0.191325 | norm 0.2680 | lr 6.63e-06 | (62.36 ms | 2052 tok/s)\n",
      "step  907/1000 | train loss 0.191282 | norm 0.2682 | lr 6.49e-06 | (62.91 ms | 2035 tok/s)\n",
      "step  908/1000 | train loss 0.191239 | norm 0.2679 | lr 6.36e-06 | (60.72 ms | 2108 tok/s)\n",
      "step  909/1000 | train loss 0.191197 | norm 0.2679 | lr 6.22e-06 | (61.57 ms | 2079 tok/s)\n",
      "step  910/1000 | train loss 0.191156 | norm 0.2680 | lr 6.09e-06 | (62.59 ms | 2045 tok/s)\n",
      "step  911/1000 | train loss 0.191116 | norm 0.2677 | lr 5.96e-06 | (61.58 ms | 2079 tok/s)\n",
      "step  912/1000 | train loss 0.191077 | norm 0.2676 | lr 5.83e-06 | (63.00 ms | 2032 tok/s)\n",
      "step  913/1000 | train loss 0.191038 | norm 0.2678 | lr 5.70e-06 | (64.38 ms | 1988 tok/s)\n",
      "step  914/1000 | train loss 0.191001 | norm 0.2676 | lr 5.57e-06 | (61.98 ms | 2065 tok/s)\n",
      "step  915/1000 | train loss 0.190964 | norm 0.2674 | lr 5.44e-06 | (59.72 ms | 2143 tok/s)\n",
      "step  916/1000 | train loss 0.190928 | norm 0.2677 | lr 5.32e-06 | (59.60 ms | 2148 tok/s)\n",
      "step  917/1000 | train loss 0.190893 | norm 0.2675 | lr 5.19e-06 | (59.93 ms | 2136 tok/s)\n",
      "step  918/1000 | train loss 0.190859 | norm 0.2673 | lr 5.07e-06 | (62.44 ms | 2050 tok/s)\n",
      "step  919/1000 | train loss 0.190825 | norm 0.2674 | lr 4.95e-06 | (61.90 ms | 2068 tok/s)\n",
      "step  920/1000 | train loss 0.190793 | norm 0.2674 | lr 4.83e-06 | (60.61 ms | 2112 tok/s)\n",
      "step  921/1000 | train loss 0.190761 | norm 0.2672 | lr 4.71e-06 | (64.71 ms | 1978 tok/s)\n",
      "step  922/1000 | train loss 0.190730 | norm 0.2672 | lr 4.60e-06 | (66.23 ms | 1933 tok/s)\n",
      "step  923/1000 | train loss 0.190700 | norm 0.2673 | lr 4.48e-06 | (69.21 ms | 1850 tok/s)\n",
      "step  924/1000 | train loss 0.190670 | norm 0.2671 | lr 4.37e-06 | (69.75 ms | 1835 tok/s)\n",
      "step  925/1000 | train loss 0.190641 | norm 0.2671 | lr 4.26e-06 | (66.54 ms | 1924 tok/s)\n",
      "step  926/1000 | train loss 0.190613 | norm 0.2671 | lr 4.14e-06 | (62.99 ms | 2032 tok/s)\n",
      "step  927/1000 | train loss 0.190586 | norm 0.2670 | lr 4.04e-06 | (61.23 ms | 2090 tok/s)\n",
      "step  928/1000 | train loss 0.190559 | norm 0.2671 | lr 3.93e-06 | (64.17 ms | 1995 tok/s)\n",
      "step  929/1000 | train loss 0.190534 | norm 0.2670 | lr 3.82e-06 | (62.44 ms | 2050 tok/s)\n",
      "step  930/1000 | train loss 0.190508 | norm 0.2669 | lr 3.72e-06 | (60.20 ms | 2126 tok/s)\n",
      "step  931/1000 | train loss 0.190484 | norm 0.2670 | lr 3.61e-06 | (60.42 ms | 2118 tok/s)\n",
      "step  932/1000 | train loss 0.190460 | norm 0.2669 | lr 3.51e-06 | (62.08 ms | 2062 tok/s)\n",
      "step  933/1000 | train loss 0.190437 | norm 0.2668 | lr 3.41e-06 | (61.56 ms | 2079 tok/s)\n",
      "step  934/1000 | train loss 0.190414 | norm 0.2669 | lr 3.31e-06 | (60.69 ms | 2109 tok/s)\n",
      "step  935/1000 | train loss 0.190393 | norm 0.2669 | lr 3.21e-06 | (60.98 ms | 2099 tok/s)\n",
      "step  936/1000 | train loss 0.190371 | norm 0.2668 | lr 3.12e-06 | (61.97 ms | 2066 tok/s)\n",
      "step  937/1000 | train loss 0.190351 | norm 0.2668 | lr 3.02e-06 | (62.41 ms | 2051 tok/s)\n",
      "step  938/1000 | train loss 0.190331 | norm 0.2668 | lr 2.93e-06 | (59.16 ms | 2163 tok/s)\n",
      "step  939/1000 | train loss 0.190312 | norm 0.2667 | lr 2.84e-06 | (59.08 ms | 2167 tok/s)\n",
      "step  940/1000 | train loss 0.190293 | norm 0.2667 | lr 2.75e-06 | (61.01 ms | 2098 tok/s)\n",
      "step  941/1000 | train loss 0.190275 | norm 0.2667 | lr 2.66e-06 | (60.34 ms | 2121 tok/s)\n",
      "step  942/1000 | train loss 0.190257 | norm 0.2667 | lr 2.57e-06 | (62.01 ms | 2064 tok/s)\n",
      "step  943/1000 | train loss 0.190240 | norm 0.2666 | lr 2.48e-06 | (61.17 ms | 2092 tok/s)\n",
      "step  944/1000 | train loss 0.190224 | norm 0.2666 | lr 2.40e-06 | (60.84 ms | 2104 tok/s)\n",
      "step  945/1000 | train loss 0.190208 | norm 0.2666 | lr 2.32e-06 | (62.66 ms | 2043 tok/s)\n",
      "step  946/1000 | train loss 0.190193 | norm 0.2666 | lr 2.23e-06 | (61.65 ms | 2076 tok/s)\n",
      "step  947/1000 | train loss 0.190178 | norm 0.2666 | lr 2.15e-06 | (63.62 ms | 2012 tok/s)\n",
      "step  948/1000 | train loss 0.190164 | norm 0.2666 | lr 2.07e-06 | (62.80 ms | 2038 tok/s)\n",
      "step  949/1000 | train loss 0.190150 | norm 0.2665 | lr 2.00e-06 | (61.91 ms | 2068 tok/s)\n",
      "step  950/1000 | train loss 0.190137 | norm 0.2665 | lr 1.92e-06 | (61.36 ms | 2086 tok/s)\n",
      "step  951/1000 | train loss 0.190124 | norm 0.2665 | lr 1.85e-06 | (59.75 ms | 2142 tok/s)\n",
      "step  952/1000 | train loss 0.190112 | norm 0.2665 | lr 1.77e-06 | (62.15 ms | 2059 tok/s)\n",
      "step  953/1000 | train loss 0.190100 | norm 0.2665 | lr 1.70e-06 | (62.44 ms | 2050 tok/s)\n",
      "step  954/1000 | train loss 0.190089 | norm 0.2665 | lr 1.63e-06 | (61.84 ms | 2070 tok/s)\n",
      "step  955/1000 | train loss 0.190078 | norm 0.2664 | lr 1.56e-06 | (60.79 ms | 2105 tok/s)\n",
      "step  956/1000 | train loss 0.190068 | norm 0.2664 | lr 1.50e-06 | (63.07 ms | 2030 tok/s)\n",
      "step  957/1000 | train loss 0.190058 | norm 0.2664 | lr 1.43e-06 | (61.99 ms | 2065 tok/s)\n",
      "step  958/1000 | train loss 0.190049 | norm 0.2664 | lr 1.37e-06 | (84.57 ms | 1514 tok/s)\n",
      "step  959/1000 | train loss 0.190040 | norm 0.2664 | lr 1.30e-06 | (60.98 ms | 2099 tok/s)\n",
      "step  960/1000 | train loss 0.190031 | norm 0.2664 | lr 1.24e-06 | (65.45 ms | 1956 tok/s)\n",
      "step  961/1000 | train loss 0.190023 | norm 0.2664 | lr 1.18e-06 | (66.06 ms | 1938 tok/s)\n",
      "step  962/1000 | train loss 0.190015 | norm 0.2664 | lr 1.12e-06 | (66.54 ms | 1924 tok/s)\n",
      "step  963/1000 | train loss 0.190008 | norm 0.2664 | lr 1.07e-06 | (66.96 ms | 1912 tok/s)\n",
      "step  964/1000 | train loss 0.190001 | norm 0.2663 | lr 1.01e-06 | (63.82 ms | 2006 tok/s)\n",
      "step  965/1000 | train loss 0.189994 | norm 0.2663 | lr 9.58e-07 | (64.51 ms | 1984 tok/s)\n",
      "step  966/1000 | train loss 0.189988 | norm 0.2663 | lr 9.06e-07 | (62.69 ms | 2042 tok/s)\n",
      "step  967/1000 | train loss 0.189982 | norm 0.2663 | lr 8.55e-07 | (62.78 ms | 2039 tok/s)\n",
      "step  968/1000 | train loss 0.189976 | norm 0.2663 | lr 8.05e-07 | (62.18 ms | 2059 tok/s)\n",
      "step  969/1000 | train loss 0.189971 | norm 0.2663 | lr 7.57e-07 | (60.35 ms | 2121 tok/s)\n",
      "step  970/1000 | train loss 0.189966 | norm 0.2663 | lr 7.11e-07 | (59.73 ms | 2143 tok/s)\n",
      "step  971/1000 | train loss 0.189961 | norm 0.2663 | lr 6.66e-07 | (61.45 ms | 2083 tok/s)\n",
      "step  972/1000 | train loss 0.189957 | norm 0.2663 | lr 6.22e-07 | (61.29 ms | 2088 tok/s)\n",
      "step  973/1000 | train loss 0.189952 | norm 0.2663 | lr 5.80e-07 | (58.97 ms | 2171 tok/s)\n",
      "step  974/1000 | train loss 0.189949 | norm 0.2663 | lr 5.39e-07 | (59.94 ms | 2135 tok/s)\n",
      "step  975/1000 | train loss 0.189945 | norm 0.2663 | lr 5.00e-07 | (62.22 ms | 2057 tok/s)\n",
      "step  976/1000 | train loss 0.189942 | norm 0.2663 | lr 4.62e-07 | (62.43 ms | 2050 tok/s)\n",
      "step  977/1000 | train loss 0.189939 | norm 0.2663 | lr 4.26e-07 | (61.49 ms | 2082 tok/s)\n",
      "step  978/1000 | train loss 0.189936 | norm 0.2663 | lr 3.91e-07 | (60.13 ms | 2129 tok/s)\n",
      "step  979/1000 | train loss 0.189933 | norm 0.2663 | lr 3.58e-07 | (60.41 ms | 2119 tok/s)\n",
      "step  980/1000 | train loss 0.189931 | norm 0.2663 | lr 3.26e-07 | (60.57 ms | 2113 tok/s)\n",
      "step  981/1000 | train loss 0.189929 | norm 0.2663 | lr 2.96e-07 | (60.26 ms | 2124 tok/s)\n",
      "step  982/1000 | train loss 0.189927 | norm 0.2663 | lr 2.67e-07 | (59.64 ms | 2146 tok/s)\n",
      "step  983/1000 | train loss 0.189925 | norm 0.2663 | lr 2.40e-07 | (59.91 ms | 2137 tok/s)\n",
      "step  984/1000 | train loss 0.189923 | norm 0.2663 | lr 2.14e-07 | (63.54 ms | 2015 tok/s)\n",
      "step  985/1000 | train loss 0.189922 | norm 0.2663 | lr 1.89e-07 | (60.19 ms | 2127 tok/s)\n",
      "step  986/1000 | train loss 0.189921 | norm 0.2663 | lr 1.67e-07 | (61.10 ms | 2095 tok/s)\n",
      "step  987/1000 | train loss 0.189920 | norm 0.2663 | lr 1.45e-07 | (61.01 ms | 2098 tok/s)\n",
      "step  988/1000 | train loss 0.189919 | norm 0.2663 | lr 1.25e-07 | (61.07 ms | 2096 tok/s)\n",
      "step  989/1000 | train loss 0.189918 | norm 0.2663 | lr 1.07e-07 | (59.88 ms | 2137 tok/s)\n",
      "step  990/1000 | train loss 0.189917 | norm 0.2663 | lr 8.96e-08 | (60.96 ms | 2100 tok/s)\n",
      "step  991/1000 | train loss 0.189917 | norm 0.2663 | lr 7.40e-08 | (61.87 ms | 2069 tok/s)\n",
      "step  992/1000 | train loss 0.189916 | norm 0.2663 | lr 6.00e-08 | (61.37 ms | 2086 tok/s)\n",
      "step  993/1000 | train loss 0.189916 | norm 0.2663 | lr 4.74e-08 | (62.91 ms | 2035 tok/s)\n",
      "step  994/1000 | train loss 0.189915 | norm 0.2663 | lr 3.63e-08 | (63.02 ms | 2031 tok/s)\n",
      "step  995/1000 | train loss 0.189915 | norm 0.2663 | lr 2.66e-08 | (99.83 ms | 1282 tok/s)\n",
      "step  996/1000 | train loss 0.189915 | norm 0.2663 | lr 1.85e-08 | (64.33 ms | 1990 tok/s)\n",
      "step  997/1000 | train loss 0.189915 | norm 0.2663 | lr 1.18e-08 | (60.49 ms | 2116 tok/s)\n",
      "step  998/1000 | train loss 0.189915 | norm 0.2663 | lr 6.66e-09 | (60.18 ms | 2127 tok/s)\n",
      "step  999/1000 | train loss 0.189915 | norm 0.2663 | lr 2.96e-09 | (63.57 ms | 2013 tok/s)\n",
      "step 1000/1000 | train loss 0.189915 | norm 0.2663 | lr 7.40e-10 | (64.58 ms | 1982 tok/s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "timings = []\n",
    "norm = -1.0   # dummy value to print in inference-only mode\n",
    "for step in range(num_iterations + 1):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == num_iterations)\n",
    "\n",
    "    # once in a while evaluate the validation dataset\n",
    "    if (val_loss_every > 0 \\\n",
    "        and (step % val_loss_every == 0 or last_step)) \\\n",
    "        and (val_loader is not None):\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for _ in range(val_max_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, loss = model(x, y, return_logits=False)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= val_max_steps\n",
    "        # log to console and to file\n",
    "        print0(f\"val loss {val_loss}\")\n",
    "\n",
    "\n",
    "    # once in a while perform model inference on the master process\n",
    "    if (sample_every > 0 \\\n",
    "        and (step % sample_every == 0 or last_step)) \\\n",
    "        and master_process:\n",
    "        model.eval()\n",
    "        # before we end, let's also do one round of inference\n",
    "        # we'll kick off the generation with \"<|endoftext|>\", which designates the start of a new sequence\n",
    "        start_ids = [63]\n",
    "        xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "        max_new_tokens = 32\n",
    "        temperature = 1.0\n",
    "        top_k = 40\n",
    "        yg = raw_model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        print0('---------------')\n",
    "        print0(decode(yg[0].tolist()))\n",
    "        print0('---------------')\n",
    "\n",
    "    # bit confusing: we want to make sure to eval and sample on 0th iteration\n",
    "    # but also after the very last iteration. so we loop for step <= num_iterations\n",
    "    # instead of just < num_iterations (one extra due to <=), only to do\n",
    "    # the validation/sampling one last time, and then we break right here as we're done.\n",
    "    if last_step:\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION BEGIN -----------------\n",
    "    model.train()\n",
    "    # micro-batch loop where we do gradient accumulation to reach desired total batch size\n",
    "    lossf = 0.0 # for getting the mean loss (as simple float) over the accumulation steps\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # fetch a batch\n",
    "        if not overfit_single_batch \\\n",
    "            or (overfit_single_batch and step == 0 and micro_step == 0):\n",
    "            x, y = train_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        with ctx:\n",
    "            _, loss = model(x, y, return_logits=False)\n",
    "            # we have to scale the loss to account for gradient accumulation,\n",
    "            # because the gradients just add on each successive backward().\n",
    "            # addition of gradients corresponds to a SUM in the objective, but\n",
    "            # instead of a SUM we want MEAN, so we scale the loss here\n",
    "            loss = loss / grad_accum_steps\n",
    "            lossf += loss.detach() # keep track of the mean loss\n",
    "        # backward pass\n",
    "        if not inference_only:\n",
    "            loss.backward()\n",
    "\n",
    "    lossf = lossf.item()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    # step the optimizer\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # --------------- TRAINING SECTION END -------------------\n",
    "    # everything that follows now is just diagnostics, prints, logging, etc.\n",
    "\n",
    "    # wait on the CPU for all device work to end so we get accurate per-iteration timings below\n",
    "    if device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "    elif device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    # time and print\n",
    "    t1 = time.time()\n",
    "    # the 0th iteration is often an outlier (much slower) => skip logging it\n",
    "    tokens_per_second = grad_accum_steps * ddp_world_size * B * T / (t1-t0)\n",
    "    print0(f\"step {step+1:4d}/{num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)\")\n",
    "\n",
    "\n",
    "    # keep track of smooth timings, last 20 iterations\n",
    "    if step > 0 and step > num_iterations - 20:\n",
    "        timings.append(t1-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final 19 iters avg: 63.656ms\n",
      "peak memory consumption: 0 MiB\n"
     ]
    }
   ],
   "source": [
    "# print the average of the last 20 timings, to get something smooth-ish\n",
    "timings = timings[-20:]\n",
    "print0(f\"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
    "print0(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Example input: Hello,\n",
      "Generated output: hello,urther, hear me\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# write an example for generating text\n",
    "sample_text = \"Hello,\"\n",
    "sample_tokens = encode(sample_text)\n",
    "sample_tokens = torch.tensor(sample_tokens, dtype=torch.long, device=device)[None, ...]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_out = model.generate(sample_tokens, max_new_tokens=15, temperature=1, top_k=20)\n",
    "\n",
    "# print the generated text\n",
    "print0('---------------')\n",
    "print0(f\"Example input: {sample_text}\")\n",
    "print0(f\"Generated output: {decode(sample_out[0].tolist())}\")\n",
    "print0('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
