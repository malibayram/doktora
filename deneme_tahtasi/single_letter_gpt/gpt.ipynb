{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT modelinin parametre sayısını hesaplamak için aşağıdaki formüller kullanılır:\n",
    "\n",
    "1. **Embedding layer**: \\( vocab\\_size \\times n\\_embd \\)\n",
    "2. **Transformer block**: Her bir Transformer bloğunda toplam parametre sayısı şu şekildedir:\n",
    "   - **LayerNorm**: İki LayerNorm katmanı vardır, her biri \\( 2 \\times n\\_embd \\)\n",
    "   - **Attention (Q, K, V)**: \\( 3 \\times (n\\_embd \\times (n\\_embd // n\\_head) + n\\_embd) \\)\n",
    "   - **Attention output projection**: \\( n\\_embd \\times n\\_embd + n\\_embd \\)\n",
    "   - **MLP (intermediate dense layers)**: \\( 2 \\times (n\\_embd \\times 4 \\times n\\_embd + 4 \\times n\\_embd) \\)\n",
    "\n",
    "Bu formülleri kullanarak parametre sayısını hesaplayalım.\n",
    "\n",
    "### Hesaplamalar\n",
    "\n",
    "#### Embedding Layer\n",
    "\n",
    "\\[\n",
    "50257 \\times 768 = 38,609,856\n",
    "\\]\n",
    "\n",
    "#### Transformer Block\n",
    "\n",
    "Her bir Transformer bloğunda:\n",
    "\n",
    "- **LayerNorm**: \n",
    "  \\[\n",
    "  2 \\times 768 = 1,536\n",
    "  \\]\n",
    "\n",
    "- **Attention (Q, K, V)**:\n",
    "  \\[\n",
    "  3 \\times (768 \\times (768 // 12) + 768) = 3 \\times (768 \\times 64 + 768) = 3 \\times (49,152 + 768) = 3 \\times 49,920 = 149,760\n",
    "  \\]\n",
    "\n",
    "- **Attention output projection**:\n",
    "  \\[\n",
    "  768 \\times 768 + 768 = 590,592 + 768 = 591,360\n",
    "  \\]\n",
    "\n",
    "- **MLP**:\n",
    "  \\[\n",
    "  2 \\times (768 \\times 4 \\times 768 + 4 \\times 768) = 2 \\times (3,145,728 + 3,072) = 2 \\times 3,148,800 = 6,297,600\n",
    "  \\]\n",
    "\n",
    "Her bir Transformer bloğu için toplam:\n",
    "\\[\n",
    "1,536 + 149,760 + 591,360 + 6,297,600 = 7,040,256\n",
    "\\]\n",
    "\n",
    "#### Toplam Transformer Bloğu Parametreleri\n",
    "\\[\n",
    "12 \\times 7,040,256 = 84,483,072\n",
    "\\]\n",
    "\n",
    "#### Toplam Parametreler\n",
    "\\[\n",
    "38,609,856 (Embedding) + 84,483,072 (Transformer blokları) = 123,092,928\n",
    "\\]\n",
    "\n",
    "Bu hesaplamalar, yaklaşık 124 milyon parametreye oldukça yakındır. Modelin toplam parametre sayısının 124M olduğu, verilen konfigürasyon değerlerine göre bu şekilde hesaplanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import struct\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch._inductor.config as config\n",
    "import torch.nn as nn\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"Careful there are a few versions of GeLU, this one is the exact one used by OpenAI\"\"\"\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        if FLASH:\n",
    "            # flashattention\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            # this materializes the large (T,T) matrix for all the queries and keys\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = NewGELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1 # special flag for residual scaling.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" @dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768 \"\"\"\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128\n",
    "    vocab_size: int = 64\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print0(*args, **kwargs):\n",
    "    # modified print that only prints from the master process\n",
    "    # if this is not a distributed run, it's just a print\n",
    "    if int(os.environ.get(\"RANK\", 0)) == 0:\n",
    "        print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # wte: word token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # wpe: positional embeddings\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # h: transformer blocks\n",
    "            ln_f = nn.LayerNorm(config.n_embd), # ln_f: final layer norm before output\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # lm_head: head for language modeling\n",
    "        self.lm_head.LLMC_SKIP_INIT = 1 # don't init this one, we will tie weights\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights, use a torch rng object to be very careful\n",
    "        self.init_rng = torch.Generator()\n",
    "        self.init_rng.manual_seed(42)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02 / math.sqrt(2 * self.config.n_layer)\n",
    "            # we want to skip initializing lm_head, which shares parameters with wte\n",
    "            # and wte was already initialized down below during the Embedding init\n",
    "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "\n",
    "    def forward(self, idx, targets=None, return_logits=True):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        # there are performance reasons why not returning logits is prudent, if not needed\n",
    "        if not return_logits:\n",
    "            logits = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type, zero_stage):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print0(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print0(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        print0(f\"using fused AdamW: {use_fused}\")\n",
    "        if zero_stage == 1:\n",
    "            print0(\"using ZeroRedundancyOptimizer\")\n",
    "            optimizer = ZeroRedundancyOptimizer(**optim_groups[0], optimizer_class=torch.optim.AdamW,\n",
    "                                                lr=learning_rate, betas=betas, fused=use_fused)\n",
    "            optimizer.add_param_group(optim_groups[1])\n",
    "        else:\n",
    "            print0(\"using regular AdamW\")\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peek_data_shard(filename):\n",
    "    # only reads the header, returns header data\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()\n",
    "        tokens = [x for x in tokens]\n",
    "    return len(tokens)\n",
    "\n",
    "def _load_data_shard(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()\n",
    "        tokens = [x for x in tokens]\n",
    "    return tokens\n",
    "\n",
    "class DistributedDataLoader:\n",
    "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # glob files that match the pattern\n",
    "        self.files = sorted(glob.glob(filename_pattern))\n",
    "        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n",
    "\n",
    "        # load and validate all data shards, count number of tokens in total\n",
    "        ntok_total = 0\n",
    "        for fname in self.files:\n",
    "            shard_ntok = _peek_data_shard(fname)\n",
    "            assert shard_ntok >= num_processes * B * T + 1, f\"dataset shard {fname} is too small for the current setting\"\n",
    "            ntok_total += shard_ntok\n",
    "        self.ntok_total = ntok_total\n",
    "        print0(f\"DataLoader: total number of tokens: {ntok_total:,} across {len(self.files)} files\")\n",
    "\n",
    "        # kick things off\n",
    "        self.current_shard = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # we're being a bit clever here: if we already had shard 0 loaded,\n",
    "        # then don't do the work to reload it, just reset the pointer\n",
    "        if self.current_shard != 0:\n",
    "            self.current_shard = 0\n",
    "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "\n",
    "    def advance(self): # advance to next data shard\n",
    "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        # buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n",
    "        buf = torch.tensor(buf, dtype=torch.long)\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the start pointer in current shard\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds advance the shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.advance()\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fp16(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.float16)\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)\n",
    "\n",
    "def write_fp32(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.float32)\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)\n",
    "\n",
    "def write_bf16(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.bfloat16)\n",
    "    # numpy doesn't have bf16 datatype so we have to trick it\n",
    "    t = t.view(torch.int16) # trick: reinterpret as int16\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tensors(model_tensors, L, file, dtype):\n",
    "    # writes the GPT-2 model's weights to a binary file\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"}\n",
    "    write_fun = write_fp16 if dtype == \"float16\" else write_fp32 if dtype == \"float32\" else write_bf16\n",
    "    write_fun(model_tensors[\"transformer.wte.weight\"], file) # (V, C)\n",
    "    write_fun(model_tensors[\"transformer.wpe.weight\"], file) # (T, C)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
    "    for i in range(L): # (L, 3C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
    "    for i in range(L): # (L, 3C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
    "    for i in range(L): # (L, C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
    "    for i in range(L): # (L, 4C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
    "    for i in range(L): # (L, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
    "    for i in range(L): # (L, C, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
    "    write_fun(model_tensors[\"transformer.ln_f.weight\"], file) # (C, )\n",
    "    write_fun(model_tensors[\"transformer.ln_f.bias\"], file) # (C, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pad_vocab(tensor, multiple=128, value=0):\n",
    "    \"\"\"\n",
    "    The dimension of the vocab size in GPT-2 is 50,257\n",
    "    which is unfortunately a very unfriendly number for a lot of\n",
    "    matrix operations on the GPU. So we pad it to the nearest\n",
    "    friendlier multiple, e.g. 50,304 if multiple=128 when we\n",
    "    export the weights into C land. This is a NOOP algorithmically\n",
    "    and is only done to make the tensor operations more efficient.\n",
    "    \"\"\"\n",
    "    assert tensor.ndim == 2\n",
    "    V, C = tensor.shape\n",
    "    # assert V == 50257, \"just being defensive here\"\n",
    "    # calculate padded vocab size by rounding up to nearest multiple\n",
    "    Vp = ((V + multiple - 1) // multiple) * multiple\n",
    "    # pad the tensor\n",
    "    pad_rows = Vp - V\n",
    "    padded = tensor if pad_rows == 0 else F.pad(tensor, (0, 0, 0, pad_rows), value=value)\n",
    "    assert padded.shape == (Vp, C)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(model, filename, dtype):\n",
    "    # everything we need to instantiate the model\n",
    "    # 1) header is: version int, GPTConfig ints, padding to 1024 bytes\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"} # float16 todo maybe later\n",
    "    version = {\n",
    "        \"float16\": 2, # 2: all tensors are fp16, padded vocab\n",
    "        \"float32\": 3, # 3: all tensors are fp32, padded vocab\n",
    "        \"bfloat16\": 5, # 5: all tensors are bf16, padded vocab\n",
    "    }[dtype]\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240326 # magic\n",
    "    header[1] = version # checkpoint version\n",
    "    header[2] = model.config.block_size\n",
    "    header[3] = model.config.vocab_size\n",
    "    header[4] = model.config.n_layer\n",
    "    header[5] = model.config.n_head\n",
    "    header[6] = model.config.n_embd\n",
    "    # 2) the parameters follow the header\n",
    "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
    "    # pad the vocab to a multiple of 128 here at export, for efficiency in C\n",
    "    wte = params[\"transformer.wte.weight\"] # (V, C)\n",
    "    wte_padded = pad_vocab(wte) # (Vp, C)\n",
    "    params[\"transformer.wte.weight\"] = wte_padded # (Vp, C)\n",
    "    print(f\"padded vocab size from {wte.size(0)} to {wte_padded.size(0)}\")\n",
    "    header[7] = wte_padded.size(0) # padded vocab size store in header\n",
    "    # now write to file\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes()) # header\n",
    "        write_tensors(params, model.config.n_layer, file, dtype) # params\n",
    "    print(f\"wrote {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_state(model, x, y, logits, loss, filename, dtype=\"float32\"):\n",
    "    # the state is used for debugging.\n",
    "    # it contains information about the input, logits, loss, and the parameter gradients\n",
    "    # this can be used for checking the computation correctness in C\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240327 # magic\n",
    "    header[1] = 2 # run state version = 2 (1 -> 2 for padded vocab changes)\n",
    "    header[2] = x.size(0) # batch size of the batch, B\n",
    "    header[3] = x.size(1) # temporal extent of the batch, T\n",
    "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
    "    # pad the vocab grads here as well, to mirror write_model\n",
    "    wte_grad = grads[\"transformer.wte.weight\"] # (V, C)\n",
    "    wte_grad_padded = pad_vocab(wte_grad, value=0) # (Vp, C) # TODO later maybe pad with nan?\n",
    "    grads[\"transformer.wte.weight\"] = wte_grad_padded # (Vp, C)\n",
    "    print(f\"padded vocab size in reference grads from {wte_grad.size(0)} to {wte_grad_padded.size(0)}\")\n",
    "    with open(filename, \"wb\") as file:\n",
    "        # header\n",
    "        file.write(header.numpy().tobytes())\n",
    "        # input x\n",
    "        file.write(x.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # targets y\n",
    "        file.write(y.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # logits (result of the model forward pass)\n",
    "        write_fp32(logits.cpu(), file)\n",
    "        # loss (single float, result of the cross entropy loss)\n",
    "        write_fp32(loss.cpu(), file)\n",
    "        # gradients\n",
    "        write_tensors(grads, model.config.n_layer, file, dtype)\n",
    "    print(f\"wrote {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokenizer(enc, filename):\n",
    "    n = enc.max_token_value + 1\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240328 # magic\n",
    "    header[1] = 2 # tokenizer version = 2 (1 -> 2: includes EOT token)\n",
    "    header[2] = n # number of tokens\n",
    "    header[3] = enc.eot_token # EOT token\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes())\n",
    "        for i in range(n):\n",
    "            b = enc.decode_bytes([i])\n",
    "            length = len(b)\n",
    "            assert length < 256, f\"Token length exceeds 255: {length}\"\n",
    "            file.write(struct.pack(\"<B\", length))  # Write the length as a 1-byte unsigned integer\n",
    "            file.write(b)  # Write the actual bytes\n",
    "    print(f\"wrote {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps (cpu)\n"
     ]
    }
   ],
   "source": [
    "# B, T = 2, 4\n",
    "\"\"\" \n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128\n",
    "    vocab_size: int = 64\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 16 \"\"\"\n",
    "\n",
    "B, T = 2, 32 # batch size, sequence length\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "print(f\"using device: {device} ({device_type})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp_rank = 0\n",
    "ddp_local_rank = 0\n",
    "zero_stage = 0\n",
    "ddp_world_size = 1\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "total_batch_size = 128\n",
    "tokens_per_fwdbwd = B * T\n",
    "tokens_per_fwdbwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 128\n",
      "=> calculated gradient accumulation steps: 2\n"
     ]
    }
   ],
   "source": [
    "assert total_batch_size % tokens_per_fwdbwd == 0\n",
    "grad_accum_steps = total_batch_size // tokens_per_fwdbwd\n",
    "print0(f\"total desired batch size: {total_batch_size}\")\n",
    "print0(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a context manager following the desired dtype and device\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16, 'float8': torch.float8_e4m3fn}['float16']\n",
    "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
    "# rng / reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(64, 32)\n",
       "    (wpe): Embedding(256, 32)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=32, out_features=96, bias=True)\n",
       "          (c_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (gelu): NewGELU()\n",
       "          (c_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "if False: # args.compile:\n",
    "    if hasattr(config, \"coordinate_descent_tuning\"):\n",
    "        config.coordinate_descent_tuning = True # suggested by @Chillee\n",
    "    print0(\"compiling the model...\")\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader: total number of tokens: 892,316 across 1 files\n",
      "DataLoader: total number of tokens: 111,539 across 1 files\n"
     ]
    }
   ],
   "source": [
    "train_loader = DistributedDataLoader(\"tokenizer/train_tokens.bin\", B, T, ddp_rank, ddp_world_size)\n",
    "val_loader = DistributedDataLoader(\"tokenizer/val_tokens.bin\", B, T, ddp_rank, ddp_world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 32]), torch.Size([2, 32]))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = train_loader.next_batch()\n",
    "x, y = x.to(device), y.to(device)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded vocab size from 64 to 128\n",
      "wrote gpt2_12K.bin\n",
      "padded vocab size from 64 to 128\n",
      "wrote gpt2_12K_bf16.bin\n",
      "padded vocab size in reference grads from 64 to 128\n",
      "wrote gpt2_12K_debug_state.bin\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model(x, y)\n",
    "loss.backward()\n",
    "# save model params, in both float32 and bfloat16\n",
    "model_to_size = {\"gpt1Letter\": \"12K\",  \"gpt2\": \"124M\", \"gpt2-medium\": \"355M\", \"gpt2-large\": \"774M\", \"gpt2-xl\": \"1558M\"}\n",
    "model_to_size.update({f\"d{d}\": f\"d{d}\" for d in [12, 24, 36, 48]})\n",
    "model_size_str = model_to_size[\"gpt1Letter\"] # e.g. \"124M\", or \"d12\"\n",
    "write_model(model, f\"gpt2_{model_size_str}.bin\", dtype=\"float16\")\n",
    "write_model(model, f\"gpt2_{model_size_str}_bf16.bin\", dtype=\"bfloat16\")\n",
    "# save x, y, logits, loss, and parameter gradients, for debugging C\n",
    "# always store these in fp32 to have an accurate reference (?)\n",
    "write_state(model, x, y, logits, loss, f\"gpt2_{model_size_str}_debug_state.bin\")\n",
    "# reset the train_loader for the optimization below\n",
    "train_loader.reset()\n",
    "# clear the grads here explicitly because otherwise we'd have a duplicate grad accumulation\n",
    "# since in the training loop we do a backward() and then zero_grad() at the end of the loop\n",
    "# this would cause an incorrect first training step\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0\n",
    "learning_rate = 3e-4\n",
    "learning_rate_decay_frac = 0.0\n",
    "warmup_iters = 0\n",
    "num_iterations = 1000\n",
    "val_loss_every = 0\n",
    "val_max_steps = 20\n",
    "sample_every = 0\n",
    "overfit_single_batch = 1\n",
    "inference_only = 0\n",
    "grad_clip = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 34, with 108,544 parameters\n",
      "num non-decayed parameter tensors: 66, with 3,392 parameters\n",
      "using fused AdamW: False\n",
      "using regular AdamW\n"
     ]
    }
   ],
   "source": [
    "raw_model = model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "# init the optimizer\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=weight_decay,\n",
    "                                            learning_rate=learning_rate, betas=(0.9, 0.95),\n",
    "                                            device_type=device, zero_stage=zero_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    min_lr = learning_rate * learning_rate_decay_frac\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it+1) / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > num_iterations:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (num_iterations - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "              'ı', 'ğ', 'ü', 'ş', 'ö', 'ç',\n",
    "              '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "              ' ', ',', '.', '!', '?', ';', ':', '-', '(', ')', '\"', \"'\", '+', '*', '/', '=', '@', '<', '>', '\\\\', '_',\n",
    "              '\\n',\n",
    "              ]\n",
    "\n",
    "def encode(text):\n",
    "  return [letter_list.index(c) for c in text.lower()]\n",
    "\n",
    "def decode(ids):\n",
    "  return ''.join([letter_list[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/1000 | train loss 4.147572 | norm 3.9352 | lr 3.00e-04 | (69.67 ms | 1837 tok/s)\n",
      "step    2/1000 | train loss 4.061966 | norm 3.0696 | lr 3.00e-04 | (63.64 ms | 2011 tok/s)\n",
      "step    3/1000 | train loss 4.000878 | norm 1.8897 | lr 3.00e-04 | (61.31 ms | 2088 tok/s)\n",
      "step    4/1000 | train loss 3.962283 | norm 1.4610 | lr 3.00e-04 | (64.28 ms | 1991 tok/s)\n",
      "step    5/1000 | train loss 3.934070 | norm 1.4428 | lr 3.00e-04 | (64.13 ms | 1996 tok/s)\n",
      "step    6/1000 | train loss 3.908884 | norm 1.5155 | lr 3.00e-04 | (64.73 ms | 1978 tok/s)\n",
      "step    7/1000 | train loss 3.883915 | norm 1.5605 | lr 3.00e-04 | (59.78 ms | 2141 tok/s)\n",
      "step    8/1000 | train loss 3.859229 | norm 1.4854 | lr 3.00e-04 | (62.06 ms | 2063 tok/s)\n",
      "step    9/1000 | train loss 3.836764 | norm 1.3438 | lr 3.00e-04 | (61.34 ms | 2087 tok/s)\n",
      "step   10/1000 | train loss 3.817059 | norm 1.2818 | lr 3.00e-04 | (63.04 ms | 2030 tok/s)\n",
      "step   11/1000 | train loss 3.798239 | norm 1.2798 | lr 3.00e-04 | (64.68 ms | 1979 tok/s)\n",
      "step   12/1000 | train loss 3.779038 | norm 1.2904 | lr 3.00e-04 | (61.94 ms | 2067 tok/s)\n",
      "step   13/1000 | train loss 3.759562 | norm 1.2883 | lr 3.00e-04 | (62.46 ms | 2049 tok/s)\n",
      "step   14/1000 | train loss 3.740544 | norm 1.2688 | lr 3.00e-04 | (61.49 ms | 2082 tok/s)\n",
      "step   15/1000 | train loss 3.722635 | norm 1.2815 | lr 3.00e-04 | (61.97 ms | 2066 tok/s)\n",
      "step   16/1000 | train loss 3.705767 | norm 1.3107 | lr 3.00e-04 | (60.43 ms | 2118 tok/s)\n",
      "step   17/1000 | train loss 3.689604 | norm 1.3548 | lr 3.00e-04 | (61.63 ms | 2077 tok/s)\n",
      "step   18/1000 | train loss 3.673574 | norm 1.3696 | lr 3.00e-04 | (62.66 ms | 2043 tok/s)\n",
      "step   19/1000 | train loss 3.657400 | norm 1.3557 | lr 3.00e-04 | (63.57 ms | 2013 tok/s)\n",
      "step   20/1000 | train loss 3.641167 | norm 1.3552 | lr 3.00e-04 | (61.13 ms | 2094 tok/s)\n",
      "step   21/1000 | train loss 3.625070 | norm 1.3490 | lr 3.00e-04 | (62.90 ms | 2035 tok/s)\n",
      "step   22/1000 | train loss 3.609541 | norm 1.3412 | lr 3.00e-04 | (61.93 ms | 2067 tok/s)\n",
      "step   23/1000 | train loss 3.594787 | norm 1.3664 | lr 3.00e-04 | (62.34 ms | 2053 tok/s)\n",
      "step   24/1000 | train loss 3.580495 | norm 1.3657 | lr 3.00e-04 | (62.24 ms | 2057 tok/s)\n",
      "step   25/1000 | train loss 3.566446 | norm 1.3601 | lr 3.00e-04 | (62.42 ms | 2051 tok/s)\n",
      "step   26/1000 | train loss 3.552765 | norm 1.3946 | lr 3.00e-04 | (63.07 ms | 2029 tok/s)\n",
      "step   27/1000 | train loss 3.539281 | norm 1.4795 | lr 2.99e-04 | (61.34 ms | 2087 tok/s)\n",
      "step   28/1000 | train loss 3.525871 | norm 1.6772 | lr 2.99e-04 | (60.73 ms | 2108 tok/s)\n",
      "step   29/1000 | train loss 3.512690 | norm 1.7626 | lr 2.99e-04 | (63.40 ms | 2019 tok/s)\n",
      "step   30/1000 | train loss 3.499448 | norm 1.5804 | lr 2.99e-04 | (64.16 ms | 1995 tok/s)\n",
      "step   31/1000 | train loss 3.485919 | norm 1.4602 | lr 2.99e-04 | (69.25 ms | 1848 tok/s)\n",
      "step   32/1000 | train loss 3.472304 | norm 1.3713 | lr 2.99e-04 | (65.19 ms | 1963 tok/s)\n",
      "step   33/1000 | train loss 3.458914 | norm 1.3993 | lr 2.99e-04 | (65.60 ms | 1951 tok/s)\n",
      "step   34/1000 | train loss 3.445725 | norm 1.4332 | lr 2.99e-04 | (62.29 ms | 2055 tok/s)\n",
      "step   35/1000 | train loss 3.432967 | norm 2.0170 | lr 2.99e-04 | (62.68 ms | 2042 tok/s)\n",
      "step   36/1000 | train loss 3.421213 | norm 2.9012 | lr 2.99e-04 | (62.38 ms | 2052 tok/s)\n",
      "step   37/1000 | train loss 3.408494 | norm 1.7596 | lr 2.99e-04 | (61.26 ms | 2089 tok/s)\n",
      "step   38/1000 | train loss 3.396523 | norm 1.4436 | lr 2.99e-04 | (61.00 ms | 2098 tok/s)\n",
      "step   39/1000 | train loss 3.384717 | norm 1.9672 | lr 2.99e-04 | (60.73 ms | 2108 tok/s)\n",
      "step   40/1000 | train loss 3.373411 | norm 3.0142 | lr 2.99e-04 | (58.96 ms | 2171 tok/s)\n",
      "step   41/1000 | train loss 3.361800 | norm 2.4599 | lr 2.99e-04 | (60.41 ms | 2119 tok/s)\n",
      "step   42/1000 | train loss 3.351132 | norm 2.5871 | lr 2.99e-04 | (59.34 ms | 2157 tok/s)\n",
      "step   43/1000 | train loss 3.339563 | norm 1.7176 | lr 2.99e-04 | (58.63 ms | 2183 tok/s)\n",
      "step   44/1000 | train loss 3.328242 | norm 1.4514 | lr 2.99e-04 | (65.24 ms | 1962 tok/s)\n",
      "step   45/1000 | train loss 3.317114 | norm 2.0781 | lr 2.99e-04 | (67.36 ms | 1900 tok/s)\n",
      "step   46/1000 | train loss 3.307427 | norm 4.2788 | lr 2.99e-04 | (65.69 ms | 1948 tok/s)\n",
      "step   47/1000 | train loss 3.295455 | norm 1.5538 | lr 2.98e-04 | (64.42 ms | 1987 tok/s)\n",
      "step   48/1000 | train loss 3.285415 | norm 2.9700 | lr 2.98e-04 | (60.26 ms | 2124 tok/s)\n",
      "step   49/1000 | train loss 3.274582 | norm 2.8691 | lr 2.98e-04 | (63.41 ms | 2018 tok/s)\n",
      "step   50/1000 | train loss 3.264767 | norm 3.3246 | lr 2.98e-04 | (60.77 ms | 2106 tok/s)\n",
      "step   51/1000 | train loss 3.254391 | norm 2.4619 | lr 2.98e-04 | (60.60 ms | 2112 tok/s)\n",
      "step   52/1000 | train loss 3.243766 | norm 1.5781 | lr 2.98e-04 | (60.89 ms | 2102 tok/s)\n",
      "step   53/1000 | train loss 3.233396 | norm 2.3442 | lr 2.98e-04 | (60.81 ms | 2105 tok/s)\n",
      "step   54/1000 | train loss 3.225147 | norm 5.4967 | lr 2.98e-04 | (63.55 ms | 2014 tok/s)\n",
      "step   55/1000 | train loss 3.213151 | norm 1.6167 | lr 2.98e-04 | (62.38 ms | 2052 tok/s)\n",
      "step   56/1000 | train loss 3.208076 | norm 8.3075 | lr 2.98e-04 | (62.11 ms | 2061 tok/s)\n",
      "step   57/1000 | train loss 3.195307 | norm 4.5559 | lr 2.98e-04 | (61.74 ms | 2073 tok/s)\n",
      "step   58/1000 | train loss 3.191330 | norm 9.1526 | lr 2.98e-04 | (62.44 ms | 2050 tok/s)\n",
      "step   59/1000 | train loss 3.183872 | norm 8.3266 | lr 2.98e-04 | (61.61 ms | 2078 tok/s)\n",
      "step   60/1000 | train loss 3.171393 | norm 5.0552 | lr 2.97e-04 | (61.35 ms | 2086 tok/s)\n",
      "step   61/1000 | train loss 3.163797 | norm 5.7084 | lr 2.97e-04 | (60.99 ms | 2099 tok/s)\n",
      "step   62/1000 | train loss 3.157877 | norm 6.4957 | lr 2.97e-04 | (60.93 ms | 2101 tok/s)\n",
      "step   63/1000 | train loss 3.146171 | norm 3.4643 | lr 2.97e-04 | (61.65 ms | 2076 tok/s)\n",
      "step   64/1000 | train loss 3.144451 | norm 7.3351 | lr 2.97e-04 | (59.60 ms | 2148 tok/s)\n",
      "step   65/1000 | train loss 3.136844 | norm 6.6247 | lr 2.97e-04 | (59.96 ms | 2135 tok/s)\n",
      "step   66/1000 | train loss 3.124961 | norm 3.5211 | lr 2.97e-04 | (61.12 ms | 2094 tok/s)\n",
      "step   67/1000 | train loss 3.118797 | norm 4.2113 | lr 2.97e-04 | (60.50 ms | 2116 tok/s)\n",
      "step   68/1000 | train loss 3.111583 | norm 5.0123 | lr 2.97e-04 | (61.18 ms | 2092 tok/s)\n",
      "step   69/1000 | train loss 3.102643 | norm 2.1584 | lr 2.97e-04 | (61.83 ms | 2070 tok/s)\n",
      "step   70/1000 | train loss 3.097758 | norm 6.4366 | lr 2.96e-04 | (60.81 ms | 2105 tok/s)\n",
      "step   71/1000 | train loss 3.091147 | norm 4.1044 | lr 2.96e-04 | (59.00 ms | 2169 tok/s)\n",
      "step   72/1000 | train loss 3.083000 | norm 5.3287 | lr 2.96e-04 | (58.57 ms | 2185 tok/s)\n",
      "step   73/1000 | train loss 3.073999 | norm 4.9751 | lr 2.96e-04 | (60.27 ms | 2124 tok/s)\n",
      "step   74/1000 | train loss 3.067051 | norm 3.4189 | lr 2.96e-04 | (60.36 ms | 2121 tok/s)\n",
      "step   75/1000 | train loss 3.058397 | norm 3.4600 | lr 2.96e-04 | (61.32 ms | 2087 tok/s)\n",
      "step   76/1000 | train loss 3.050417 | norm 3.2518 | lr 2.96e-04 | (59.60 ms | 2148 tok/s)\n",
      "step   77/1000 | train loss 3.042562 | norm 2.5241 | lr 2.96e-04 | (60.36 ms | 2120 tok/s)\n",
      "step   78/1000 | train loss 3.034684 | norm 3.7505 | lr 2.96e-04 | (60.54 ms | 2114 tok/s)\n",
      "step   79/1000 | train loss 3.026208 | norm 2.4659 | lr 2.96e-04 | (60.30 ms | 2123 tok/s)\n",
      "step   80/1000 | train loss 3.018394 | norm 4.2915 | lr 2.95e-04 | (57.91 ms | 2210 tok/s)\n",
      "step   81/1000 | train loss 3.009506 | norm 3.1097 | lr 2.95e-04 | (57.56 ms | 2224 tok/s)\n",
      "step   82/1000 | train loss 3.001856 | norm 4.6432 | lr 2.95e-04 | (58.87 ms | 2174 tok/s)\n",
      "step   83/1000 | train loss 2.993146 | norm 4.2745 | lr 2.95e-04 | (58.55 ms | 2186 tok/s)\n",
      "step   84/1000 | train loss 2.983496 | norm 2.6273 | lr 2.95e-04 | (58.28 ms | 2196 tok/s)\n",
      "step   85/1000 | train loss 2.974648 | norm 2.9230 | lr 2.95e-04 | (60.77 ms | 2106 tok/s)\n",
      "step   86/1000 | train loss 2.966097 | norm 2.7620 | lr 2.95e-04 | (60.97 ms | 2099 tok/s)\n",
      "step   87/1000 | train loss 2.956022 | norm 2.1705 | lr 2.95e-04 | (58.69 ms | 2181 tok/s)\n",
      "step   88/1000 | train loss 2.948199 | norm 3.4407 | lr 2.94e-04 | (58.57 ms | 2185 tok/s)\n",
      "step   89/1000 | train loss 2.938204 | norm 2.7591 | lr 2.94e-04 | (58.16 ms | 2201 tok/s)\n",
      "step   90/1000 | train loss 2.928911 | norm 3.3902 | lr 2.94e-04 | (57.71 ms | 2218 tok/s)\n",
      "step   91/1000 | train loss 2.919454 | norm 3.0207 | lr 2.94e-04 | (57.84 ms | 2213 tok/s)\n",
      "step   92/1000 | train loss 2.909944 | norm 3.3449 | lr 2.94e-04 | (59.03 ms | 2168 tok/s)\n",
      "step   93/1000 | train loss 2.900444 | norm 2.8862 | lr 2.94e-04 | (58.43 ms | 2191 tok/s)\n",
      "step   94/1000 | train loss 2.890696 | norm 2.8136 | lr 2.94e-04 | (59.81 ms | 2140 tok/s)\n",
      "step   95/1000 | train loss 2.881433 | norm 3.1052 | lr 2.94e-04 | (58.25 ms | 2197 tok/s)\n",
      "step   96/1000 | train loss 2.871765 | norm 4.1208 | lr 2.93e-04 | (58.31 ms | 2195 tok/s)\n",
      "step   97/1000 | train loss 2.862284 | norm 2.8627 | lr 2.93e-04 | (60.02 ms | 2133 tok/s)\n",
      "step   98/1000 | train loss 2.851564 | norm 2.1716 | lr 2.93e-04 | (58.99 ms | 2170 tok/s)\n",
      "step   99/1000 | train loss 2.843564 | norm 3.9415 | lr 2.93e-04 | (59.91 ms | 2137 tok/s)\n",
      "step  100/1000 | train loss 2.833852 | norm 3.3411 | lr 2.93e-04 | (59.95 ms | 2135 tok/s)\n",
      "step  101/1000 | train loss 2.822883 | norm 2.6349 | lr 2.93e-04 | (60.97 ms | 2100 tok/s)\n",
      "step  102/1000 | train loss 2.813314 | norm 2.5554 | lr 2.93e-04 | (60.70 ms | 2109 tok/s)\n",
      "step  103/1000 | train loss 2.803820 | norm 3.3181 | lr 2.92e-04 | (59.46 ms | 2153 tok/s)\n",
      "step  104/1000 | train loss 2.793931 | norm 2.6969 | lr 2.92e-04 | (59.74 ms | 2142 tok/s)\n",
      "step  105/1000 | train loss 2.784385 | norm 3.4543 | lr 2.92e-04 | (60.34 ms | 2121 tok/s)\n",
      "step  106/1000 | train loss 2.774142 | norm 2.4016 | lr 2.92e-04 | (58.44 ms | 2190 tok/s)\n",
      "step  107/1000 | train loss 2.764547 | norm 3.1281 | lr 2.92e-04 | (58.61 ms | 2184 tok/s)\n",
      "step  108/1000 | train loss 2.754410 | norm 2.2802 | lr 2.92e-04 | (58.13 ms | 2202 tok/s)\n",
      "step  109/1000 | train loss 2.745067 | norm 3.0445 | lr 2.91e-04 | (58.47 ms | 2189 tok/s)\n",
      "step  110/1000 | train loss 2.735225 | norm 3.4214 | lr 2.91e-04 | (58.47 ms | 2189 tok/s)\n",
      "step  111/1000 | train loss 2.724573 | norm 2.3717 | lr 2.91e-04 | (58.33 ms | 2194 tok/s)\n",
      "step  112/1000 | train loss 2.714968 | norm 2.5573 | lr 2.91e-04 | (61.11 ms | 2095 tok/s)\n",
      "step  113/1000 | train loss 2.704380 | norm 1.8767 | lr 2.91e-04 | (64.33 ms | 1990 tok/s)\n",
      "step  114/1000 | train loss 2.694917 | norm 2.8560 | lr 2.91e-04 | (66.18 ms | 1934 tok/s)\n",
      "step  115/1000 | train loss 2.685025 | norm 3.3691 | lr 2.90e-04 | (64.97 ms | 1970 tok/s)\n",
      "step  116/1000 | train loss 2.674902 | norm 3.4349 | lr 2.90e-04 | (60.16 ms | 2128 tok/s)\n",
      "step  117/1000 | train loss 2.664403 | norm 1.9334 | lr 2.90e-04 | (60.16 ms | 2128 tok/s)\n",
      "step  118/1000 | train loss 2.654554 | norm 2.5506 | lr 2.90e-04 | (59.23 ms | 2161 tok/s)\n",
      "step  119/1000 | train loss 2.644903 | norm 2.5232 | lr 2.90e-04 | (60.49 ms | 2116 tok/s)\n",
      "step  120/1000 | train loss 2.635141 | norm 3.7479 | lr 2.90e-04 | (58.88 ms | 2174 tok/s)\n",
      "step  121/1000 | train loss 2.625439 | norm 3.3838 | lr 2.89e-04 | (59.90 ms | 2137 tok/s)\n",
      "step  122/1000 | train loss 2.614717 | norm 1.8734 | lr 2.89e-04 | (60.81 ms | 2105 tok/s)\n",
      "step  123/1000 | train loss 2.606211 | norm 2.9132 | lr 2.89e-04 | (61.26 ms | 2090 tok/s)\n",
      "step  124/1000 | train loss 2.596008 | norm 2.6644 | lr 2.89e-04 | (59.33 ms | 2157 tok/s)\n",
      "step  125/1000 | train loss 2.586003 | norm 3.5362 | lr 2.89e-04 | (61.15 ms | 2093 tok/s)\n",
      "step  126/1000 | train loss 2.576972 | norm 3.0306 | lr 2.89e-04 | (65.23 ms | 1962 tok/s)\n",
      "step  127/1000 | train loss 2.566482 | norm 2.3071 | lr 2.88e-04 | (65.81 ms | 1945 tok/s)\n",
      "step  128/1000 | train loss 2.557389 | norm 1.9886 | lr 2.88e-04 | (65.27 ms | 1961 tok/s)\n",
      "step  129/1000 | train loss 2.548807 | norm 2.5977 | lr 2.88e-04 | (61.84 ms | 2070 tok/s)\n",
      "step  130/1000 | train loss 2.538019 | norm 3.1066 | lr 2.88e-04 | (60.33 ms | 2122 tok/s)\n",
      "step  131/1000 | train loss 2.529267 | norm 2.7048 | lr 2.88e-04 | (61.62 ms | 2077 tok/s)\n",
      "step  132/1000 | train loss 2.518803 | norm 2.3357 | lr 2.87e-04 | (60.26 ms | 2124 tok/s)\n",
      "step  133/1000 | train loss 2.509431 | norm 2.6105 | lr 2.87e-04 | (59.54 ms | 2150 tok/s)\n",
      "step  134/1000 | train loss 2.499511 | norm 2.1906 | lr 2.87e-04 | (60.47 ms | 2117 tok/s)\n",
      "step  135/1000 | train loss 2.489283 | norm 1.9686 | lr 2.87e-04 | (59.76 ms | 2142 tok/s)\n",
      "step  136/1000 | train loss 2.480862 | norm 3.4837 | lr 2.87e-04 | (57.96 ms | 2208 tok/s)\n",
      "step  137/1000 | train loss 2.470340 | norm 2.2450 | lr 2.87e-04 | (58.36 ms | 2193 tok/s)\n",
      "step  138/1000 | train loss 2.461518 | norm 2.4992 | lr 2.86e-04 | (58.66 ms | 2182 tok/s)\n",
      "step  139/1000 | train loss 2.451123 | norm 2.1765 | lr 2.86e-04 | (58.60 ms | 2184 tok/s)\n",
      "step  140/1000 | train loss 2.442308 | norm 3.1056 | lr 2.86e-04 | (58.91 ms | 2173 tok/s)\n",
      "step  141/1000 | train loss 2.432401 | norm 2.2531 | lr 2.86e-04 | (58.86 ms | 2175 tok/s)\n",
      "step  142/1000 | train loss 2.423183 | norm 2.4626 | lr 2.86e-04 | (61.86 ms | 2069 tok/s)\n",
      "step  143/1000 | train loss 2.413394 | norm 2.6529 | lr 2.85e-04 | (61.17 ms | 2092 tok/s)\n",
      "step  144/1000 | train loss 2.403498 | norm 2.1202 | lr 2.85e-04 | (58.52 ms | 2187 tok/s)\n",
      "step  145/1000 | train loss 2.394203 | norm 2.3154 | lr 2.85e-04 | (58.85 ms | 2175 tok/s)\n",
      "step  146/1000 | train loss 2.384712 | norm 2.7837 | lr 2.85e-04 | (61.43 ms | 2084 tok/s)\n",
      "step  147/1000 | train loss 2.375477 | norm 2.9070 | lr 2.84e-04 | (59.52 ms | 2151 tok/s)\n",
      "step  148/1000 | train loss 2.365427 | norm 1.9067 | lr 2.84e-04 | (58.56 ms | 2186 tok/s)\n",
      "step  149/1000 | train loss 2.356337 | norm 2.2894 | lr 2.84e-04 | (58.71 ms | 2180 tok/s)\n",
      "step  150/1000 | train loss 2.346947 | norm 2.5140 | lr 2.84e-04 | (59.60 ms | 2148 tok/s)\n",
      "step  151/1000 | train loss 2.337374 | norm 2.5096 | lr 2.84e-04 | (61.31 ms | 2088 tok/s)\n",
      "step  152/1000 | train loss 2.327611 | norm 2.0843 | lr 2.83e-04 | (60.85 ms | 2103 tok/s)\n",
      "step  153/1000 | train loss 2.318153 | norm 1.7354 | lr 2.83e-04 | (58.22 ms | 2199 tok/s)\n",
      "step  154/1000 | train loss 2.308472 | norm 1.7538 | lr 2.83e-04 | (58.45 ms | 2190 tok/s)\n",
      "step  155/1000 | train loss 2.299440 | norm 2.6477 | lr 2.83e-04 | (60.43 ms | 2118 tok/s)\n",
      "step  156/1000 | train loss 2.289747 | norm 3.0849 | lr 2.83e-04 | (59.06 ms | 2167 tok/s)\n",
      "step  157/1000 | train loss 2.280538 | norm 2.5765 | lr 2.82e-04 | (59.85 ms | 2139 tok/s)\n",
      "step  158/1000 | train loss 2.270644 | norm 2.0164 | lr 2.82e-04 | (60.15 ms | 2128 tok/s)\n",
      "step  159/1000 | train loss 2.261640 | norm 2.6541 | lr 2.82e-04 | (59.02 ms | 2169 tok/s)\n",
      "step  160/1000 | train loss 2.252613 | norm 2.8742 | lr 2.82e-04 | (58.83 ms | 2176 tok/s)\n",
      "step  161/1000 | train loss 2.242688 | norm 1.8842 | lr 2.81e-04 | (99.98 ms | 1280 tok/s)\n",
      "step  162/1000 | train loss 2.233858 | norm 2.2613 | lr 2.81e-04 | (62.19 ms | 2058 tok/s)\n",
      "step  163/1000 | train loss 2.225051 | norm 3.0381 | lr 2.81e-04 | (59.81 ms | 2140 tok/s)\n",
      "step  164/1000 | train loss 2.215655 | norm 2.7072 | lr 2.81e-04 | (86.56 ms | 1479 tok/s)\n",
      "step  165/1000 | train loss 2.206604 | norm 2.3330 | lr 2.81e-04 | (61.10 ms | 2095 tok/s)\n",
      "step  166/1000 | train loss 2.197160 | norm 1.8506 | lr 2.80e-04 | (58.97 ms | 2170 tok/s)\n",
      "step  167/1000 | train loss 2.188413 | norm 2.4826 | lr 2.80e-04 | (60.28 ms | 2123 tok/s)\n",
      "step  168/1000 | train loss 2.179763 | norm 3.5494 | lr 2.80e-04 | (59.91 ms | 2136 tok/s)\n",
      "step  169/1000 | train loss 2.170150 | norm 1.9257 | lr 2.80e-04 | (60.36 ms | 2121 tok/s)\n",
      "step  170/1000 | train loss 2.161884 | norm 2.6386 | lr 2.79e-04 | (58.63 ms | 2183 tok/s)\n",
      "step  171/1000 | train loss 2.152421 | norm 1.8680 | lr 2.79e-04 | (58.42 ms | 2191 tok/s)\n",
      "step  172/1000 | train loss 2.143993 | norm 2.6689 | lr 2.79e-04 | (59.91 ms | 2137 tok/s)\n",
      "step  173/1000 | train loss 2.135339 | norm 3.1569 | lr 2.79e-04 | (60.08 ms | 2130 tok/s)\n",
      "step  174/1000 | train loss 2.126310 | norm 2.2887 | lr 2.78e-04 | (58.45 ms | 2190 tok/s)\n",
      "step  175/1000 | train loss 2.117441 | norm 2.2114 | lr 2.78e-04 | (58.79 ms | 2177 tok/s)\n",
      "step  176/1000 | train loss 2.108702 | norm 2.3884 | lr 2.78e-04 | (58.91 ms | 2173 tok/s)\n",
      "step  177/1000 | train loss 2.099872 | norm 2.7298 | lr 2.78e-04 | (59.59 ms | 2148 tok/s)\n",
      "step  178/1000 | train loss 2.091212 | norm 2.3620 | lr 2.77e-04 | (60.51 ms | 2115 tok/s)\n",
      "step  179/1000 | train loss 2.083113 | norm 2.8689 | lr 2.77e-04 | (59.29 ms | 2159 tok/s)\n",
      "step  180/1000 | train loss 2.073762 | norm 2.1715 | lr 2.77e-04 | (60.63 ms | 2111 tok/s)\n",
      "step  181/1000 | train loss 2.066375 | norm 3.2230 | lr 2.77e-04 | (60.26 ms | 2124 tok/s)\n",
      "step  182/1000 | train loss 2.056722 | norm 2.0258 | lr 2.76e-04 | (58.30 ms | 2196 tok/s)\n",
      "step  183/1000 | train loss 2.048965 | norm 2.9575 | lr 2.76e-04 | (59.46 ms | 2153 tok/s)\n",
      "step  184/1000 | train loss 2.040241 | norm 2.5769 | lr 2.76e-04 | (61.62 ms | 2077 tok/s)\n",
      "step  185/1000 | train loss 2.031522 | norm 2.2816 | lr 2.76e-04 | (60.06 ms | 2131 tok/s)\n",
      "step  186/1000 | train loss 2.024087 | norm 3.4861 | lr 2.75e-04 | (59.52 ms | 2151 tok/s)\n",
      "step  187/1000 | train loss 2.014478 | norm 1.7625 | lr 2.75e-04 | (58.88 ms | 2174 tok/s)\n",
      "step  188/1000 | train loss 2.007586 | norm 2.3869 | lr 2.75e-04 | (59.71 ms | 2144 tok/s)\n",
      "step  189/1000 | train loss 1.999224 | norm 3.0644 | lr 2.75e-04 | (60.30 ms | 2123 tok/s)\n",
      "step  190/1000 | train loss 1.990041 | norm 2.1975 | lr 2.74e-04 | (57.46 ms | 2228 tok/s)\n",
      "step  191/1000 | train loss 1.982399 | norm 2.2403 | lr 2.74e-04 | (57.68 ms | 2219 tok/s)\n",
      "step  192/1000 | train loss 1.973523 | norm 1.9879 | lr 2.74e-04 | (57.89 ms | 2211 tok/s)\n",
      "step  193/1000 | train loss 1.965351 | norm 2.3492 | lr 2.74e-04 | (59.21 ms | 2162 tok/s)\n",
      "step  194/1000 | train loss 1.956957 | norm 2.7920 | lr 2.73e-04 | (60.67 ms | 2110 tok/s)\n",
      "step  195/1000 | train loss 1.948443 | norm 2.0407 | lr 2.73e-04 | (59.75 ms | 2142 tok/s)\n",
      "step  196/1000 | train loss 1.940369 | norm 2.2197 | lr 2.73e-04 | (59.81 ms | 2140 tok/s)\n",
      "step  197/1000 | train loss 1.931761 | norm 2.3775 | lr 2.72e-04 | (59.37 ms | 2156 tok/s)\n",
      "step  198/1000 | train loss 1.923941 | norm 2.6460 | lr 2.72e-04 | (58.36 ms | 2193 tok/s)\n",
      "step  199/1000 | train loss 1.914993 | norm 2.0685 | lr 2.72e-04 | (57.97 ms | 2208 tok/s)\n",
      "step  200/1000 | train loss 1.907236 | norm 2.3211 | lr 2.72e-04 | (58.70 ms | 2181 tok/s)\n",
      "step  201/1000 | train loss 1.898284 | norm 1.9113 | lr 2.71e-04 | (59.24 ms | 2161 tok/s)\n",
      "step  202/1000 | train loss 1.891085 | norm 2.7457 | lr 2.71e-04 | (58.75 ms | 2179 tok/s)\n",
      "step  203/1000 | train loss 1.882149 | norm 2.1412 | lr 2.71e-04 | (57.34 ms | 2232 tok/s)\n",
      "step  204/1000 | train loss 1.874171 | norm 2.4002 | lr 2.71e-04 | (58.83 ms | 2176 tok/s)\n",
      "step  205/1000 | train loss 1.865757 | norm 2.2161 | lr 2.70e-04 | (59.51 ms | 2151 tok/s)\n",
      "step  206/1000 | train loss 1.857792 | norm 2.2869 | lr 2.70e-04 | (58.15 ms | 2201 tok/s)\n",
      "step  207/1000 | train loss 1.849603 | norm 2.6314 | lr 2.70e-04 | (57.77 ms | 2216 tok/s)\n",
      "step  208/1000 | train loss 1.841538 | norm 2.0227 | lr 2.69e-04 | (58.80 ms | 2177 tok/s)\n",
      "step  209/1000 | train loss 1.832874 | norm 1.7001 | lr 2.69e-04 | (58.87 ms | 2174 tok/s)\n",
      "step  210/1000 | train loss 1.824865 | norm 2.0896 | lr 2.69e-04 | (57.67 ms | 2219 tok/s)\n",
      "step  211/1000 | train loss 1.816921 | norm 2.8044 | lr 2.69e-04 | (57.26 ms | 2235 tok/s)\n",
      "step  212/1000 | train loss 1.808511 | norm 1.8812 | lr 2.68e-04 | (57.93 ms | 2210 tok/s)\n",
      "step  213/1000 | train loss 1.800382 | norm 1.9230 | lr 2.68e-04 | (57.73 ms | 2217 tok/s)\n",
      "step  214/1000 | train loss 1.792702 | norm 2.7657 | lr 2.68e-04 | (59.49 ms | 2152 tok/s)\n",
      "step  215/1000 | train loss 1.784280 | norm 2.2211 | lr 2.67e-04 | (58.33 ms | 2194 tok/s)\n",
      "step  216/1000 | train loss 1.776220 | norm 1.8532 | lr 2.67e-04 | (58.94 ms | 2172 tok/s)\n",
      "step  217/1000 | train loss 1.768661 | norm 2.3878 | lr 2.67e-04 | (59.56 ms | 2149 tok/s)\n",
      "step  218/1000 | train loss 1.760231 | norm 2.0920 | lr 2.66e-04 | (59.03 ms | 2168 tok/s)\n",
      "step  219/1000 | train loss 1.751891 | norm 1.7948 | lr 2.66e-04 | (57.81 ms | 2214 tok/s)\n",
      "step  220/1000 | train loss 1.744619 | norm 2.2496 | lr 2.66e-04 | (57.99 ms | 2207 tok/s)\n",
      "step  221/1000 | train loss 1.736215 | norm 2.1690 | lr 2.66e-04 | (60.99 ms | 2099 tok/s)\n",
      "step  222/1000 | train loss 1.728482 | norm 2.5267 | lr 2.65e-04 | (58.89 ms | 2173 tok/s)\n",
      "step  223/1000 | train loss 1.720596 | norm 2.3473 | lr 2.65e-04 | (57.90 ms | 2211 tok/s)\n",
      "step  224/1000 | train loss 1.712562 | norm 2.1301 | lr 2.65e-04 | (58.33 ms | 2194 tok/s)\n",
      "step  225/1000 | train loss 1.705339 | norm 2.8291 | lr 2.64e-04 | (58.67 ms | 2182 tok/s)\n",
      "step  226/1000 | train loss 1.697037 | norm 1.6846 | lr 2.64e-04 | (60.47 ms | 2117 tok/s)\n",
      "step  227/1000 | train loss 1.689682 | norm 1.9247 | lr 2.64e-04 | (61.56 ms | 2079 tok/s)\n",
      "step  228/1000 | train loss 1.682027 | norm 2.6740 | lr 2.63e-04 | (59.75 ms | 2142 tok/s)\n",
      "step  229/1000 | train loss 1.673924 | norm 1.8179 | lr 2.63e-04 | (58.97 ms | 2171 tok/s)\n",
      "step  230/1000 | train loss 1.666638 | norm 1.9993 | lr 2.63e-04 | (59.06 ms | 2167 tok/s)\n",
      "step  231/1000 | train loss 1.659210 | norm 3.1684 | lr 2.63e-04 | (59.44 ms | 2153 tok/s)\n",
      "step  232/1000 | train loss 1.650946 | norm 1.6714 | lr 2.62e-04 | (61.58 ms | 2079 tok/s)\n",
      "step  233/1000 | train loss 1.644087 | norm 2.1323 | lr 2.62e-04 | (59.07 ms | 2167 tok/s)\n",
      "step  234/1000 | train loss 1.636772 | norm 2.5285 | lr 2.62e-04 | (59.67 ms | 2145 tok/s)\n",
      "step  235/1000 | train loss 1.628144 | norm 1.5251 | lr 2.61e-04 | (59.72 ms | 2143 tok/s)\n",
      "step  236/1000 | train loss 1.622755 | norm 3.0179 | lr 2.61e-04 | (58.97 ms | 2171 tok/s)\n",
      "step  237/1000 | train loss 1.613512 | norm 1.7890 | lr 2.61e-04 | (59.85 ms | 2139 tok/s)\n",
      "step  238/1000 | train loss 1.608274 | norm 3.3643 | lr 2.60e-04 | (59.42 ms | 2154 tok/s)\n",
      "step  239/1000 | train loss 1.599691 | norm 1.9629 | lr 2.60e-04 | (57.66 ms | 2220 tok/s)\n",
      "step  240/1000 | train loss 1.594146 | norm 3.0366 | lr 2.60e-04 | (58.74 ms | 2179 tok/s)\n",
      "step  241/1000 | train loss 1.586529 | norm 2.5951 | lr 2.59e-04 | (60.70 ms | 2109 tok/s)\n",
      "step  242/1000 | train loss 1.578726 | norm 2.2550 | lr 2.59e-04 | (63.50 ms | 2016 tok/s)\n",
      "step  243/1000 | train loss 1.571657 | norm 2.2337 | lr 2.59e-04 | (63.86 ms | 2004 tok/s)\n",
      "step  244/1000 | train loss 1.564640 | norm 2.2903 | lr 2.58e-04 | (64.74 ms | 1977 tok/s)\n",
      "step  245/1000 | train loss 1.557769 | norm 2.4712 | lr 2.58e-04 | (61.48 ms | 2082 tok/s)\n",
      "step  246/1000 | train loss 1.550173 | norm 2.0098 | lr 2.58e-04 | (62.00 ms | 2064 tok/s)\n",
      "step  247/1000 | train loss 1.543656 | norm 2.1616 | lr 2.57e-04 | (61.58 ms | 2079 tok/s)\n",
      "step  248/1000 | train loss 1.535912 | norm 1.6457 | lr 2.57e-04 | (64.12 ms | 1996 tok/s)\n",
      "step  249/1000 | train loss 1.529770 | norm 1.9754 | lr 2.57e-04 | (61.18 ms | 2092 tok/s)\n",
      "step  250/1000 | train loss 1.521971 | norm 2.0142 | lr 2.56e-04 | (60.84 ms | 2104 tok/s)\n",
      "step  251/1000 | train loss 1.514975 | norm 1.8573 | lr 2.56e-04 | (59.90 ms | 2137 tok/s)\n",
      "step  252/1000 | train loss 1.507968 | norm 1.8822 | lr 2.56e-04 | (58.62 ms | 2184 tok/s)\n",
      "step  253/1000 | train loss 1.500303 | norm 1.5683 | lr 2.55e-04 | (58.53 ms | 2187 tok/s)\n",
      "step  254/1000 | train loss 1.494004 | norm 2.4916 | lr 2.55e-04 | (60.41 ms | 2119 tok/s)\n",
      "step  255/1000 | train loss 1.486299 | norm 1.9336 | lr 2.55e-04 | (58.88 ms | 2174 tok/s)\n",
      "step  256/1000 | train loss 1.479695 | norm 2.4191 | lr 2.54e-04 | (59.26 ms | 2160 tok/s)\n",
      "step  257/1000 | train loss 1.472436 | norm 1.9775 | lr 2.54e-04 | (59.43 ms | 2154 tok/s)\n",
      "step  258/1000 | train loss 1.465328 | norm 1.7373 | lr 2.54e-04 | (59.14 ms | 2164 tok/s)\n",
      "step  259/1000 | train loss 1.458396 | norm 1.7879 | lr 2.53e-04 | (59.26 ms | 2160 tok/s)\n",
      "step  260/1000 | train loss 1.451121 | norm 1.7037 | lr 2.53e-04 | (60.18 ms | 2127 tok/s)\n",
      "step  261/1000 | train loss 1.444601 | norm 2.1250 | lr 2.53e-04 | (64.73 ms | 1977 tok/s)\n",
      "step  262/1000 | train loss 1.437188 | norm 1.8772 | lr 2.52e-04 | (66.29 ms | 1931 tok/s)\n",
      "step  263/1000 | train loss 1.430488 | norm 2.1530 | lr 2.52e-04 | (64.53 ms | 1984 tok/s)\n",
      "step  264/1000 | train loss 1.423560 | norm 2.1889 | lr 2.52e-04 | (63.13 ms | 2028 tok/s)\n",
      "step  265/1000 | train loss 1.416657 | norm 2.2135 | lr 2.51e-04 | (62.74 ms | 2040 tok/s)\n",
      "step  266/1000 | train loss 1.409610 | norm 1.5678 | lr 2.51e-04 | (60.05 ms | 2131 tok/s)\n",
      "step  267/1000 | train loss 1.402722 | norm 1.4634 | lr 2.51e-04 | (61.40 ms | 2085 tok/s)\n",
      "step  268/1000 | train loss 1.396391 | norm 2.1345 | lr 2.50e-04 | (62.19 ms | 2058 tok/s)\n",
      "step  269/1000 | train loss 1.389135 | norm 2.0258 | lr 2.50e-04 | (107.08 ms | 1195 tok/s)\n",
      "step  270/1000 | train loss 1.382567 | norm 1.9783 | lr 2.50e-04 | (61.50 ms | 2081 tok/s)\n",
      "step  271/1000 | train loss 1.375634 | norm 1.9257 | lr 2.49e-04 | (62.31 ms | 2054 tok/s)\n",
      "step  272/1000 | train loss 1.369426 | norm 2.6278 | lr 2.49e-04 | (59.36 ms | 2156 tok/s)\n",
      "step  273/1000 | train loss 1.362087 | norm 1.3329 | lr 2.48e-04 | (60.72 ms | 2108 tok/s)\n",
      "step  274/1000 | train loss 1.355785 | norm 1.6424 | lr 2.48e-04 | (61.78 ms | 2072 tok/s)\n",
      "step  275/1000 | train loss 1.349281 | norm 2.0858 | lr 2.48e-04 | (61.61 ms | 2078 tok/s)\n",
      "step  276/1000 | train loss 1.342432 | norm 1.7385 | lr 2.47e-04 | (63.24 ms | 2024 tok/s)\n",
      "step  277/1000 | train loss 1.336057 | norm 1.9787 | lr 2.47e-04 | (65.87 ms | 1943 tok/s)\n",
      "step  278/1000 | train loss 1.329081 | norm 1.6060 | lr 2.47e-04 | (66.83 ms | 1915 tok/s)\n",
      "step  279/1000 | train loss 1.322958 | norm 2.1820 | lr 2.46e-04 | (67.37 ms | 1900 tok/s)\n",
      "step  280/1000 | train loss 1.316077 | norm 1.9392 | lr 2.46e-04 | (66.76 ms | 1917 tok/s)\n",
      "step  281/1000 | train loss 1.309945 | norm 2.2298 | lr 2.46e-04 | (62.66 ms | 2043 tok/s)\n",
      "step  282/1000 | train loss 1.303276 | norm 1.8511 | lr 2.45e-04 | (60.87 ms | 2103 tok/s)\n",
      "step  283/1000 | train loss 1.296934 | norm 1.8887 | lr 2.45e-04 | (61.53 ms | 2080 tok/s)\n",
      "step  284/1000 | train loss 1.290873 | norm 2.2386 | lr 2.45e-04 | (59.59 ms | 2148 tok/s)\n",
      "step  285/1000 | train loss 1.284123 | norm 1.5713 | lr 2.44e-04 | (59.59 ms | 2148 tok/s)\n",
      "step  286/1000 | train loss 1.278093 | norm 1.8002 | lr 2.44e-04 | (58.95 ms | 2171 tok/s)\n",
      "step  287/1000 | train loss 1.271806 | norm 2.0884 | lr 2.43e-04 | (58.61 ms | 2184 tok/s)\n",
      "step  288/1000 | train loss 1.265745 | norm 2.2017 | lr 2.43e-04 | (59.07 ms | 2167 tok/s)\n",
      "step  289/1000 | train loss 1.259119 | norm 1.3810 | lr 2.43e-04 | (59.45 ms | 2153 tok/s)\n",
      "step  290/1000 | train loss 1.253342 | norm 1.7410 | lr 2.42e-04 | (60.44 ms | 2118 tok/s)\n",
      "step  291/1000 | train loss 1.247356 | norm 2.2777 | lr 2.42e-04 | (59.73 ms | 2143 tok/s)\n",
      "step  292/1000 | train loss 1.240873 | norm 1.8150 | lr 2.42e-04 | (60.66 ms | 2110 tok/s)\n",
      "step  293/1000 | train loss 1.234574 | norm 1.7545 | lr 2.41e-04 | (64.05 ms | 1998 tok/s)\n",
      "step  294/1000 | train loss 1.229134 | norm 2.2116 | lr 2.41e-04 | (65.63 ms | 1950 tok/s)\n",
      "step  295/1000 | train loss 1.222437 | norm 1.5619 | lr 2.40e-04 | (66.50 ms | 1925 tok/s)\n",
      "step  296/1000 | train loss 1.216686 | norm 1.8498 | lr 2.40e-04 | (66.09 ms | 1937 tok/s)\n",
      "step  297/1000 | train loss 1.210859 | norm 2.1665 | lr 2.40e-04 | (66.76 ms | 1917 tok/s)\n",
      "step  298/1000 | train loss 1.204569 | norm 1.6362 | lr 2.39e-04 | (65.70 ms | 1948 tok/s)\n",
      "step  299/1000 | train loss 1.198833 | norm 1.8932 | lr 2.39e-04 | (63.58 ms | 2013 tok/s)\n",
      "step  300/1000 | train loss 1.192758 | norm 1.7248 | lr 2.39e-04 | (63.53 ms | 2015 tok/s)\n",
      "step  301/1000 | train loss 1.186946 | norm 1.8169 | lr 2.38e-04 | (62.15 ms | 2060 tok/s)\n",
      "step  302/1000 | train loss 1.180841 | norm 1.6722 | lr 2.38e-04 | (61.52 ms | 2081 tok/s)\n",
      "step  303/1000 | train loss 1.175411 | norm 2.1754 | lr 2.37e-04 | (59.62 ms | 2147 tok/s)\n",
      "step  304/1000 | train loss 1.169254 | norm 1.6385 | lr 2.37e-04 | (59.07 ms | 2167 tok/s)\n",
      "step  305/1000 | train loss 1.163675 | norm 1.8711 | lr 2.37e-04 | (59.99 ms | 2134 tok/s)\n",
      "step  306/1000 | train loss 1.158399 | norm 2.3774 | lr 2.36e-04 | (61.15 ms | 2093 tok/s)\n",
      "step  307/1000 | train loss 1.151821 | norm 1.3523 | lr 2.36e-04 | (59.27 ms | 2160 tok/s)\n",
      "step  308/1000 | train loss 1.147440 | norm 2.0696 | lr 2.35e-04 | (58.49 ms | 2188 tok/s)\n",
      "step  309/1000 | train loss 1.141031 | norm 1.7598 | lr 2.35e-04 | (58.89 ms | 2174 tok/s)\n",
      "step  310/1000 | train loss 1.135220 | norm 1.5114 | lr 2.35e-04 | (59.99 ms | 2134 tok/s)\n",
      "step  311/1000 | train loss 1.130083 | norm 2.2120 | lr 2.34e-04 | (64.37 ms | 1989 tok/s)\n",
      "step  312/1000 | train loss 1.124064 | norm 1.6162 | lr 2.34e-04 | (64.85 ms | 1974 tok/s)\n",
      "step  313/1000 | train loss 1.118847 | norm 1.5222 | lr 2.34e-04 | (65.00 ms | 1969 tok/s)\n",
      "step  314/1000 | train loss 1.112832 | norm 1.5000 | lr 2.33e-04 | (65.43 ms | 1956 tok/s)\n",
      "step  315/1000 | train loss 1.107973 | norm 1.8863 | lr 2.33e-04 | (65.55 ms | 1953 tok/s)\n",
      "step  316/1000 | train loss 1.101516 | norm 1.3297 | lr 2.32e-04 | (68.35 ms | 1873 tok/s)\n",
      "step  317/1000 | train loss 1.097258 | norm 2.2146 | lr 2.32e-04 | (65.31 ms | 1960 tok/s)\n",
      "step  318/1000 | train loss 1.091162 | norm 1.6585 | lr 2.32e-04 | (61.11 ms | 2095 tok/s)\n",
      "step  319/1000 | train loss 1.085844 | norm 1.9567 | lr 2.31e-04 | (63.47 ms | 2017 tok/s)\n",
      "step  320/1000 | train loss 1.080692 | norm 1.8941 | lr 2.31e-04 | (61.92 ms | 2067 tok/s)\n",
      "step  321/1000 | train loss 1.074564 | norm 1.5264 | lr 2.30e-04 | (61.35 ms | 2086 tok/s)\n",
      "step  322/1000 | train loss 1.070086 | norm 2.0971 | lr 2.30e-04 | (66.76 ms | 1917 tok/s)\n",
      "step  323/1000 | train loss 1.063967 | norm 1.4600 | lr 2.30e-04 | (115.11 ms | 1112 tok/s)\n",
      "step  324/1000 | train loss 1.059011 | norm 1.4494 | lr 2.29e-04 | (66.17 ms | 1934 tok/s)\n",
      "step  325/1000 | train loss 1.053359 | norm 1.2073 | lr 2.29e-04 | (62.92 ms | 2034 tok/s)\n",
      "step  326/1000 | train loss 1.048269 | norm 1.5679 | lr 2.28e-04 | (62.91 ms | 2035 tok/s)\n",
      "step  327/1000 | train loss 1.042874 | norm 1.5077 | lr 2.28e-04 | (60.56 ms | 2114 tok/s)\n",
      "step  328/1000 | train loss 1.037390 | norm 1.4135 | lr 2.28e-04 | (59.94 ms | 2135 tok/s)\n",
      "step  329/1000 | train loss 1.032296 | norm 1.6448 | lr 2.27e-04 | (62.76 ms | 2040 tok/s)\n",
      "step  330/1000 | train loss 1.026952 | norm 1.7366 | lr 2.27e-04 | (60.80 ms | 2105 tok/s)\n",
      "step  331/1000 | train loss 1.021614 | norm 1.7062 | lr 2.26e-04 | (61.83 ms | 2070 tok/s)\n",
      "step  332/1000 | train loss 1.016690 | norm 1.8359 | lr 2.26e-04 | (66.40 ms | 1928 tok/s)\n",
      "step  333/1000 | train loss 1.011391 | norm 1.6498 | lr 2.26e-04 | (67.97 ms | 1883 tok/s)\n",
      "step  334/1000 | train loss 1.006251 | norm 1.6105 | lr 2.25e-04 | (68.27 ms | 1875 tok/s)\n",
      "step  335/1000 | train loss 1.001190 | norm 1.5681 | lr 2.25e-04 | (68.39 ms | 1872 tok/s)\n",
      "step  336/1000 | train loss 0.996033 | norm 1.4312 | lr 2.24e-04 | (64.38 ms | 1988 tok/s)\n",
      "step  337/1000 | train loss 0.991211 | norm 1.7075 | lr 2.24e-04 | (63.97 ms | 2001 tok/s)\n",
      "step  338/1000 | train loss 0.985878 | norm 1.3001 | lr 2.23e-04 | (63.63 ms | 2012 tok/s)\n",
      "step  339/1000 | train loss 0.981047 | norm 1.4408 | lr 2.23e-04 | (61.74 ms | 2073 tok/s)\n",
      "step  340/1000 | train loss 0.976023 | norm 1.5936 | lr 2.23e-04 | (69.22 ms | 1849 tok/s)\n",
      "step  341/1000 | train loss 0.971255 | norm 1.8379 | lr 2.22e-04 | (82.16 ms | 1558 tok/s)\n",
      "step  342/1000 | train loss 0.966287 | norm 1.6642 | lr 2.22e-04 | (68.43 ms | 1870 tok/s)\n",
      "step  343/1000 | train loss 0.961135 | norm 1.4173 | lr 2.21e-04 | (64.85 ms | 1974 tok/s)\n",
      "step  344/1000 | train loss 0.956300 | norm 1.3944 | lr 2.21e-04 | (64.25 ms | 1992 tok/s)\n",
      "step  345/1000 | train loss 0.951418 | norm 1.5029 | lr 2.21e-04 | (63.90 ms | 2003 tok/s)\n",
      "step  346/1000 | train loss 0.946671 | norm 1.6251 | lr 2.20e-04 | (62.10 ms | 2061 tok/s)\n",
      "step  347/1000 | train loss 0.941739 | norm 1.3991 | lr 2.20e-04 | (64.18 ms | 1994 tok/s)\n",
      "step  348/1000 | train loss 0.937062 | norm 1.3934 | lr 2.19e-04 | (64.52 ms | 1984 tok/s)\n",
      "step  349/1000 | train loss 0.932427 | norm 1.6730 | lr 2.19e-04 | (62.83 ms | 2037 tok/s)\n",
      "step  350/1000 | train loss 0.927742 | norm 1.5870 | lr 2.19e-04 | (59.79 ms | 2141 tok/s)\n",
      "step  351/1000 | train loss 0.922879 | norm 1.4798 | lr 2.18e-04 | (59.65 ms | 2146 tok/s)\n",
      "step  352/1000 | train loss 0.918146 | norm 1.3839 | lr 2.18e-04 | (62.01 ms | 2064 tok/s)\n",
      "step  353/1000 | train loss 0.913495 | norm 1.4978 | lr 2.17e-04 | (62.78 ms | 2039 tok/s)\n",
      "step  354/1000 | train loss 0.908974 | norm 1.8168 | lr 2.17e-04 | (62.81 ms | 2038 tok/s)\n",
      "step  355/1000 | train loss 0.904279 | norm 1.5233 | lr 2.16e-04 | (59.18 ms | 2163 tok/s)\n",
      "step  356/1000 | train loss 0.899672 | norm 1.3366 | lr 2.16e-04 | (62.81 ms | 2038 tok/s)\n",
      "step  357/1000 | train loss 0.895057 | norm 1.2939 | lr 2.16e-04 | (72.17 ms | 1774 tok/s)\n",
      "step  358/1000 | train loss 0.890626 | norm 1.5025 | lr 2.15e-04 | (61.51 ms | 2081 tok/s)\n",
      "step  359/1000 | train loss 0.886166 | norm 1.6504 | lr 2.15e-04 | (60.32 ms | 2122 tok/s)\n",
      "step  360/1000 | train loss 0.881339 | norm 1.1886 | lr 2.14e-04 | (61.69 ms | 2075 tok/s)\n",
      "step  361/1000 | train loss 0.877117 | norm 1.5281 | lr 2.14e-04 | (63.56 ms | 2014 tok/s)\n",
      "step  362/1000 | train loss 0.872602 | norm 1.5546 | lr 2.13e-04 | (62.39 ms | 2052 tok/s)\n",
      "step  363/1000 | train loss 0.868002 | norm 1.1854 | lr 2.13e-04 | (62.32 ms | 2054 tok/s)\n",
      "step  364/1000 | train loss 0.863819 | norm 1.2997 | lr 2.13e-04 | (63.97 ms | 2001 tok/s)\n",
      "step  365/1000 | train loss 0.859244 | norm 1.4218 | lr 2.12e-04 | (66.57 ms | 1923 tok/s)\n",
      "step  366/1000 | train loss 0.854844 | norm 1.5099 | lr 2.12e-04 | (67.90 ms | 1885 tok/s)\n",
      "step  367/1000 | train loss 0.850570 | norm 1.6303 | lr 2.11e-04 | (107.86 ms | 1187 tok/s)\n",
      "step  368/1000 | train loss 0.846261 | norm 1.5194 | lr 2.11e-04 | (66.40 ms | 1928 tok/s)\n",
      "step  369/1000 | train loss 0.841914 | norm 1.4715 | lr 2.10e-04 | (64.28 ms | 1991 tok/s)\n",
      "step  370/1000 | train loss 0.837654 | norm 1.5160 | lr 2.10e-04 | (64.36 ms | 1989 tok/s)\n",
      "step  371/1000 | train loss 0.833354 | norm 1.3165 | lr 2.10e-04 | (63.71 ms | 2009 tok/s)\n",
      "step  372/1000 | train loss 0.829215 | norm 1.3232 | lr 2.09e-04 | (62.26 ms | 2056 tok/s)\n",
      "step  373/1000 | train loss 0.825098 | norm 1.5911 | lr 2.09e-04 | (59.14 ms | 2164 tok/s)\n",
      "step  374/1000 | train loss 0.820847 | norm 1.2770 | lr 2.08e-04 | (63.34 ms | 2021 tok/s)\n",
      "step  375/1000 | train loss 0.816776 | norm 1.4812 | lr 2.08e-04 | (66.71 ms | 1919 tok/s)\n",
      "step  376/1000 | train loss 0.812594 | norm 1.5128 | lr 2.07e-04 | (60.89 ms | 2102 tok/s)\n",
      "step  377/1000 | train loss 0.808471 | norm 1.3786 | lr 2.07e-04 | (60.52 ms | 2115 tok/s)\n",
      "step  378/1000 | train loss 0.804234 | norm 1.1323 | lr 2.07e-04 | (59.63 ms | 2146 tok/s)\n",
      "step  379/1000 | train loss 0.800585 | norm 1.5465 | lr 2.06e-04 | (60.82 ms | 2105 tok/s)\n",
      "step  380/1000 | train loss 0.796254 | norm 1.3915 | lr 2.06e-04 | (61.66 ms | 2076 tok/s)\n",
      "step  381/1000 | train loss 0.792301 | norm 1.3522 | lr 2.05e-04 | (66.82 ms | 1916 tok/s)\n",
      "step  382/1000 | train loss 0.788316 | norm 1.3950 | lr 2.05e-04 | (62.08 ms | 2062 tok/s)\n",
      "step  383/1000 | train loss 0.784605 | norm 1.6694 | lr 2.04e-04 | (60.48 ms | 2117 tok/s)\n",
      "step  384/1000 | train loss 0.780376 | norm 1.1518 | lr 2.04e-04 | (62.25 ms | 2056 tok/s)\n",
      "step  385/1000 | train loss 0.776457 | norm 1.2459 | lr 2.03e-04 | (63.58 ms | 2013 tok/s)\n",
      "step  386/1000 | train loss 0.772637 | norm 1.3096 | lr 2.03e-04 | (62.98 ms | 2032 tok/s)\n",
      "step  387/1000 | train loss 0.768551 | norm 1.2742 | lr 2.03e-04 | (65.14 ms | 1965 tok/s)\n",
      "step  388/1000 | train loss 0.764766 | norm 1.4525 | lr 2.02e-04 | (62.85 ms | 2037 tok/s)\n",
      "step  389/1000 | train loss 0.761037 | norm 1.6127 | lr 2.02e-04 | (61.68 ms | 2075 tok/s)\n",
      "step  390/1000 | train loss 0.756974 | norm 1.1667 | lr 2.01e-04 | (64.30 ms | 1991 tok/s)\n",
      "step  391/1000 | train loss 0.753237 | norm 1.2463 | lr 2.01e-04 | (63.70 ms | 2009 tok/s)\n",
      "step  392/1000 | train loss 0.749380 | norm 1.2082 | lr 2.00e-04 | (61.56 ms | 2079 tok/s)\n",
      "step  393/1000 | train loss 0.745591 | norm 1.1307 | lr 2.00e-04 | (61.40 ms | 2085 tok/s)\n",
      "step  394/1000 | train loss 0.741817 | norm 1.0794 | lr 1.99e-04 | (62.48 ms | 2049 tok/s)\n",
      "step  395/1000 | train loss 0.738169 | norm 1.2295 | lr 1.99e-04 | (61.61 ms | 2078 tok/s)\n",
      "step  396/1000 | train loss 0.734548 | norm 1.5735 | lr 1.99e-04 | (60.61 ms | 2112 tok/s)\n",
      "step  397/1000 | train loss 0.730646 | norm 1.2611 | lr 1.98e-04 | (59.73 ms | 2143 tok/s)\n",
      "step  398/1000 | train loss 0.727010 | norm 1.3068 | lr 1.98e-04 | (61.10 ms | 2095 tok/s)\n",
      "step  399/1000 | train loss 0.723104 | norm 1.0775 | lr 1.97e-04 | (61.29 ms | 2089 tok/s)\n",
      "step  400/1000 | train loss 0.719589 | norm 1.3206 | lr 1.97e-04 | (61.07 ms | 2096 tok/s)\n",
      "step  401/1000 | train loss 0.716071 | norm 1.4908 | lr 1.96e-04 | (59.87 ms | 2138 tok/s)\n",
      "step  402/1000 | train loss 0.712341 | norm 1.1502 | lr 1.96e-04 | (60.43 ms | 2118 tok/s)\n",
      "step  403/1000 | train loss 0.709081 | norm 1.3858 | lr 1.95e-04 | (66.38 ms | 1928 tok/s)\n",
      "step  404/1000 | train loss 0.705178 | norm 1.1129 | lr 1.95e-04 | (70.43 ms | 1817 tok/s)\n",
      "step  405/1000 | train loss 0.701834 | norm 1.2674 | lr 1.95e-04 | (63.61 ms | 2012 tok/s)\n",
      "step  406/1000 | train loss 0.698051 | norm 1.1098 | lr 1.94e-04 | (61.60 ms | 2078 tok/s)\n",
      "step  407/1000 | train loss 0.694808 | norm 1.2421 | lr 1.94e-04 | (63.39 ms | 2019 tok/s)\n",
      "step  408/1000 | train loss 0.691254 | norm 1.3508 | lr 1.93e-04 | (62.43 ms | 2050 tok/s)\n",
      "step  409/1000 | train loss 0.687945 | norm 1.3435 | lr 1.93e-04 | (61.98 ms | 2065 tok/s)\n",
      "step  410/1000 | train loss 0.684344 | norm 1.1718 | lr 1.92e-04 | (60.56 ms | 2114 tok/s)\n",
      "step  411/1000 | train loss 0.680964 | norm 1.2069 | lr 1.92e-04 | (61.82 ms | 2070 tok/s)\n",
      "step  412/1000 | train loss 0.677539 | norm 1.1833 | lr 1.91e-04 | (62.30 ms | 2055 tok/s)\n",
      "step  413/1000 | train loss 0.674152 | norm 1.0674 | lr 1.91e-04 | (59.67 ms | 2145 tok/s)\n",
      "step  414/1000 | train loss 0.670889 | norm 1.1929 | lr 1.90e-04 | (61.85 ms | 2069 tok/s)\n",
      "step  415/1000 | train loss 0.667501 | norm 1.1113 | lr 1.90e-04 | (62.13 ms | 2060 tok/s)\n",
      "step  416/1000 | train loss 0.664467 | norm 1.4166 | lr 1.90e-04 | (62.00 ms | 2065 tok/s)\n",
      "step  417/1000 | train loss 0.660725 | norm 0.9564 | lr 1.89e-04 | (60.49 ms | 2116 tok/s)\n",
      "step  418/1000 | train loss 0.657666 | norm 1.1864 | lr 1.89e-04 | (60.91 ms | 2101 tok/s)\n",
      "step  419/1000 | train loss 0.654372 | norm 1.2448 | lr 1.88e-04 | (61.05 ms | 2096 tok/s)\n",
      "step  420/1000 | train loss 0.651184 | norm 1.2141 | lr 1.88e-04 | (64.09 ms | 1997 tok/s)\n",
      "step  421/1000 | train loss 0.647869 | norm 1.1789 | lr 1.87e-04 | (65.95 ms | 1941 tok/s)\n",
      "step  422/1000 | train loss 0.644726 | norm 1.1617 | lr 1.87e-04 | (62.53 ms | 2047 tok/s)\n",
      "step  423/1000 | train loss 0.641579 | norm 1.2129 | lr 1.86e-04 | (61.28 ms | 2089 tok/s)\n",
      "step  424/1000 | train loss 0.638435 | norm 1.3440 | lr 1.86e-04 | (63.02 ms | 2031 tok/s)\n",
      "step  425/1000 | train loss 0.635171 | norm 1.0893 | lr 1.85e-04 | (63.44 ms | 2018 tok/s)\n",
      "step  426/1000 | train loss 0.632155 | norm 1.1057 | lr 1.85e-04 | (60.23 ms | 2125 tok/s)\n",
      "step  427/1000 | train loss 0.628963 | norm 1.0325 | lr 1.85e-04 | (61.23 ms | 2091 tok/s)\n",
      "step  428/1000 | train loss 0.626028 | norm 1.2643 | lr 1.84e-04 | (63.25 ms | 2024 tok/s)\n",
      "step  429/1000 | train loss 0.622828 | norm 0.9896 | lr 1.84e-04 | (61.17 ms | 2093 tok/s)\n",
      "step  430/1000 | train loss 0.619988 | norm 1.1756 | lr 1.83e-04 | (59.50 ms | 2151 tok/s)\n",
      "step  431/1000 | train loss 0.616872 | norm 1.1412 | lr 1.83e-04 | (60.45 ms | 2117 tok/s)\n",
      "step  432/1000 | train loss 0.613832 | norm 1.0939 | lr 1.82e-04 | (100.84 ms | 1269 tok/s)\n",
      "step  433/1000 | train loss 0.610814 | norm 1.1295 | lr 1.82e-04 | (61.27 ms | 2089 tok/s)\n",
      "step  434/1000 | train loss 0.607885 | norm 1.2948 | lr 1.81e-04 | (59.70 ms | 2144 tok/s)\n",
      "step  435/1000 | train loss 0.604766 | norm 0.9278 | lr 1.81e-04 | (62.59 ms | 2045 tok/s)\n",
      "step  436/1000 | train loss 0.601907 | norm 1.0519 | lr 1.80e-04 | (65.04 ms | 1968 tok/s)\n",
      "step  437/1000 | train loss 0.599158 | norm 1.3157 | lr 1.80e-04 | (64.28 ms | 1991 tok/s)\n",
      "step  438/1000 | train loss 0.596039 | norm 0.8933 | lr 1.79e-04 | (61.38 ms | 2085 tok/s)\n",
      "step  439/1000 | train loss 0.593411 | norm 1.1192 | lr 1.79e-04 | (60.29 ms | 2123 tok/s)\n",
      "step  440/1000 | train loss 0.590363 | norm 0.9513 | lr 1.79e-04 | (60.76 ms | 2107 tok/s)\n",
      "step  441/1000 | train loss 0.587533 | norm 0.9924 | lr 1.78e-04 | (60.97 ms | 2099 tok/s)\n",
      "step  442/1000 | train loss 0.584803 | norm 1.4550 | lr 1.78e-04 | (60.25 ms | 2124 tok/s)\n",
      "step  443/1000 | train loss 0.581836 | norm 0.9654 | lr 1.77e-04 | (62.39 ms | 2051 tok/s)\n",
      "step  444/1000 | train loss 0.579102 | norm 0.9937 | lr 1.77e-04 | (62.67 ms | 2042 tok/s)\n",
      "step  445/1000 | train loss 0.576314 | norm 1.0983 | lr 1.76e-04 | (62.24 ms | 2057 tok/s)\n",
      "step  446/1000 | train loss 0.573610 | norm 1.0895 | lr 1.76e-04 | (60.10 ms | 2130 tok/s)\n",
      "step  447/1000 | train loss 0.570720 | norm 0.8983 | lr 1.75e-04 | (60.90 ms | 2102 tok/s)\n",
      "step  448/1000 | train loss 0.567979 | norm 0.8764 | lr 1.75e-04 | (62.83 ms | 2037 tok/s)\n",
      "step  449/1000 | train loss 0.565244 | norm 0.8620 | lr 1.74e-04 | (65.44 ms | 1956 tok/s)\n",
      "step  450/1000 | train loss 0.562617 | norm 1.0988 | lr 1.74e-04 | (63.42 ms | 2018 tok/s)\n",
      "step  451/1000 | train loss 0.559938 | norm 1.0750 | lr 1.73e-04 | (64.79 ms | 1976 tok/s)\n",
      "step  452/1000 | train loss 0.557246 | norm 1.0705 | lr 1.73e-04 | (63.76 ms | 2007 tok/s)\n",
      "step  453/1000 | train loss 0.554604 | norm 1.1464 | lr 1.73e-04 | (61.87 ms | 2069 tok/s)\n",
      "step  454/1000 | train loss 0.552007 | norm 1.2165 | lr 1.72e-04 | (63.14 ms | 2027 tok/s)\n",
      "step  455/1000 | train loss 0.549262 | norm 0.9160 | lr 1.72e-04 | (64.49 ms | 1985 tok/s)\n",
      "step  456/1000 | train loss 0.546759 | norm 0.9163 | lr 1.71e-04 | (63.61 ms | 2012 tok/s)\n",
      "step  457/1000 | train loss 0.544218 | norm 0.9832 | lr 1.71e-04 | (63.05 ms | 2030 tok/s)\n",
      "step  458/1000 | train loss 0.541713 | norm 1.0757 | lr 1.70e-04 | (60.68 ms | 2110 tok/s)\n",
      "step  459/1000 | train loss 0.539221 | norm 1.2797 | lr 1.70e-04 | (60.60 ms | 2112 tok/s)\n",
      "step  460/1000 | train loss 0.536500 | norm 0.8306 | lr 1.69e-04 | (61.50 ms | 2081 tok/s)\n",
      "step  461/1000 | train loss 0.534070 | norm 0.8701 | lr 1.69e-04 | (60.57 ms | 2113 tok/s)\n",
      "step  462/1000 | train loss 0.531814 | norm 1.2824 | lr 1.68e-04 | (60.66 ms | 2110 tok/s)\n",
      "step  463/1000 | train loss 0.529134 | norm 0.8777 | lr 1.68e-04 | (63.12 ms | 2028 tok/s)\n",
      "step  464/1000 | train loss 0.526676 | norm 0.8494 | lr 1.67e-04 | (101.77 ms | 1258 tok/s)\n",
      "step  465/1000 | train loss 0.524249 | norm 0.9000 | lr 1.67e-04 | (63.63 ms | 2012 tok/s)\n",
      "step  466/1000 | train loss 0.521929 | norm 0.9584 | lr 1.66e-04 | (60.87 ms | 2103 tok/s)\n",
      "step  467/1000 | train loss 0.519608 | norm 1.1644 | lr 1.66e-04 | (60.29 ms | 2123 tok/s)\n",
      "step  468/1000 | train loss 0.517042 | norm 0.8623 | lr 1.66e-04 | (61.02 ms | 2098 tok/s)\n",
      "step  469/1000 | train loss 0.514610 | norm 0.7130 | lr 1.65e-04 | (61.17 ms | 2092 tok/s)\n",
      "step  470/1000 | train loss 0.512349 | norm 0.9882 | lr 1.65e-04 | (61.20 ms | 2091 tok/s)\n",
      "step  471/1000 | train loss 0.510141 | norm 1.1817 | lr 1.64e-04 | (59.99 ms | 2134 tok/s)\n",
      "step  472/1000 | train loss 0.507591 | norm 0.7551 | lr 1.64e-04 | (60.33 ms | 2122 tok/s)\n",
      "step  473/1000 | train loss 0.505305 | norm 0.8204 | lr 1.63e-04 | (60.33 ms | 2122 tok/s)\n",
      "step  474/1000 | train loss 0.503076 | norm 0.9789 | lr 1.63e-04 | (61.84 ms | 2070 tok/s)\n",
      "step  475/1000 | train loss 0.500870 | norm 1.0360 | lr 1.62e-04 | (60.78 ms | 2106 tok/s)\n",
      "step  476/1000 | train loss 0.498499 | norm 0.8207 | lr 1.62e-04 | (61.81 ms | 2071 tok/s)\n",
      "step  477/1000 | train loss 0.496412 | norm 1.0086 | lr 1.61e-04 | (60.06 ms | 2131 tok/s)\n",
      "step  478/1000 | train loss 0.494397 | norm 1.4495 | lr 1.61e-04 | (61.09 ms | 2095 tok/s)\n",
      "step  479/1000 | train loss 0.491895 | norm 1.0452 | lr 1.60e-04 | (62.70 ms | 2041 tok/s)\n",
      "step  480/1000 | train loss 0.489877 | norm 1.2125 | lr 1.60e-04 | (62.29 ms | 2055 tok/s)\n",
      "step  481/1000 | train loss 0.487495 | norm 0.7861 | lr 1.59e-04 | (61.21 ms | 2091 tok/s)\n",
      "step  482/1000 | train loss 0.485555 | norm 1.1787 | lr 1.59e-04 | (60.34 ms | 2121 tok/s)\n",
      "step  483/1000 | train loss 0.483323 | norm 1.0341 | lr 1.58e-04 | (59.72 ms | 2143 tok/s)\n",
      "step  484/1000 | train loss 0.481219 | norm 0.9049 | lr 1.58e-04 | (62.21 ms | 2058 tok/s)\n",
      "step  485/1000 | train loss 0.479072 | norm 0.9743 | lr 1.58e-04 | (63.31 ms | 2022 tok/s)\n",
      "step  486/1000 | train loss 0.476959 | norm 0.8769 | lr 1.57e-04 | (63.47 ms | 2017 tok/s)\n",
      "step  487/1000 | train loss 0.474862 | norm 0.9375 | lr 1.57e-04 | (64.61 ms | 1981 tok/s)\n",
      "step  488/1000 | train loss 0.472851 | norm 0.9983 | lr 1.56e-04 | (61.07 ms | 2096 tok/s)\n",
      "step  489/1000 | train loss 0.470698 | norm 0.8404 | lr 1.56e-04 | (60.89 ms | 2102 tok/s)\n",
      "step  490/1000 | train loss 0.468603 | norm 0.7608 | lr 1.55e-04 | (60.33 ms | 2122 tok/s)\n",
      "step  491/1000 | train loss 0.466558 | norm 0.7419 | lr 1.55e-04 | (60.05 ms | 2131 tok/s)\n",
      "step  492/1000 | train loss 0.464492 | norm 0.7495 | lr 1.54e-04 | (60.29 ms | 2123 tok/s)\n",
      "step  493/1000 | train loss 0.462453 | norm 0.7069 | lr 1.54e-04 | (60.68 ms | 2109 tok/s)\n",
      "step  494/1000 | train loss 0.460491 | norm 0.7647 | lr 1.53e-04 | (60.95 ms | 2100 tok/s)\n",
      "step  495/1000 | train loss 0.458416 | norm 0.6553 | lr 1.53e-04 | (67.19 ms | 1905 tok/s)\n",
      "step  496/1000 | train loss 0.456446 | norm 0.6968 | lr 1.52e-04 | (71.58 ms | 1788 tok/s)\n",
      "step  497/1000 | train loss 0.454423 | norm 0.6624 | lr 1.52e-04 | (65.73 ms | 1947 tok/s)\n",
      "step  498/1000 | train loss 0.452433 | norm 0.6557 | lr 1.51e-04 | (70.61 ms | 1813 tok/s)\n",
      "step  499/1000 | train loss 0.450530 | norm 0.6993 | lr 1.51e-04 | (68.75 ms | 1862 tok/s)\n",
      "step  500/1000 | train loss 0.448527 | norm 0.6790 | lr 1.50e-04 | (69.21 ms | 1849 tok/s)\n",
      "step  501/1000 | train loss 0.446677 | norm 0.7069 | lr 1.50e-04 | (110.03 ms | 1163 tok/s)\n",
      "step  502/1000 | train loss 0.444774 | norm 0.7646 | lr 1.50e-04 | (66.32 ms | 1930 tok/s)\n",
      "step  503/1000 | train loss 0.442917 | norm 0.8036 | lr 1.49e-04 | (64.35 ms | 1989 tok/s)\n",
      "step  504/1000 | train loss 0.441036 | norm 0.8747 | lr 1.49e-04 | (68.51 ms | 1868 tok/s)\n",
      "step  505/1000 | train loss 0.439059 | norm 0.7690 | lr 1.48e-04 | (72.84 ms | 1757 tok/s)\n",
      "step  506/1000 | train loss 0.437169 | norm 0.7269 | lr 1.48e-04 | (69.74 ms | 1835 tok/s)\n",
      "step  507/1000 | train loss 0.435322 | norm 0.7322 | lr 1.47e-04 | (65.45 ms | 1956 tok/s)\n",
      "step  508/1000 | train loss 0.433518 | norm 0.7209 | lr 1.47e-04 | (63.66 ms | 2011 tok/s)\n",
      "step  509/1000 | train loss 0.431682 | norm 0.8069 | lr 1.46e-04 | (62.98 ms | 2032 tok/s)\n",
      "step  510/1000 | train loss 0.429901 | norm 0.9114 | lr 1.46e-04 | (64.46 ms | 1986 tok/s)\n",
      "step  511/1000 | train loss 0.428124 | norm 0.9672 | lr 1.45e-04 | (62.46 ms | 2049 tok/s)\n",
      "step  512/1000 | train loss 0.426366 | norm 0.9898 | lr 1.45e-04 | (62.28 ms | 2055 tok/s)\n",
      "step  513/1000 | train loss 0.424548 | norm 0.9526 | lr 1.44e-04 | (63.52 ms | 2015 tok/s)\n",
      "step  514/1000 | train loss 0.422787 | norm 0.9192 | lr 1.44e-04 | (60.05 ms | 2132 tok/s)\n",
      "step  515/1000 | train loss 0.420988 | norm 0.8519 | lr 1.43e-04 | (59.12 ms | 2165 tok/s)\n",
      "step  516/1000 | train loss 0.419214 | norm 0.7887 | lr 1.43e-04 | (65.72 ms | 1948 tok/s)\n",
      "step  517/1000 | train loss 0.417461 | norm 0.7365 | lr 1.42e-04 | (66.73 ms | 1918 tok/s)\n",
      "step  518/1000 | train loss 0.415762 | norm 0.7347 | lr 1.42e-04 | (67.76 ms | 1889 tok/s)\n",
      "step  519/1000 | train loss 0.414048 | norm 0.7401 | lr 1.42e-04 | (68.73 ms | 1862 tok/s)\n",
      "step  520/1000 | train loss 0.412359 | norm 0.6879 | lr 1.41e-04 | (62.26 ms | 2056 tok/s)\n",
      "step  521/1000 | train loss 0.410633 | norm 0.6183 | lr 1.41e-04 | (63.22 ms | 2025 tok/s)\n",
      "step  522/1000 | train loss 0.408937 | norm 0.6220 | lr 1.40e-04 | (63.29 ms | 2022 tok/s)\n",
      "step  523/1000 | train loss 0.407298 | norm 0.6657 | lr 1.40e-04 | (63.24 ms | 2024 tok/s)\n",
      "step  524/1000 | train loss 0.405678 | norm 0.6923 | lr 1.39e-04 | (60.85 ms | 2104 tok/s)\n",
      "step  525/1000 | train loss 0.404014 | norm 0.6463 | lr 1.39e-04 | (62.47 ms | 2049 tok/s)\n",
      "step  526/1000 | train loss 0.402432 | norm 0.7231 | lr 1.38e-04 | (59.84 ms | 2139 tok/s)\n",
      "step  527/1000 | train loss 0.400818 | norm 0.7138 | lr 1.38e-04 | (58.96 ms | 2171 tok/s)\n",
      "step  528/1000 | train loss 0.399182 | norm 0.7062 | lr 1.37e-04 | (60.94 ms | 2100 tok/s)\n",
      "step  529/1000 | train loss 0.397621 | norm 0.7592 | lr 1.37e-04 | (60.35 ms | 2121 tok/s)\n",
      "step  530/1000 | train loss 0.396034 | norm 0.8188 | lr 1.36e-04 | (60.78 ms | 2106 tok/s)\n",
      "step  531/1000 | train loss 0.394473 | norm 0.8216 | lr 1.36e-04 | (59.90 ms | 2137 tok/s)\n",
      "step  532/1000 | train loss 0.392913 | norm 0.7791 | lr 1.35e-04 | (61.40 ms | 2085 tok/s)\n",
      "step  533/1000 | train loss 0.391336 | norm 0.7345 | lr 1.35e-04 | (60.62 ms | 2111 tok/s)\n",
      "step  534/1000 | train loss 0.389719 | norm 0.5825 | lr 1.34e-04 | (59.59 ms | 2148 tok/s)\n",
      "step  535/1000 | train loss 0.388196 | norm 0.6483 | lr 1.34e-04 | (59.59 ms | 2148 tok/s)\n",
      "step  536/1000 | train loss 0.386725 | norm 0.7348 | lr 1.34e-04 | (61.16 ms | 2093 tok/s)\n",
      "step  537/1000 | train loss 0.385206 | norm 0.6949 | lr 1.33e-04 | (59.94 ms | 2136 tok/s)\n",
      "step  538/1000 | train loss 0.383687 | norm 0.6567 | lr 1.33e-04 | (59.38 ms | 2155 tok/s)\n",
      "step  539/1000 | train loss 0.382180 | norm 0.6065 | lr 1.32e-04 | (59.13 ms | 2165 tok/s)\n",
      "step  540/1000 | train loss 0.380674 | norm 0.5984 | lr 1.32e-04 | (60.34 ms | 2121 tok/s)\n",
      "step  541/1000 | train loss 0.379193 | norm 0.5631 | lr 1.31e-04 | (60.04 ms | 2132 tok/s)\n",
      "step  542/1000 | train loss 0.377758 | norm 0.6125 | lr 1.31e-04 | (60.92 ms | 2101 tok/s)\n",
      "step  543/1000 | train loss 0.376328 | norm 0.6469 | lr 1.30e-04 | (60.00 ms | 2133 tok/s)\n",
      "step  544/1000 | train loss 0.374882 | norm 0.6119 | lr 1.30e-04 | (60.75 ms | 2107 tok/s)\n",
      "step  545/1000 | train loss 0.373447 | norm 0.6313 | lr 1.29e-04 | (62.43 ms | 2050 tok/s)\n",
      "step  546/1000 | train loss 0.372012 | norm 0.6689 | lr 1.29e-04 | (62.53 ms | 2047 tok/s)\n",
      "step  547/1000 | train loss 0.370631 | norm 0.6445 | lr 1.28e-04 | (60.60 ms | 2112 tok/s)\n",
      "step  548/1000 | train loss 0.369260 | norm 0.6964 | lr 1.28e-04 | (61.30 ms | 2088 tok/s)\n",
      "step  549/1000 | train loss 0.367921 | norm 0.7401 | lr 1.27e-04 | (61.52 ms | 2081 tok/s)\n",
      "step  550/1000 | train loss 0.366578 | norm 0.8611 | lr 1.27e-04 | (60.52 ms | 2115 tok/s)\n",
      "step  551/1000 | train loss 0.365114 | norm 0.7772 | lr 1.27e-04 | (60.31 ms | 2122 tok/s)\n",
      "step  552/1000 | train loss 0.363688 | norm 0.6471 | lr 1.26e-04 | (61.27 ms | 2089 tok/s)\n",
      "step  553/1000 | train loss 0.362365 | norm 0.6342 | lr 1.26e-04 | (61.75 ms | 2073 tok/s)\n",
      "step  554/1000 | train loss 0.361013 | norm 0.6313 | lr 1.25e-04 | (60.38 ms | 2120 tok/s)\n",
      "step  555/1000 | train loss 0.359673 | norm 0.6824 | lr 1.25e-04 | (61.30 ms | 2088 tok/s)\n",
      "step  556/1000 | train loss 0.358336 | norm 0.6227 | lr 1.24e-04 | (59.46 ms | 2153 tok/s)\n",
      "step  557/1000 | train loss 0.356976 | norm 0.5251 | lr 1.24e-04 | (59.63 ms | 2147 tok/s)\n",
      "step  558/1000 | train loss 0.355709 | norm 0.6221 | lr 1.23e-04 | (58.07 ms | 2204 tok/s)\n",
      "step  559/1000 | train loss 0.354407 | norm 0.6260 | lr 1.23e-04 | (58.43 ms | 2191 tok/s)\n",
      "step  560/1000 | train loss 0.353061 | norm 0.5282 | lr 1.22e-04 | (62.07 ms | 2062 tok/s)\n",
      "step  561/1000 | train loss 0.351786 | norm 0.5137 | lr 1.22e-04 | (92.77 ms | 1380 tok/s)\n",
      "step  562/1000 | train loss 0.350527 | norm 0.5786 | lr 1.21e-04 | (62.02 ms | 2064 tok/s)\n",
      "step  563/1000 | train loss 0.349263 | norm 0.5877 | lr 1.21e-04 | (60.92 ms | 2101 tok/s)\n",
      "step  564/1000 | train loss 0.347976 | norm 0.4863 | lr 1.21e-04 | (59.51 ms | 2151 tok/s)\n",
      "step  565/1000 | train loss 0.346711 | norm 0.5150 | lr 1.20e-04 | (60.31 ms | 2122 tok/s)\n",
      "step  566/1000 | train loss 0.345497 | norm 0.5879 | lr 1.20e-04 | (60.26 ms | 2124 tok/s)\n",
      "step  567/1000 | train loss 0.344268 | norm 0.6108 | lr 1.19e-04 | (60.39 ms | 2119 tok/s)\n",
      "step  568/1000 | train loss 0.343064 | norm 0.6631 | lr 1.19e-04 | (58.26 ms | 2197 tok/s)\n",
      "step  569/1000 | train loss 0.341910 | norm 0.7483 | lr 1.18e-04 | (59.13 ms | 2165 tok/s)\n",
      "step  570/1000 | train loss 0.340801 | norm 0.8574 | lr 1.18e-04 | (59.24 ms | 2161 tok/s)\n",
      "step  571/1000 | train loss 0.339554 | norm 0.8275 | lr 1.17e-04 | (58.73 ms | 2179 tok/s)\n",
      "step  572/1000 | train loss 0.338244 | norm 0.6068 | lr 1.17e-04 | (58.36 ms | 2193 tok/s)\n",
      "step  573/1000 | train loss 0.337056 | norm 0.5683 | lr 1.16e-04 | (60.87 ms | 2103 tok/s)\n",
      "step  574/1000 | train loss 0.335926 | norm 0.6660 | lr 1.16e-04 | (60.91 ms | 2101 tok/s)\n",
      "step  575/1000 | train loss 0.334730 | norm 0.6455 | lr 1.15e-04 | (59.73 ms | 2143 tok/s)\n",
      "step  576/1000 | train loss 0.333524 | norm 0.5211 | lr 1.15e-04 | (64.37 ms | 1988 tok/s)\n",
      "step  577/1000 | train loss 0.332399 | norm 0.5547 | lr 1.15e-04 | (120.39 ms | 1063 tok/s)\n",
      "step  578/1000 | train loss 0.331304 | norm 0.6387 | lr 1.14e-04 | (71.27 ms | 1796 tok/s)\n",
      "step  579/1000 | train loss 0.330071 | norm 0.5058 | lr 1.14e-04 | (73.49 ms | 1742 tok/s)\n",
      "step  580/1000 | train loss 0.328979 | norm 0.5255 | lr 1.13e-04 | (73.05 ms | 1752 tok/s)\n",
      "step  581/1000 | train loss 0.327886 | norm 0.5907 | lr 1.13e-04 | (66.87 ms | 1914 tok/s)\n",
      "step  582/1000 | train loss 0.326704 | norm 0.5104 | lr 1.12e-04 | (65.34 ms | 1959 tok/s)\n",
      "step  583/1000 | train loss 0.325645 | norm 0.5494 | lr 1.12e-04 | (68.65 ms | 1864 tok/s)\n",
      "step  584/1000 | train loss 0.324531 | norm 0.5531 | lr 1.11e-04 | (68.71 ms | 1863 tok/s)\n",
      "step  585/1000 | train loss 0.323408 | norm 0.5067 | lr 1.11e-04 | (67.11 ms | 1907 tok/s)\n",
      "step  586/1000 | train loss 0.322347 | norm 0.5170 | lr 1.10e-04 | (66.49 ms | 1925 tok/s)\n",
      "step  587/1000 | train loss 0.321260 | norm 0.5129 | lr 1.10e-04 | (68.89 ms | 1858 tok/s)\n",
      "step  588/1000 | train loss 0.320175 | norm 0.4898 | lr 1.10e-04 | (66.10 ms | 1936 tok/s)\n",
      "step  589/1000 | train loss 0.319114 | norm 0.4834 | lr 1.09e-04 | (119.65 ms | 1070 tok/s)\n",
      "step  590/1000 | train loss 0.318060 | norm 0.4853 | lr 1.09e-04 | (72.64 ms | 1762 tok/s)\n",
      "step  591/1000 | train loss 0.317000 | norm 0.4636 | lr 1.08e-04 | (67.35 ms | 1900 tok/s)\n",
      "step  592/1000 | train loss 0.315949 | norm 0.4623 | lr 1.08e-04 | (66.79 ms | 1916 tok/s)\n",
      "step  593/1000 | train loss 0.314927 | norm 0.4666 | lr 1.07e-04 | (71.53 ms | 1789 tok/s)\n",
      "step  594/1000 | train loss 0.313885 | norm 0.4616 | lr 1.07e-04 | (71.51 ms | 1790 tok/s)\n",
      "step  595/1000 | train loss 0.312869 | norm 0.4775 | lr 1.06e-04 | (73.63 ms | 1738 tok/s)\n",
      "step  596/1000 | train loss 0.311888 | norm 0.5702 | lr 1.06e-04 | (70.51 ms | 1815 tok/s)\n",
      "step  597/1000 | train loss 0.310949 | norm 0.6841 | lr 1.05e-04 | (124.72 ms | 1026 tok/s)\n",
      "step  598/1000 | train loss 0.310063 | norm 0.8717 | lr 1.05e-04 | (69.88 ms | 1832 tok/s)\n",
      "step  599/1000 | train loss 0.309034 | norm 0.8130 | lr 1.05e-04 | (68.61 ms | 1866 tok/s)\n",
      "step  600/1000 | train loss 0.307910 | norm 0.5648 | lr 1.04e-04 | (69.11 ms | 1852 tok/s)\n",
      "step  601/1000 | train loss 0.306908 | norm 0.5486 | lr 1.04e-04 | (70.92 ms | 1805 tok/s)\n",
      "step  602/1000 | train loss 0.306035 | norm 0.7377 | lr 1.03e-04 | (68.88 ms | 1858 tok/s)\n",
      "step  603/1000 | train loss 0.305004 | norm 0.5913 | lr 1.03e-04 | (68.65 ms | 1865 tok/s)\n",
      "step  604/1000 | train loss 0.303980 | norm 0.4640 | lr 1.02e-04 | (68.61 ms | 1866 tok/s)\n",
      "step  605/1000 | train loss 0.303122 | norm 0.6699 | lr 1.02e-04 | (64.40 ms | 1988 tok/s)\n",
      "step  606/1000 | train loss 0.302119 | norm 0.5263 | lr 1.01e-04 | (63.98 ms | 2001 tok/s)\n",
      "step  607/1000 | train loss 0.301155 | norm 0.4622 | lr 1.01e-04 | (63.30 ms | 2022 tok/s)\n",
      "step  608/1000 | train loss 0.300263 | norm 0.5721 | lr 1.01e-04 | (62.61 ms | 2044 tok/s)\n",
      "step  609/1000 | train loss 0.299312 | norm 0.4934 | lr 1.00e-04 | (64.87 ms | 1973 tok/s)\n",
      "step  610/1000 | train loss 0.298377 | norm 0.4473 | lr 9.96e-05 | (67.47 ms | 1897 tok/s)\n",
      "step  611/1000 | train loss 0.297474 | norm 0.4916 | lr 9.92e-05 | (69.55 ms | 1840 tok/s)\n",
      "step  612/1000 | train loss 0.296567 | norm 0.4856 | lr 9.87e-05 | (66.59 ms | 1922 tok/s)\n",
      "step  613/1000 | train loss 0.295641 | norm 0.4266 | lr 9.83e-05 | (68.67 ms | 1864 tok/s)\n",
      "step  614/1000 | train loss 0.294764 | norm 0.4665 | lr 9.79e-05 | (64.36 ms | 1989 tok/s)\n",
      "step  615/1000 | train loss 0.293856 | norm 0.4557 | lr 9.74e-05 | (64.90 ms | 1972 tok/s)\n",
      "step  616/1000 | train loss 0.292977 | norm 0.4351 | lr 9.70e-05 | (64.12 ms | 1996 tok/s)\n",
      "step  617/1000 | train loss 0.292092 | norm 0.4406 | lr 9.65e-05 | (62.65 ms | 2043 tok/s)\n",
      "step  618/1000 | train loss 0.291218 | norm 0.4295 | lr 9.61e-05 | (60.91 ms | 2101 tok/s)\n",
      "step  619/1000 | train loss 0.290352 | norm 0.4267 | lr 9.57e-05 | (62.34 ms | 2053 tok/s)\n",
      "step  620/1000 | train loss 0.289491 | norm 0.4454 | lr 9.52e-05 | (61.05 ms | 2097 tok/s)\n",
      "step  621/1000 | train loss 0.288655 | norm 0.4903 | lr 9.48e-05 | (59.91 ms | 2136 tok/s)\n",
      "step  622/1000 | train loss 0.287814 | norm 0.5182 | lr 9.43e-05 | (58.37 ms | 2193 tok/s)\n",
      "step  623/1000 | train loss 0.287046 | norm 0.6394 | lr 9.39e-05 | (61.63 ms | 2077 tok/s)\n",
      "step  624/1000 | train loss 0.286217 | norm 0.6681 | lr 9.35e-05 | (61.32 ms | 2087 tok/s)\n",
      "step  625/1000 | train loss 0.285328 | norm 0.5551 | lr 9.30e-05 | (61.45 ms | 2083 tok/s)\n",
      "step  626/1000 | train loss 0.284447 | norm 0.4344 | lr 9.26e-05 | (59.81 ms | 2140 tok/s)\n",
      "step  627/1000 | train loss 0.283669 | norm 0.5265 | lr 9.22e-05 | (61.54 ms | 2080 tok/s)\n",
      "step  628/1000 | train loss 0.282877 | norm 0.5643 | lr 9.17e-05 | (62.71 ms | 2041 tok/s)\n",
      "step  629/1000 | train loss 0.282028 | norm 0.4848 | lr 9.13e-05 | (61.34 ms | 2087 tok/s)\n",
      "step  630/1000 | train loss 0.281213 | norm 0.4578 | lr 9.09e-05 | (59.77 ms | 2141 tok/s)\n",
      "step  631/1000 | train loss 0.280434 | norm 0.4990 | lr 9.04e-05 | (62.44 ms | 2050 tok/s)\n",
      "step  632/1000 | train loss 0.279641 | norm 0.4942 | lr 9.00e-05 | (63.43 ms | 2018 tok/s)\n",
      "step  633/1000 | train loss 0.278830 | norm 0.4295 | lr 8.96e-05 | (62.10 ms | 2061 tok/s)\n",
      "step  634/1000 | train loss 0.278046 | norm 0.4378 | lr 8.91e-05 | (59.75 ms | 2142 tok/s)\n",
      "step  635/1000 | train loss 0.277298 | norm 0.4744 | lr 8.87e-05 | (61.51 ms | 2081 tok/s)\n",
      "step  636/1000 | train loss 0.276498 | norm 0.4160 | lr 8.83e-05 | (62.37 ms | 2052 tok/s)\n",
      "step  637/1000 | train loss 0.275725 | norm 0.3974 | lr 8.78e-05 | (59.95 ms | 2135 tok/s)\n",
      "step  638/1000 | train loss 0.274990 | norm 0.4544 | lr 8.74e-05 | (60.53 ms | 2115 tok/s)\n",
      "step  639/1000 | train loss 0.274220 | norm 0.4229 | lr 8.70e-05 | (61.19 ms | 2092 tok/s)\n",
      "step  640/1000 | train loss 0.273460 | norm 0.3869 | lr 8.66e-05 | (60.62 ms | 2111 tok/s)\n",
      "step  641/1000 | train loss 0.272728 | norm 0.4237 | lr 8.61e-05 | (60.71 ms | 2108 tok/s)\n",
      "step  642/1000 | train loss 0.271982 | norm 0.4214 | lr 8.57e-05 | (60.80 ms | 2105 tok/s)\n",
      "step  643/1000 | train loss 0.271249 | norm 0.3918 | lr 8.53e-05 | (62.23 ms | 2057 tok/s)\n",
      "step  644/1000 | train loss 0.270538 | norm 0.4355 | lr 8.49e-05 | (60.53 ms | 2115 tok/s)\n",
      "step  645/1000 | train loss 0.269838 | norm 0.4643 | lr 8.44e-05 | (59.93 ms | 2136 tok/s)\n",
      "step  646/1000 | train loss 0.269177 | norm 0.5983 | lr 8.40e-05 | (60.43 ms | 2118 tok/s)\n",
      "step  647/1000 | train loss 0.268492 | norm 0.6212 | lr 8.36e-05 | (62.87 ms | 2036 tok/s)\n",
      "step  648/1000 | train loss 0.267678 | norm 0.4589 | lr 8.32e-05 | (61.18 ms | 2092 tok/s)\n",
      "step  649/1000 | train loss 0.266980 | norm 0.4550 | lr 8.27e-05 | (60.76 ms | 2107 tok/s)\n",
      "step  650/1000 | train loss 0.266343 | norm 0.5463 | lr 8.23e-05 | (61.18 ms | 2092 tok/s)\n",
      "step  651/1000 | train loss 0.265581 | norm 0.4205 | lr 8.19e-05 | (61.13 ms | 2094 tok/s)\n",
      "step  652/1000 | train loss 0.264905 | norm 0.4491 | lr 8.15e-05 | (62.44 ms | 2050 tok/s)\n",
      "step  653/1000 | train loss 0.264254 | norm 0.4949 | lr 8.11e-05 | (60.48 ms | 2117 tok/s)\n",
      "step  654/1000 | train loss 0.263530 | norm 0.3876 | lr 8.06e-05 | (60.18 ms | 2127 tok/s)\n",
      "step  655/1000 | train loss 0.262877 | norm 0.4433 | lr 8.02e-05 | (63.62 ms | 2012 tok/s)\n",
      "step  656/1000 | train loss 0.262213 | norm 0.4377 | lr 7.98e-05 | (62.21 ms | 2058 tok/s)\n",
      "step  657/1000 | train loss 0.261530 | norm 0.3902 | lr 7.94e-05 | (61.81 ms | 2071 tok/s)\n",
      "step  658/1000 | train loss 0.260883 | norm 0.4231 | lr 7.90e-05 | (61.31 ms | 2088 tok/s)\n",
      "step  659/1000 | train loss 0.260226 | norm 0.4040 | lr 7.86e-05 | (62.86 ms | 2036 tok/s)\n",
      "step  660/1000 | train loss 0.259576 | norm 0.4078 | lr 7.82e-05 | (60.90 ms | 2102 tok/s)\n",
      "step  661/1000 | train loss 0.258927 | norm 0.3982 | lr 7.77e-05 | (60.01 ms | 2133 tok/s)\n",
      "step  662/1000 | train loss 0.258289 | norm 0.3753 | lr 7.73e-05 | (59.77 ms | 2142 tok/s)\n",
      "step  663/1000 | train loss 0.257655 | norm 0.3944 | lr 7.69e-05 | (61.47 ms | 2082 tok/s)\n",
      "step  664/1000 | train loss 0.257016 | norm 0.3733 | lr 7.65e-05 | (64.68 ms | 1979 tok/s)\n",
      "step  665/1000 | train loss 0.256396 | norm 0.3663 | lr 7.61e-05 | (61.15 ms | 2093 tok/s)\n",
      "step  666/1000 | train loss 0.255770 | norm 0.3757 | lr 7.57e-05 | (60.25 ms | 2125 tok/s)\n",
      "step  667/1000 | train loss 0.255153 | norm 0.3637 | lr 7.53e-05 | (62.76 ms | 2040 tok/s)\n",
      "step  668/1000 | train loss 0.254540 | norm 0.3645 | lr 7.49e-05 | (63.60 ms | 2013 tok/s)\n",
      "step  669/1000 | train loss 0.253929 | norm 0.3632 | lr 7.45e-05 | (61.17 ms | 2093 tok/s)\n",
      "step  670/1000 | train loss 0.253334 | norm 0.3783 | lr 7.40e-05 | (61.67 ms | 2076 tok/s)\n",
      "step  671/1000 | train loss 0.252737 | norm 0.3807 | lr 7.36e-05 | (61.13 ms | 2094 tok/s)\n",
      "step  672/1000 | train loss 0.252162 | norm 0.4416 | lr 7.32e-05 | (63.90 ms | 2003 tok/s)\n",
      "step  673/1000 | train loss 0.251599 | norm 0.5232 | lr 7.28e-05 | (61.96 ms | 2066 tok/s)\n",
      "step  674/1000 | train loss 0.251032 | norm 0.5606 | lr 7.24e-05 | (60.66 ms | 2110 tok/s)\n",
      "step  675/1000 | train loss 0.250396 | norm 0.4257 | lr 7.20e-05 | (62.44 ms | 2050 tok/s)\n",
      "step  676/1000 | train loss 0.249795 | norm 0.3578 | lr 7.16e-05 | (61.71 ms | 2074 tok/s)\n",
      "step  677/1000 | train loss 0.249251 | norm 0.4396 | lr 7.12e-05 | (61.45 ms | 2083 tok/s)\n",
      "step  678/1000 | train loss 0.248686 | norm 0.4436 | lr 7.08e-05 | (60.28 ms | 2123 tok/s)\n",
      "step  679/1000 | train loss 0.248102 | norm 0.3879 | lr 7.04e-05 | (62.19 ms | 2058 tok/s)\n",
      "step  680/1000 | train loss 0.247524 | norm 0.3576 | lr 7.00e-05 | (62.69 ms | 2042 tok/s)\n",
      "step  681/1000 | train loss 0.246995 | norm 0.4213 | lr 6.96e-05 | (61.99 ms | 2065 tok/s)\n",
      "step  682/1000 | train loss 0.246440 | norm 0.4110 | lr 6.92e-05 | (62.07 ms | 2062 tok/s)\n",
      "step  683/1000 | train loss 0.245863 | norm 0.3367 | lr 6.88e-05 | (62.87 ms | 2036 tok/s)\n",
      "step  684/1000 | train loss 0.245338 | norm 0.3793 | lr 6.84e-05 | (64.01 ms | 2000 tok/s)\n",
      "step  685/1000 | train loss 0.244802 | norm 0.3997 | lr 6.80e-05 | (61.36 ms | 2086 tok/s)\n",
      "step  686/1000 | train loss 0.244251 | norm 0.3495 | lr 6.76e-05 | (62.15 ms | 2060 tok/s)\n",
      "step  687/1000 | train loss 0.243719 | norm 0.3493 | lr 6.73e-05 | (64.76 ms | 1977 tok/s)\n",
      "step  688/1000 | train loss 0.243195 | norm 0.3750 | lr 6.69e-05 | (62.57 ms | 2046 tok/s)\n",
      "step  689/1000 | train loss 0.242671 | norm 0.3669 | lr 6.65e-05 | (63.06 ms | 2030 tok/s)\n",
      "step  690/1000 | train loss 0.242144 | norm 0.3471 | lr 6.61e-05 | (62.55 ms | 2046 tok/s)\n",
      "step  691/1000 | train loss 0.241625 | norm 0.3464 | lr 6.57e-05 | (63.06 ms | 2030 tok/s)\n",
      "step  692/1000 | train loss 0.241118 | norm 0.3624 | lr 6.53e-05 | (60.12 ms | 2129 tok/s)\n",
      "step  693/1000 | train loss 0.240607 | norm 0.3573 | lr 6.49e-05 | (61.36 ms | 2086 tok/s)\n",
      "step  694/1000 | train loss 0.240096 | norm 0.3358 | lr 6.45e-05 | (60.13 ms | 2129 tok/s)\n",
      "step  695/1000 | train loss 0.239598 | norm 0.3529 | lr 6.41e-05 | (62.24 ms | 2057 tok/s)\n",
      "step  696/1000 | train loss 0.239104 | norm 0.3715 | lr 6.37e-05 | (60.31 ms | 2122 tok/s)\n",
      "step  697/1000 | train loss 0.238629 | norm 0.4269 | lr 6.34e-05 | (60.76 ms | 2106 tok/s)\n",
      "step  698/1000 | train loss 0.238185 | norm 0.4965 | lr 6.30e-05 | (63.06 ms | 2030 tok/s)\n",
      "step  699/1000 | train loss 0.237738 | norm 0.5782 | lr 6.26e-05 | (62.85 ms | 2037 tok/s)\n",
      "step  700/1000 | train loss 0.237206 | norm 0.4565 | lr 6.22e-05 | (63.35 ms | 2020 tok/s)\n",
      "step  701/1000 | train loss 0.236680 | norm 0.3606 | lr 6.18e-05 | (61.14 ms | 2093 tok/s)\n",
      "step  702/1000 | train loss 0.236266 | norm 0.5097 | lr 6.15e-05 | (60.41 ms | 2119 tok/s)\n",
      "step  703/1000 | train loss 0.235767 | norm 0.4185 | lr 6.11e-05 | (61.21 ms | 2091 tok/s)\n",
      "step  704/1000 | train loss 0.235272 | norm 0.3620 | lr 6.07e-05 | (63.44 ms | 2018 tok/s)\n",
      "step  705/1000 | train loss 0.234861 | norm 0.4822 | lr 6.03e-05 | (59.53 ms | 2150 tok/s)\n",
      "step  706/1000 | train loss 0.234343 | norm 0.3366 | lr 5.99e-05 | (58.61 ms | 2184 tok/s)\n",
      "step  707/1000 | train loss 0.233922 | norm 0.4090 | lr 5.96e-05 | (61.73 ms | 2074 tok/s)\n",
      "step  708/1000 | train loss 0.233454 | norm 0.3920 | lr 5.92e-05 | (63.33 ms | 2021 tok/s)\n",
      "step  709/1000 | train loss 0.232998 | norm 0.3433 | lr 5.88e-05 | (60.76 ms | 2107 tok/s)\n",
      "step  710/1000 | train loss 0.232566 | norm 0.3857 | lr 5.84e-05 | (61.19 ms | 2092 tok/s)\n",
      "step  711/1000 | train loss 0.232108 | norm 0.3362 | lr 5.81e-05 | (61.39 ms | 2085 tok/s)\n",
      "step  712/1000 | train loss 0.231681 | norm 0.3688 | lr 5.77e-05 | (61.59 ms | 2078 tok/s)\n",
      "step  713/1000 | train loss 0.231239 | norm 0.3345 | lr 5.73e-05 | (62.12 ms | 2060 tok/s)\n",
      "step  714/1000 | train loss 0.230807 | norm 0.3374 | lr 5.70e-05 | (60.64 ms | 2111 tok/s)\n",
      "step  715/1000 | train loss 0.230382 | norm 0.3517 | lr 5.66e-05 | (62.20 ms | 2058 tok/s)\n",
      "step  716/1000 | train loss 0.229951 | norm 0.3210 | lr 5.62e-05 | (63.80 ms | 2006 tok/s)\n",
      "step  717/1000 | train loss 0.229532 | norm 0.3320 | lr 5.58e-05 | (59.74 ms | 2143 tok/s)\n",
      "step  718/1000 | train loss 0.229115 | norm 0.3341 | lr 5.55e-05 | (60.15 ms | 2128 tok/s)\n",
      "step  719/1000 | train loss 0.228694 | norm 0.3216 | lr 5.51e-05 | (103.38 ms | 1238 tok/s)\n",
      "step  720/1000 | train loss 0.228287 | norm 0.3258 | lr 5.47e-05 | (64.65 ms | 1980 tok/s)\n",
      "step  721/1000 | train loss 0.227876 | norm 0.3239 | lr 5.44e-05 | (60.93 ms | 2101 tok/s)\n",
      "step  722/1000 | train loss 0.227469 | norm 0.3200 | lr 5.40e-05 | (61.64 ms | 2077 tok/s)\n",
      "step  723/1000 | train loss 0.227070 | norm 0.3249 | lr 5.37e-05 | (63.87 ms | 2004 tok/s)\n",
      "step  724/1000 | train loss 0.226667 | norm 0.3156 | lr 5.33e-05 | (62.66 ms | 2043 tok/s)\n",
      "step  725/1000 | train loss 0.226272 | norm 0.3197 | lr 5.29e-05 | (61.18 ms | 2092 tok/s)\n",
      "step  726/1000 | train loss 0.225880 | norm 0.3208 | lr 5.26e-05 | (60.84 ms | 2104 tok/s)\n",
      "step  727/1000 | train loss 0.225488 | norm 0.3118 | lr 5.22e-05 | (61.84 ms | 2070 tok/s)\n",
      "step  728/1000 | train loss 0.225102 | norm 0.3186 | lr 5.19e-05 | (62.79 ms | 2038 tok/s)\n",
      "step  729/1000 | train loss 0.224720 | norm 0.3146 | lr 5.15e-05 | (60.79 ms | 2106 tok/s)\n",
      "step  730/1000 | train loss 0.224338 | norm 0.3175 | lr 5.12e-05 | (60.92 ms | 2101 tok/s)\n",
      "step  731/1000 | train loss 0.223963 | norm 0.3176 | lr 5.08e-05 | (61.88 ms | 2068 tok/s)\n",
      "step  732/1000 | train loss 0.223594 | norm 0.3374 | lr 5.05e-05 | (60.79 ms | 2106 tok/s)\n",
      "step  733/1000 | train loss 0.223237 | norm 0.3750 | lr 5.01e-05 | (59.90 ms | 2137 tok/s)\n",
      "step  734/1000 | train loss 0.222892 | norm 0.4399 | lr 4.97e-05 | (59.42 ms | 2154 tok/s)\n",
      "step  735/1000 | train loss 0.222516 | norm 0.4085 | lr 4.94e-05 | (63.99 ms | 2000 tok/s)\n",
      "step  736/1000 | train loss 0.222124 | norm 0.3220 | lr 4.90e-05 | (62.59 ms | 2045 tok/s)\n",
      "step  737/1000 | train loss 0.221773 | norm 0.3536 | lr 4.87e-05 | (60.93 ms | 2101 tok/s)\n",
      "step  738/1000 | train loss 0.221432 | norm 0.3818 | lr 4.84e-05 | (60.01 ms | 2133 tok/s)\n",
      "step  739/1000 | train loss 0.221062 | norm 0.3309 | lr 4.80e-05 | (61.54 ms | 2080 tok/s)\n",
      "step  740/1000 | train loss 0.220709 | norm 0.3277 | lr 4.77e-05 | (62.78 ms | 2039 tok/s)\n",
      "step  741/1000 | train loss 0.220376 | norm 0.3612 | lr 4.73e-05 | (62.80 ms | 2038 tok/s)\n",
      "step  742/1000 | train loss 0.220020 | norm 0.3257 | lr 4.70e-05 | (63.00 ms | 2032 tok/s)\n",
      "step  743/1000 | train loss 0.219678 | norm 0.3216 | lr 4.66e-05 | (61.37 ms | 2086 tok/s)\n",
      "step  744/1000 | train loss 0.219347 | norm 0.3489 | lr 4.63e-05 | (61.06 ms | 2096 tok/s)\n",
      "step  745/1000 | train loss 0.219004 | norm 0.3190 | lr 4.60e-05 | (61.00 ms | 2098 tok/s)\n",
      "step  746/1000 | train loss 0.218671 | norm 0.3190 | lr 4.56e-05 | (61.48 ms | 2082 tok/s)\n",
      "step  747/1000 | train loss 0.218344 | norm 0.3343 | lr 4.53e-05 | (61.21 ms | 2091 tok/s)\n",
      "step  748/1000 | train loss 0.218015 | norm 0.3126 | lr 4.49e-05 | (61.94 ms | 2066 tok/s)\n",
      "step  749/1000 | train loss 0.217688 | norm 0.3138 | lr 4.46e-05 | (60.13 ms | 2129 tok/s)\n",
      "step  750/1000 | train loss 0.217369 | norm 0.3180 | lr 4.43e-05 | (59.64 ms | 2146 tok/s)\n",
      "step  751/1000 | train loss 0.217049 | norm 0.3083 | lr 4.39e-05 | (61.30 ms | 2088 tok/s)\n",
      "step  752/1000 | train loss 0.216733 | norm 0.3091 | lr 4.36e-05 | (62.92 ms | 2034 tok/s)\n",
      "step  753/1000 | train loss 0.216419 | norm 0.3080 | lr 4.33e-05 | (60.18 ms | 2127 tok/s)\n",
      "step  754/1000 | train loss 0.216108 | norm 0.3059 | lr 4.29e-05 | (60.48 ms | 2116 tok/s)\n",
      "step  755/1000 | train loss 0.215801 | norm 0.3058 | lr 4.26e-05 | (61.62 ms | 2077 tok/s)\n",
      "step  756/1000 | train loss 0.215493 | norm 0.3036 | lr 4.23e-05 | (61.56 ms | 2079 tok/s)\n",
      "step  757/1000 | train loss 0.215191 | norm 0.3021 | lr 4.20e-05 | (60.24 ms | 2125 tok/s)\n",
      "step  758/1000 | train loss 0.214892 | norm 0.3033 | lr 4.16e-05 | (62.63 ms | 2044 tok/s)\n",
      "step  759/1000 | train loss 0.214593 | norm 0.3026 | lr 4.13e-05 | (62.97 ms | 2033 tok/s)\n",
      "step  760/1000 | train loss 0.214299 | norm 0.3076 | lr 4.10e-05 | (61.74 ms | 2073 tok/s)\n",
      "step  761/1000 | train loss 0.214009 | norm 0.3134 | lr 4.07e-05 | (60.69 ms | 2109 tok/s)\n",
      "step  762/1000 | train loss 0.213723 | norm 0.3316 | lr 4.03e-05 | (60.94 ms | 2100 tok/s)\n",
      "step  763/1000 | train loss 0.213441 | norm 0.3376 | lr 4.00e-05 | (62.25 ms | 2056 tok/s)\n",
      "step  764/1000 | train loss 0.213161 | norm 0.3566 | lr 3.97e-05 | (62.50 ms | 2048 tok/s)\n",
      "step  765/1000 | train loss 0.212882 | norm 0.3404 | lr 3.94e-05 | (60.06 ms | 2131 tok/s)\n",
      "step  766/1000 | train loss 0.212595 | norm 0.3324 | lr 3.91e-05 | (60.05 ms | 2132 tok/s)\n",
      "step  767/1000 | train loss 0.212312 | norm 0.3158 | lr 3.87e-05 | (62.32 ms | 2054 tok/s)\n",
      "step  768/1000 | train loss 0.212041 | norm 0.3232 | lr 3.84e-05 | (62.44 ms | 2050 tok/s)\n",
      "step  769/1000 | train loss 0.211771 | norm 0.3303 | lr 3.81e-05 | (59.93 ms | 2136 tok/s)\n",
      "step  770/1000 | train loss 0.211496 | norm 0.3056 | lr 3.78e-05 | (59.74 ms | 2143 tok/s)\n",
      "step  771/1000 | train loss 0.211230 | norm 0.3095 | lr 3.75e-05 | (60.85 ms | 2104 tok/s)\n",
      "step  772/1000 | train loss 0.210969 | norm 0.3243 | lr 3.72e-05 | (61.11 ms | 2095 tok/s)\n",
      "step  773/1000 | train loss 0.210705 | norm 0.3090 | lr 3.69e-05 | (60.72 ms | 2108 tok/s)\n",
      "step  774/1000 | train loss 0.210442 | norm 0.2985 | lr 3.66e-05 | (60.71 ms | 2108 tok/s)\n",
      "step  775/1000 | train loss 0.210187 | norm 0.3119 | lr 3.62e-05 | (61.01 ms | 2098 tok/s)\n",
      "step  776/1000 | train loss 0.209935 | norm 0.3122 | lr 3.59e-05 | (63.66 ms | 2011 tok/s)\n",
      "step  777/1000 | train loss 0.209678 | norm 0.2954 | lr 3.56e-05 | (59.85 ms | 2139 tok/s)\n",
      "step  778/1000 | train loss 0.209428 | norm 0.2994 | lr 3.53e-05 | (59.59 ms | 2148 tok/s)\n",
      "step  779/1000 | train loss 0.209184 | norm 0.3040 | lr 3.50e-05 | (61.29 ms | 2088 tok/s)\n",
      "step  780/1000 | train loss 0.208935 | norm 0.2963 | lr 3.47e-05 | (77.13 ms | 1659 tok/s)\n",
      "step  781/1000 | train loss 0.208693 | norm 0.2958 | lr 3.44e-05 | (61.16 ms | 2093 tok/s)\n",
      "step  782/1000 | train loss 0.208453 | norm 0.2940 | lr 3.41e-05 | (58.78 ms | 2178 tok/s)\n",
      "step  783/1000 | train loss 0.208214 | norm 0.2986 | lr 3.38e-05 | (61.85 ms | 2070 tok/s)\n",
      "step  784/1000 | train loss 0.207979 | norm 0.2964 | lr 3.35e-05 | (61.22 ms | 2091 tok/s)\n",
      "step  785/1000 | train loss 0.207742 | norm 0.2910 | lr 3.32e-05 | (58.74 ms | 2179 tok/s)\n",
      "step  786/1000 | train loss 0.207512 | norm 0.2942 | lr 3.29e-05 | (58.32 ms | 2195 tok/s)\n",
      "step  787/1000 | train loss 0.207284 | norm 0.2960 | lr 3.26e-05 | (60.24 ms | 2125 tok/s)\n",
      "step  788/1000 | train loss 0.207055 | norm 0.2898 | lr 3.23e-05 | (61.89 ms | 2068 tok/s)\n",
      "step  789/1000 | train loss 0.206830 | norm 0.2912 | lr 3.21e-05 | (60.48 ms | 2116 tok/s)\n",
      "step  790/1000 | train loss 0.206608 | norm 0.2899 | lr 3.18e-05 | (60.10 ms | 2130 tok/s)\n",
      "step  791/1000 | train loss 0.206387 | norm 0.2934 | lr 3.15e-05 | (63.20 ms | 2025 tok/s)\n",
      "step  792/1000 | train loss 0.206170 | norm 0.2877 | lr 3.12e-05 | (61.69 ms | 2075 tok/s)\n",
      "step  793/1000 | train loss 0.205954 | norm 0.2973 | lr 3.09e-05 | (59.32 ms | 2158 tok/s)\n",
      "step  794/1000 | train loss 0.205744 | norm 0.2999 | lr 3.06e-05 | (59.07 ms | 2167 tok/s)\n",
      "step  795/1000 | train loss 0.205536 | norm 0.3181 | lr 3.03e-05 | (61.16 ms | 2093 tok/s)\n",
      "step  796/1000 | train loss 0.205325 | norm 0.3069 | lr 3.00e-05 | (64.30 ms | 1991 tok/s)\n",
      "step  797/1000 | train loss 0.205111 | norm 0.2983 | lr 2.98e-05 | (61.19 ms | 2092 tok/s)\n",
      "step  798/1000 | train loss 0.204904 | norm 0.2893 | lr 2.95e-05 | (59.93 ms | 2136 tok/s)\n",
      "step  799/1000 | train loss 0.204704 | norm 0.2963 | lr 2.92e-05 | (60.23 ms | 2125 tok/s)\n",
      "step  800/1000 | train loss 0.204504 | norm 0.3060 | lr 2.89e-05 | (61.14 ms | 2094 tok/s)\n",
      "step  801/1000 | train loss 0.204301 | norm 0.2886 | lr 2.86e-05 | (60.79 ms | 2106 tok/s)\n",
      "step  802/1000 | train loss 0.204103 | norm 0.2887 | lr 2.84e-05 | (61.65 ms | 2076 tok/s)\n",
      "step  803/1000 | train loss 0.203911 | norm 0.2997 | lr 2.81e-05 | (62.01 ms | 2064 tok/s)\n",
      "step  804/1000 | train loss 0.203716 | norm 0.2900 | lr 2.78e-05 | (63.60 ms | 2013 tok/s)\n",
      "step  805/1000 | train loss 0.203523 | norm 0.2858 | lr 2.75e-05 | (61.25 ms | 2090 tok/s)\n",
      "step  806/1000 | train loss 0.203336 | norm 0.2917 | lr 2.73e-05 | (59.31 ms | 2158 tok/s)\n",
      "step  807/1000 | train loss 0.203149 | norm 0.2915 | lr 2.70e-05 | (61.81 ms | 2071 tok/s)\n",
      "step  808/1000 | train loss 0.202962 | norm 0.2848 | lr 2.67e-05 | (63.33 ms | 2021 tok/s)\n",
      "step  809/1000 | train loss 0.202779 | norm 0.2847 | lr 2.65e-05 | (60.01 ms | 2133 tok/s)\n",
      "step  810/1000 | train loss 0.202598 | norm 0.2881 | lr 2.62e-05 | (59.17 ms | 2163 tok/s)\n",
      "step  811/1000 | train loss 0.202419 | norm 0.2866 | lr 2.59e-05 | (61.24 ms | 2090 tok/s)\n",
      "step  812/1000 | train loss 0.202240 | norm 0.2817 | lr 2.57e-05 | (64.96 ms | 1970 tok/s)\n",
      "step  813/1000 | train loss 0.202065 | norm 0.2824 | lr 2.54e-05 | (71.91 ms | 1780 tok/s)\n",
      "step  814/1000 | train loss 0.201892 | norm 0.2861 | lr 2.51e-05 | (69.39 ms | 1845 tok/s)\n",
      "step  815/1000 | train loss 0.201720 | norm 0.2824 | lr 2.49e-05 | (66.93 ms | 1913 tok/s)\n",
      "step  816/1000 | train loss 0.201549 | norm 0.2809 | lr 2.46e-05 | (63.61 ms | 2012 tok/s)\n",
      "step  817/1000 | train loss 0.201382 | norm 0.2828 | lr 2.44e-05 | (62.61 ms | 2045 tok/s)\n",
      "step  818/1000 | train loss 0.201216 | norm 0.2825 | lr 2.41e-05 | (65.24 ms | 1962 tok/s)\n",
      "step  819/1000 | train loss 0.201051 | norm 0.2820 | lr 2.39e-05 | (63.95 ms | 2002 tok/s)\n",
      "step  820/1000 | train loss 0.200889 | norm 0.2823 | lr 2.36e-05 | (60.40 ms | 2119 tok/s)\n",
      "step  821/1000 | train loss 0.200728 | norm 0.2844 | lr 2.34e-05 | (60.99 ms | 2099 tok/s)\n",
      "step  822/1000 | train loss 0.200571 | norm 0.2897 | lr 2.31e-05 | (61.91 ms | 2067 tok/s)\n",
      "step  823/1000 | train loss 0.200416 | norm 0.2942 | lr 2.28e-05 | (62.64 ms | 2043 tok/s)\n",
      "step  824/1000 | train loss 0.200263 | norm 0.3079 | lr 2.26e-05 | (60.69 ms | 2109 tok/s)\n",
      "step  825/1000 | train loss 0.200111 | norm 0.2992 | lr 2.24e-05 | (59.88 ms | 2138 tok/s)\n",
      "step  826/1000 | train loss 0.199956 | norm 0.2966 | lr 2.21e-05 | (63.26 ms | 2023 tok/s)\n",
      "step  827/1000 | train loss 0.199804 | norm 0.2856 | lr 2.19e-05 | (62.33 ms | 2053 tok/s)\n",
      "step  828/1000 | train loss 0.199657 | norm 0.2906 | lr 2.16e-05 | (61.15 ms | 2093 tok/s)\n",
      "step  829/1000 | train loss 0.199511 | norm 0.2928 | lr 2.14e-05 | (60.51 ms | 2115 tok/s)\n",
      "step  830/1000 | train loss 0.199363 | norm 0.2806 | lr 2.11e-05 | (60.72 ms | 2108 tok/s)\n",
      "step  831/1000 | train loss 0.199222 | norm 0.2876 | lr 2.09e-05 | (63.54 ms | 2014 tok/s)\n",
      "step  832/1000 | train loss 0.199081 | norm 0.2871 | lr 2.06e-05 | (61.61 ms | 2077 tok/s)\n",
      "step  833/1000 | train loss 0.198939 | norm 0.2797 | lr 2.04e-05 | (60.59 ms | 2112 tok/s)\n",
      "step  834/1000 | train loss 0.198803 | norm 0.2839 | lr 2.02e-05 | (61.60 ms | 2078 tok/s)\n",
      "step  835/1000 | train loss 0.198666 | norm 0.2827 | lr 1.99e-05 | (63.82 ms | 2006 tok/s)\n",
      "step  836/1000 | train loss 0.198531 | norm 0.2802 | lr 1.97e-05 | (61.73 ms | 2074 tok/s)\n",
      "step  837/1000 | train loss 0.198398 | norm 0.2793 | lr 1.95e-05 | (60.16 ms | 2128 tok/s)\n",
      "step  838/1000 | train loss 0.198267 | norm 0.2810 | lr 1.92e-05 | (63.19 ms | 2026 tok/s)\n",
      "step  839/1000 | train loss 0.198137 | norm 0.2789 | lr 1.90e-05 | (62.94 ms | 2034 tok/s)\n",
      "step  840/1000 | train loss 0.198009 | norm 0.2764 | lr 1.88e-05 | (60.20 ms | 2126 tok/s)\n",
      "step  841/1000 | train loss 0.197883 | norm 0.2794 | lr 1.86e-05 | (59.71 ms | 2144 tok/s)\n",
      "step  842/1000 | train loss 0.197757 | norm 0.2773 | lr 1.83e-05 | (60.63 ms | 2111 tok/s)\n",
      "step  843/1000 | train loss 0.197634 | norm 0.2758 | lr 1.81e-05 | (63.20 ms | 2025 tok/s)\n",
      "step  844/1000 | train loss 0.197512 | norm 0.2768 | lr 1.79e-05 | (60.15 ms | 2128 tok/s)\n",
      "step  845/1000 | train loss 0.197392 | norm 0.2764 | lr 1.77e-05 | (60.91 ms | 2101 tok/s)\n",
      "step  846/1000 | train loss 0.197273 | norm 0.2765 | lr 1.74e-05 | (62.97 ms | 2033 tok/s)\n",
      "step  847/1000 | train loss 0.197156 | norm 0.2749 | lr 1.72e-05 | (61.97 ms | 2065 tok/s)\n",
      "step  848/1000 | train loss 0.197040 | norm 0.2760 | lr 1.70e-05 | (60.20 ms | 2126 tok/s)\n",
      "step  849/1000 | train loss 0.196926 | norm 0.2762 | lr 1.68e-05 | (61.53 ms | 2080 tok/s)\n",
      "step  850/1000 | train loss 0.196813 | norm 0.2748 | lr 1.66e-05 | (61.66 ms | 2076 tok/s)\n",
      "step  851/1000 | train loss 0.196702 | norm 0.2749 | lr 1.63e-05 | (62.39 ms | 2052 tok/s)\n",
      "step  852/1000 | train loss 0.196592 | norm 0.2751 | lr 1.61e-05 | (61.95 ms | 2066 tok/s)\n",
      "step  853/1000 | train loss 0.196484 | norm 0.2751 | lr 1.59e-05 | (60.60 ms | 2112 tok/s)\n",
      "step  854/1000 | train loss 0.196377 | norm 0.2740 | lr 1.57e-05 | (63.37 ms | 2020 tok/s)\n",
      "step  855/1000 | train loss 0.196272 | norm 0.2744 | lr 1.55e-05 | (63.34 ms | 2021 tok/s)\n",
      "step  856/1000 | train loss 0.196168 | norm 0.2742 | lr 1.53e-05 | (61.55 ms | 2080 tok/s)\n",
      "step  857/1000 | train loss 0.196066 | norm 0.2749 | lr 1.51e-05 | (60.89 ms | 2102 tok/s)\n",
      "step  858/1000 | train loss 0.195965 | norm 0.2730 | lr 1.49e-05 | (62.96 ms | 2033 tok/s)\n",
      "step  859/1000 | train loss 0.195865 | norm 0.2758 | lr 1.47e-05 | (62.86 ms | 2036 tok/s)\n",
      "step  860/1000 | train loss 0.195767 | norm 0.2740 | lr 1.45e-05 | (61.26 ms | 2089 tok/s)\n",
      "step  861/1000 | train loss 0.195671 | norm 0.2787 | lr 1.43e-05 | (60.52 ms | 2115 tok/s)\n",
      "step  862/1000 | train loss 0.195576 | norm 0.2751 | lr 1.41e-05 | (61.62 ms | 2077 tok/s)\n",
      "step  863/1000 | train loss 0.195481 | norm 0.2778 | lr 1.39e-05 | (63.67 ms | 2010 tok/s)\n",
      "step  864/1000 | train loss 0.195388 | norm 0.2730 | lr 1.37e-05 | (60.36 ms | 2121 tok/s)\n",
      "step  865/1000 | train loss 0.195296 | norm 0.2721 | lr 1.35e-05 | (60.35 ms | 2121 tok/s)\n",
      "step  866/1000 | train loss 0.195207 | norm 0.2766 | lr 1.33e-05 | (63.03 ms | 2031 tok/s)\n",
      "step  867/1000 | train loss 0.195118 | norm 0.2734 | lr 1.31e-05 | (62.09 ms | 2062 tok/s)\n",
      "step  868/1000 | train loss 0.195030 | norm 0.2733 | lr 1.29e-05 | (63.04 ms | 2030 tok/s)\n",
      "step  869/1000 | train loss 0.194944 | norm 0.2736 | lr 1.27e-05 | (60.39 ms | 2120 tok/s)\n",
      "step  870/1000 | train loss 0.194859 | norm 0.2725 | lr 1.25e-05 | (61.64 ms | 2077 tok/s)\n",
      "step  871/1000 | train loss 0.194776 | norm 0.2742 | lr 1.23e-05 | (62.14 ms | 2060 tok/s)\n",
      "step  872/1000 | train loss 0.194693 | norm 0.2721 | lr 1.22e-05 | (61.09 ms | 2095 tok/s)\n",
      "step  873/1000 | train loss 0.194612 | norm 0.2718 | lr 1.20e-05 | (60.31 ms | 2122 tok/s)\n",
      "step  874/1000 | train loss 0.194533 | norm 0.2738 | lr 1.18e-05 | (60.20 ms | 2126 tok/s)\n",
      "step  875/1000 | train loss 0.194454 | norm 0.2717 | lr 1.16e-05 | (63.26 ms | 2023 tok/s)\n",
      "step  876/1000 | train loss 0.194376 | norm 0.2716 | lr 1.14e-05 | (61.26 ms | 2089 tok/s)\n",
      "step  877/1000 | train loss 0.194301 | norm 0.2726 | lr 1.12e-05 | (59.66 ms | 2145 tok/s)\n",
      "step  878/1000 | train loss 0.194226 | norm 0.2715 | lr 1.11e-05 | (61.39 ms | 2085 tok/s)\n",
      "step  879/1000 | train loss 0.194152 | norm 0.2720 | lr 1.09e-05 | (61.11 ms | 2095 tok/s)\n",
      "step  880/1000 | train loss 0.194079 | norm 0.2713 | lr 1.07e-05 | (61.57 ms | 2079 tok/s)\n",
      "step  881/1000 | train loss 0.194008 | norm 0.2711 | lr 1.05e-05 | (60.74 ms | 2107 tok/s)\n",
      "step  882/1000 | train loss 0.193938 | norm 0.2720 | lr 1.04e-05 | (61.89 ms | 2068 tok/s)\n",
      "step  883/1000 | train loss 0.193869 | norm 0.2706 | lr 1.02e-05 | (61.94 ms | 2067 tok/s)\n",
      "step  884/1000 | train loss 0.193801 | norm 0.2710 | lr 1.00e-05 | (61.20 ms | 2091 tok/s)\n",
      "step  885/1000 | train loss 0.193735 | norm 0.2712 | lr 9.85e-06 | (60.55 ms | 2114 tok/s)\n",
      "step  886/1000 | train loss 0.193669 | norm 0.2704 | lr 9.68e-06 | (60.74 ms | 2107 tok/s)\n",
      "step  887/1000 | train loss 0.193605 | norm 0.2712 | lr 9.52e-06 | (60.87 ms | 2103 tok/s)\n",
      "step  888/1000 | train loss 0.193541 | norm 0.2703 | lr 9.35e-06 | (60.89 ms | 2102 tok/s)\n",
      "step  889/1000 | train loss 0.193479 | norm 0.2703 | lr 9.19e-06 | (60.57 ms | 2113 tok/s)\n",
      "step  890/1000 | train loss 0.193418 | norm 0.2710 | lr 9.03e-06 | (60.45 ms | 2117 tok/s)\n",
      "step  891/1000 | train loss 0.193358 | norm 0.2700 | lr 8.87e-06 | (62.36 ms | 2052 tok/s)\n",
      "step  892/1000 | train loss 0.193299 | norm 0.2702 | lr 8.71e-06 | (60.99 ms | 2099 tok/s)\n",
      "step  893/1000 | train loss 0.193241 | norm 0.2703 | lr 8.55e-06 | (77.52 ms | 1651 tok/s)\n",
      "step  894/1000 | train loss 0.193184 | norm 0.2698 | lr 8.40e-06 | (62.22 ms | 2057 tok/s)\n",
      "step  895/1000 | train loss 0.193129 | norm 0.2703 | lr 8.24e-06 | (62.20 ms | 2058 tok/s)\n",
      "step  896/1000 | train loss 0.193074 | norm 0.2697 | lr 8.09e-06 | (62.72 ms | 2041 tok/s)\n",
      "step  897/1000 | train loss 0.193020 | norm 0.2698 | lr 7.94e-06 | (61.18 ms | 2092 tok/s)\n",
      "step  898/1000 | train loss 0.192967 | norm 0.2699 | lr 7.78e-06 | (61.28 ms | 2089 tok/s)\n",
      "step  899/1000 | train loss 0.192916 | norm 0.2695 | lr 7.64e-06 | (60.91 ms | 2101 tok/s)\n",
      "step  900/1000 | train loss 0.192865 | norm 0.2698 | lr 7.49e-06 | (60.54 ms | 2114 tok/s)\n",
      "step  901/1000 | train loss 0.192815 | norm 0.2696 | lr 7.34e-06 | (63.37 ms | 2020 tok/s)\n",
      "step  902/1000 | train loss 0.192766 | norm 0.2694 | lr 7.20e-06 | (62.03 ms | 2064 tok/s)\n",
      "step  903/1000 | train loss 0.192719 | norm 0.2696 | lr 7.05e-06 | (62.27 ms | 2056 tok/s)\n",
      "step  904/1000 | train loss 0.192672 | norm 0.2693 | lr 6.91e-06 | (60.34 ms | 2121 tok/s)\n",
      "step  905/1000 | train loss 0.192626 | norm 0.2698 | lr 6.77e-06 | (62.65 ms | 2043 tok/s)\n",
      "step  906/1000 | train loss 0.192581 | norm 0.2691 | lr 6.63e-06 | (62.44 ms | 2050 tok/s)\n",
      "step  907/1000 | train loss 0.192537 | norm 0.2707 | lr 6.49e-06 | (61.56 ms | 2079 tok/s)\n",
      "step  908/1000 | train loss 0.192494 | norm 0.2699 | lr 6.36e-06 | (60.55 ms | 2114 tok/s)\n",
      "step  909/1000 | train loss 0.192452 | norm 0.2710 | lr 6.22e-06 | (60.64 ms | 2111 tok/s)\n",
      "step  910/1000 | train loss 0.192410 | norm 0.2692 | lr 6.09e-06 | (62.46 ms | 2049 tok/s)\n",
      "step  911/1000 | train loss 0.192370 | norm 0.2689 | lr 5.96e-06 | (61.04 ms | 2097 tok/s)\n",
      "step  912/1000 | train loss 0.192331 | norm 0.2699 | lr 5.83e-06 | (61.40 ms | 2085 tok/s)\n",
      "step  913/1000 | train loss 0.192292 | norm 0.2692 | lr 5.70e-06 | (62.13 ms | 2060 tok/s)\n",
      "step  914/1000 | train loss 0.192254 | norm 0.2692 | lr 5.57e-06 | (61.66 ms | 2076 tok/s)\n",
      "step  915/1000 | train loss 0.192217 | norm 0.2691 | lr 5.44e-06 | (62.38 ms | 2052 tok/s)\n",
      "step  916/1000 | train loss 0.192181 | norm 0.2688 | lr 5.32e-06 | (60.14 ms | 2129 tok/s)\n",
      "step  917/1000 | train loss 0.192146 | norm 0.2693 | lr 5.19e-06 | (62.34 ms | 2053 tok/s)\n",
      "step  918/1000 | train loss 0.192111 | norm 0.2687 | lr 5.07e-06 | (61.72 ms | 2074 tok/s)\n",
      "step  919/1000 | train loss 0.192078 | norm 0.2686 | lr 4.95e-06 | (61.10 ms | 2095 tok/s)\n",
      "step  920/1000 | train loss 0.192045 | norm 0.2691 | lr 4.83e-06 | (61.12 ms | 2094 tok/s)\n",
      "step  921/1000 | train loss 0.192013 | norm 0.2686 | lr 4.71e-06 | (60.45 ms | 2118 tok/s)\n",
      "step  922/1000 | train loss 0.191982 | norm 0.2684 | lr 4.60e-06 | (60.24 ms | 2125 tok/s)\n",
      "step  923/1000 | train loss 0.191951 | norm 0.2688 | lr 4.48e-06 | (59.19 ms | 2163 tok/s)\n",
      "step  924/1000 | train loss 0.191921 | norm 0.2685 | lr 4.37e-06 | (60.80 ms | 2105 tok/s)\n",
      "step  925/1000 | train loss 0.191893 | norm 0.2683 | lr 4.26e-06 | (104.44 ms | 1226 tok/s)\n",
      "step  926/1000 | train loss 0.191864 | norm 0.2686 | lr 4.14e-06 | (64.02 ms | 2000 tok/s)\n",
      "step  927/1000 | train loss 0.191837 | norm 0.2683 | lr 4.04e-06 | (60.21 ms | 2126 tok/s)\n",
      "step  928/1000 | train loss 0.191810 | norm 0.2683 | lr 3.93e-06 | (63.29 ms | 2022 tok/s)\n",
      "step  929/1000 | train loss 0.191784 | norm 0.2685 | lr 3.82e-06 | (63.64 ms | 2011 tok/s)\n",
      "step  930/1000 | train loss 0.191758 | norm 0.2682 | lr 3.72e-06 | (63.24 ms | 2024 tok/s)\n",
      "step  931/1000 | train loss 0.191734 | norm 0.2682 | lr 3.61e-06 | (61.56 ms | 2079 tok/s)\n",
      "step  932/1000 | train loss 0.191710 | norm 0.2683 | lr 3.51e-06 | (59.72 ms | 2143 tok/s)\n",
      "step  933/1000 | train loss 0.191687 | norm 0.2682 | lr 3.41e-06 | (59.97 ms | 2135 tok/s)\n",
      "step  934/1000 | train loss 0.191664 | norm 0.2681 | lr 3.31e-06 | (60.96 ms | 2100 tok/s)\n",
      "step  935/1000 | train loss 0.191642 | norm 0.2682 | lr 3.21e-06 | (60.94 ms | 2100 tok/s)\n",
      "step  936/1000 | train loss 0.191621 | norm 0.2681 | lr 3.12e-06 | (60.26 ms | 2124 tok/s)\n",
      "step  937/1000 | train loss 0.191600 | norm 0.2680 | lr 3.02e-06 | (61.71 ms | 2074 tok/s)\n",
      "step  938/1000 | train loss 0.191580 | norm 0.2680 | lr 2.93e-06 | (63.05 ms | 2030 tok/s)\n",
      "step  939/1000 | train loss 0.191561 | norm 0.2681 | lr 2.84e-06 | (61.41 ms | 2084 tok/s)\n",
      "step  940/1000 | train loss 0.191542 | norm 0.2680 | lr 2.75e-06 | (60.92 ms | 2101 tok/s)\n",
      "step  941/1000 | train loss 0.191523 | norm 0.2679 | lr 2.66e-06 | (60.69 ms | 2109 tok/s)\n",
      "step  942/1000 | train loss 0.191506 | norm 0.2680 | lr 2.57e-06 | (61.77 ms | 2072 tok/s)\n",
      "step  943/1000 | train loss 0.191489 | norm 0.2679 | lr 2.48e-06 | (61.87 ms | 2069 tok/s)\n",
      "step  944/1000 | train loss 0.191472 | norm 0.2679 | lr 2.40e-06 | (60.82 ms | 2105 tok/s)\n",
      "step  945/1000 | train loss 0.191456 | norm 0.2679 | lr 2.32e-06 | (63.36 ms | 2020 tok/s)\n",
      "step  946/1000 | train loss 0.191441 | norm 0.2679 | lr 2.23e-06 | (62.81 ms | 2038 tok/s)\n",
      "step  947/1000 | train loss 0.191426 | norm 0.2678 | lr 2.15e-06 | (61.82 ms | 2071 tok/s)\n",
      "step  948/1000 | train loss 0.191412 | norm 0.2678 | lr 2.07e-06 | (61.57 ms | 2079 tok/s)\n",
      "step  949/1000 | train loss 0.191398 | norm 0.2679 | lr 2.00e-06 | (62.15 ms | 2059 tok/s)\n",
      "step  950/1000 | train loss 0.191385 | norm 0.2678 | lr 1.92e-06 | (60.99 ms | 2099 tok/s)\n",
      "step  951/1000 | train loss 0.191372 | norm 0.2677 | lr 1.85e-06 | (60.29 ms | 2123 tok/s)\n",
      "step  952/1000 | train loss 0.191360 | norm 0.2678 | lr 1.77e-06 | (61.20 ms | 2091 tok/s)\n",
      "step  953/1000 | train loss 0.191348 | norm 0.2678 | lr 1.70e-06 | (62.28 ms | 2055 tok/s)\n",
      "step  954/1000 | train loss 0.191337 | norm 0.2677 | lr 1.63e-06 | (63.15 ms | 2027 tok/s)\n",
      "step  955/1000 | train loss 0.191326 | norm 0.2677 | lr 1.56e-06 | (62.91 ms | 2035 tok/s)\n",
      "step  956/1000 | train loss 0.191315 | norm 0.2677 | lr 1.50e-06 | (60.60 ms | 2112 tok/s)\n",
      "step  957/1000 | train loss 0.191305 | norm 0.2677 | lr 1.43e-06 | (62.48 ms | 2049 tok/s)\n",
      "step  958/1000 | train loss 0.191296 | norm 0.2677 | lr 1.37e-06 | (61.67 ms | 2075 tok/s)\n",
      "step  959/1000 | train loss 0.191287 | norm 0.2677 | lr 1.30e-06 | (62.22 ms | 2057 tok/s)\n",
      "step  960/1000 | train loss 0.191278 | norm 0.2677 | lr 1.24e-06 | (61.12 ms | 2094 tok/s)\n",
      "step  961/1000 | train loss 0.191270 | norm 0.2677 | lr 1.18e-06 | (63.53 ms | 2015 tok/s)\n",
      "step  962/1000 | train loss 0.191262 | norm 0.2676 | lr 1.12e-06 | (62.34 ms | 2053 tok/s)\n",
      "step  963/1000 | train loss 0.191255 | norm 0.2676 | lr 1.07e-06 | (60.47 ms | 2117 tok/s)\n",
      "step  964/1000 | train loss 0.191248 | norm 0.2676 | lr 1.01e-06 | (62.61 ms | 2044 tok/s)\n",
      "step  965/1000 | train loss 0.191241 | norm 0.2676 | lr 9.58e-07 | (61.82 ms | 2071 tok/s)\n",
      "step  966/1000 | train loss 0.191235 | norm 0.2676 | lr 9.06e-07 | (62.00 ms | 2065 tok/s)\n",
      "step  967/1000 | train loss 0.191229 | norm 0.2676 | lr 8.55e-07 | (61.29 ms | 2089 tok/s)\n",
      "step  968/1000 | train loss 0.191223 | norm 0.2676 | lr 8.05e-07 | (60.15 ms | 2128 tok/s)\n",
      "step  969/1000 | train loss 0.191217 | norm 0.2676 | lr 7.57e-07 | (61.99 ms | 2065 tok/s)\n",
      "step  970/1000 | train loss 0.191212 | norm 0.2676 | lr 7.11e-07 | (62.50 ms | 2048 tok/s)\n",
      "step  971/1000 | train loss 0.191208 | norm 0.2676 | lr 6.66e-07 | (62.53 ms | 2047 tok/s)\n",
      "step  972/1000 | train loss 0.191203 | norm 0.2676 | lr 6.22e-07 | (60.17 ms | 2127 tok/s)\n",
      "step  973/1000 | train loss 0.191199 | norm 0.2676 | lr 5.80e-07 | (63.28 ms | 2023 tok/s)\n",
      "step  974/1000 | train loss 0.191195 | norm 0.2676 | lr 5.39e-07 | (62.56 ms | 2046 tok/s)\n",
      "step  975/1000 | train loss 0.191192 | norm 0.2676 | lr 5.00e-07 | (62.01 ms | 2064 tok/s)\n",
      "step  976/1000 | train loss 0.191188 | norm 0.2676 | lr 4.62e-07 | (60.70 ms | 2109 tok/s)\n",
      "step  977/1000 | train loss 0.191185 | norm 0.2676 | lr 4.26e-07 | (62.83 ms | 2037 tok/s)\n",
      "step  978/1000 | train loss 0.191182 | norm 0.2676 | lr 3.91e-07 | (61.16 ms | 2093 tok/s)\n",
      "step  979/1000 | train loss 0.191180 | norm 0.2676 | lr 3.58e-07 | (60.97 ms | 2099 tok/s)\n",
      "step  980/1000 | train loss 0.191177 | norm 0.2676 | lr 3.26e-07 | (61.51 ms | 2081 tok/s)\n",
      "step  981/1000 | train loss 0.191175 | norm 0.2675 | lr 2.96e-07 | (62.01 ms | 2064 tok/s)\n",
      "step  982/1000 | train loss 0.191173 | norm 0.2675 | lr 2.67e-07 | (61.90 ms | 2068 tok/s)\n",
      "step  983/1000 | train loss 0.191171 | norm 0.2675 | lr 2.40e-07 | (59.40 ms | 2155 tok/s)\n",
      "step  984/1000 | train loss 0.191170 | norm 0.2675 | lr 2.14e-07 | (58.98 ms | 2170 tok/s)\n",
      "step  985/1000 | train loss 0.191168 | norm 0.2675 | lr 1.89e-07 | (60.05 ms | 2132 tok/s)\n",
      "step  986/1000 | train loss 0.191167 | norm 0.2675 | lr 1.67e-07 | (60.68 ms | 2109 tok/s)\n",
      "step  987/1000 | train loss 0.191166 | norm 0.2675 | lr 1.45e-07 | (62.20 ms | 2058 tok/s)\n",
      "step  988/1000 | train loss 0.191165 | norm 0.2675 | lr 1.25e-07 | (67.67 ms | 1891 tok/s)\n",
      "step  989/1000 | train loss 0.191164 | norm 0.2675 | lr 1.07e-07 | (71.18 ms | 1798 tok/s)\n",
      "step  990/1000 | train loss 0.191163 | norm 0.2675 | lr 8.96e-08 | (67.57 ms | 1894 tok/s)\n",
      "step  991/1000 | train loss 0.191163 | norm 0.2675 | lr 7.40e-08 | (63.24 ms | 2024 tok/s)\n",
      "step  992/1000 | train loss 0.191162 | norm 0.2675 | lr 6.00e-08 | (63.76 ms | 2008 tok/s)\n",
      "step  993/1000 | train loss 0.191162 | norm 0.2675 | lr 4.74e-08 | (65.39 ms | 1957 tok/s)\n",
      "step  994/1000 | train loss 0.191162 | norm 0.2675 | lr 3.63e-08 | (62.70 ms | 2041 tok/s)\n",
      "step  995/1000 | train loss 0.191161 | norm 0.2675 | lr 2.66e-08 | (62.20 ms | 2058 tok/s)\n",
      "step  996/1000 | train loss 0.191161 | norm 0.2675 | lr 1.85e-08 | (60.67 ms | 2110 tok/s)\n",
      "step  997/1000 | train loss 0.191161 | norm 0.2675 | lr 1.18e-08 | (61.40 ms | 2085 tok/s)\n",
      "step  998/1000 | train loss 0.191161 | norm 0.2675 | lr 6.66e-09 | (62.85 ms | 2037 tok/s)\n",
      "step  999/1000 | train loss 0.191161 | norm 0.2675 | lr 2.96e-09 | (60.99 ms | 2099 tok/s)\n",
      "step 1000/1000 | train loss 0.191161 | norm 0.2675 | lr 7.40e-10 | (66.08 ms | 1937 tok/s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "timings = []\n",
    "norm = -1.0   # dummy value to print in inference-only mode\n",
    "for step in range(num_iterations + 1):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == num_iterations)\n",
    "\n",
    "    # once in a while evaluate the validation dataset\n",
    "    if (val_loss_every > 0 \\\n",
    "        and (step % val_loss_every == 0 or last_step)) \\\n",
    "        and (val_loader is not None):\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for _ in range(val_max_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, loss = model(x, y, return_logits=False)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= val_max_steps\n",
    "        # log to console and to file\n",
    "        print0(f\"val loss {val_loss}\")\n",
    "\n",
    "\n",
    "    # once in a while perform model inference on the master process\n",
    "    if (sample_every > 0 \\\n",
    "        and (step % sample_every == 0 or last_step)) \\\n",
    "        and master_process:\n",
    "        model.eval()\n",
    "        # before we end, let's also do one round of inference\n",
    "        # we'll kick off the generation with \"<|endoftext|>\", which designates the start of a new sequence\n",
    "        start_ids = [63]\n",
    "        xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "        max_new_tokens = 32\n",
    "        temperature = 1.0\n",
    "        top_k = 40\n",
    "        yg = raw_model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        print0('---------------')\n",
    "        print0(decode(yg[0].tolist()))\n",
    "        print0('---------------')\n",
    "\n",
    "    # bit confusing: we want to make sure to eval and sample on 0th iteration\n",
    "    # but also after the very last iteration. so we loop for step <= num_iterations\n",
    "    # instead of just < num_iterations (one extra due to <=), only to do\n",
    "    # the validation/sampling one last time, and then we break right here as we're done.\n",
    "    if last_step:\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION BEGIN -----------------\n",
    "    model.train()\n",
    "    # micro-batch loop where we do gradient accumulation to reach desired total batch size\n",
    "    lossf = 0.0 # for getting the mean loss (as simple float) over the accumulation steps\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # fetch a batch\n",
    "        if not overfit_single_batch \\\n",
    "            or (overfit_single_batch and step == 0 and micro_step == 0):\n",
    "            x, y = train_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        with ctx:\n",
    "            _, loss = model(x, y, return_logits=False)\n",
    "            # we have to scale the loss to account for gradient accumulation,\n",
    "            # because the gradients just add on each successive backward().\n",
    "            # addition of gradients corresponds to a SUM in the objective, but\n",
    "            # instead of a SUM we want MEAN, so we scale the loss here\n",
    "            loss = loss / grad_accum_steps\n",
    "            lossf += loss.detach() # keep track of the mean loss\n",
    "        # backward pass\n",
    "        if not inference_only:\n",
    "            loss.backward()\n",
    "\n",
    "    lossf = lossf.item()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    # step the optimizer\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # --------------- TRAINING SECTION END -------------------\n",
    "    # everything that follows now is just diagnostics, prints, logging, etc.\n",
    "\n",
    "    # wait on the CPU for all device work to end so we get accurate per-iteration timings below\n",
    "    if device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "    elif device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    # time and print\n",
    "    t1 = time.time()\n",
    "    # the 0th iteration is often an outlier (much slower) => skip logging it\n",
    "    tokens_per_second = grad_accum_steps * ddp_world_size * B * T / (t1-t0)\n",
    "    print0(f\"step {step+1:4d}/{num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)\")\n",
    "\n",
    "\n",
    "    # keep track of smooth timings, last 20 iterations\n",
    "    if step > 0 and step > num_iterations - 20:\n",
    "        timings.append(t1-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final 19 iters avg: 63.101ms\n",
      "peak memory consumption: 0 MiB\n"
     ]
    }
   ],
   "source": [
    "# print the average of the last 20 timings, to get something smooth-ish\n",
    "timings = timings[-20:]\n",
    "print0(f\"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
    "print0(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Example input: Hello,\n",
      "Generated output: hello,y:p\n",
      "lllllllllae\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# write an example for generating text\n",
    "sample_text = \"Hello,\"\n",
    "sample_tokens = encode(sample_text)\n",
    "sample_tokens = torch.tensor(sample_tokens, dtype=torch.long, device=device)[None, ...]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_out = model.generate(sample_tokens, max_new_tokens=15, temperature=1, top_k=20)\n",
    "\n",
    "# print the generated text\n",
    "print0('---------------')\n",
    "print0(f\"Example input: {sample_text}\")\n",
    "print0(f\"Generated output: {decode(sample_out[0].tolist())}\")\n",
    "print0('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
