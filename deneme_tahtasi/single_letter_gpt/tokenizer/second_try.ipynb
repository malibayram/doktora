{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ../../video_ve_ozet_havuzu/reproduce_gpt2/tinyshakespeare/tiny_shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115394,\n",
       " 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "data_filename = os.path.join(\"\", \"tiny_shakespeare.txt\")\n",
    "text = open(data_filename, 'r').read()\n",
    "len(text), text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "               ' ', ',', '.', '!', '?', '-']\n",
    "len(letter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_encoder(letter):\n",
    "  letter = letter.lower()\n",
    "  if letter in letter_list:\n",
    "    return letter_list.index(letter)\n",
    "  else:\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_decoder(index):\n",
    "  return letter_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_decoder(letter_encoder(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "  tokens = []\n",
    "  for letter in text:\n",
    "    token = letter_encoder(letter)\n",
    "    if token != -1:\n",
    "      tokens.append(token)\n",
    "  # %10 of the tokens are for validation and test set\n",
    "  val_test_size = len(tokens) // 10\n",
    "  train_tokens = tokens[:-2*val_test_size]\n",
    "  val_tokens = tokens[-2*val_test_size:-val_test_size]\n",
    "  test_tokens = tokens[-val_test_size:]\n",
    "  # save the tokens as binary files\n",
    "  with open(\"train_tokens2.bin\", \"wb\") as f:\n",
    "    f.write(bytes(train_tokens))\n",
    "  with open(\"val_tokens2.bin\", \"wb\") as f:\n",
    "    f.write(bytes(val_tokens))\n",
    "  with open(\"test_tokens2.bin\", \"wb\") as f:\n",
    "    f.write(bytes(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23642"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_set = set(text.lower().replace('\\n', ' ').split(\" \"))\n",
    "len(text_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 36.7k/36.7k [00:00<00:00, 2.94MB/s]\n",
      "Downloading readme: 100%|██████████| 16.0k/16.0k [00:00<00:00, 14.3MB/s]\n",
      "Downloading data: 100%|██████████| 134M/134M [01:53<00:00, 1.18MB/s] \n",
      "Generating train split: 100%|██████████| 205328/205328 [00:00<00:00, 968427.50 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 205328\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load a small dataset for testing from the Hugging Face datasets library\n",
    "dataset = load_dataset(\"wikipedia\", language=\"simple\", date=\"20220301\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset to text\n",
    "text = dataset['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205328"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413854"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"\"\n",
    "\n",
    "for i in range(100):\n",
    "  test_str += text[i].lower().replace('\\n', ' ')\n",
    "\n",
    "len(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13513,\n",
       " ['',\n",
       "  'australasia,',\n",
       "  'phillip,',\n",
       "  'a\\xa0',\n",
       "  'belgium,',\n",
       "  'tattoo',\n",
       "  'personal',\n",
       "  '0s).',\n",
       "  'words',\n",
       "  'gone'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_set = list(set(test_str.split(\" \")))\n",
    "\n",
    "len(text_set), text_set[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roots',\n",
       " 'third-largest',\n",
       " 'jules',\n",
       " 'grammminology/',\n",
       " 'africa:',\n",
       " 'sides',\n",
       " 'sue',\n",
       " 'aymará',\n",
       " 'speed',\n",
       " 'y',\n",
       " 'otherwise,',\n",
       " 'rumi,',\n",
       " 'functions',\n",
       " \"(today's\",\n",
       " 'profiles',\n",
       " 'plateaus',\n",
       " 'doubted.',\n",
       " 'abbasids',\n",
       " 'curve.',\n",
       " 'savimbi',\n",
       " 'mays,',\n",
       " 'foil,',\n",
       " 'wine.',\n",
       " 'military',\n",
       " 'line.',\n",
       " 'authors,',\n",
       " 'homo,',\n",
       " 'dramatically,',\n",
       " 'lydia',\n",
       " '(dame',\n",
       " 'adds',\n",
       " 'indivisible,',\n",
       " 'truly',\n",
       " 'toots',\n",
       " ')',\n",
       " 'rebellion',\n",
       " 'table',\n",
       " 'by.',\n",
       " '1804',\n",
       " 'direction),',\n",
       " 'artist,',\n",
       " 'dynasty.',\n",
       " 'information,',\n",
       " 'cellular',\n",
       " 'barcelona)',\n",
       " 'productive',\n",
       " 'forth',\n",
       " 'explorers.',\n",
       " 'indiana.',\n",
       " 'authority',\n",
       " 'heads',\n",
       " 'ensor',\n",
       " 'cross-quarter',\n",
       " 'crossword',\n",
       " 'casting',\n",
       " 'few',\n",
       " 'darkness',\n",
       " 'explodes',\n",
       " 'fish-traps',\n",
       " 'harvest.',\n",
       " 'inverse',\n",
       " 'cobs.',\n",
       " 'trappist',\n",
       " 'acronyms',\n",
       " 'osaka,',\n",
       " 'italian.',\n",
       " 'kurdish',\n",
       " 'initiates',\n",
       " 'toxic.',\n",
       " 'team',\n",
       " 'argon,',\n",
       " 'occitan',\n",
       " '1994',\n",
       " '():',\n",
       " 'notations',\n",
       " \"'new\",\n",
       " 'imprisoned',\n",
       " 'numbers',\n",
       " 'industry.',\n",
       " 'post',\n",
       " 'toledo)',\n",
       " 'groups',\n",
       " 'lambs',\n",
       " '“cantoris”',\n",
       " \"britain's\",\n",
       " 'abolition',\n",
       " 'graph,',\n",
       " 'teaching',\n",
       " 'deserted',\n",
       " 'blossom',\n",
       " 'http://philtar.ucsm.ac.uk/encyclopedia/christ/west/cathar.html',\n",
       " 'decelerates.',\n",
       " 'raspberry,',\n",
       " 'atmosphere,',\n",
       " 'known,',\n",
       " 'habana\".',\n",
       " 'judge',\n",
       " 'politicians.',\n",
       " 'gamma',\n",
       " 'electrostatic']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# generate a random text contains just 100 different words\n",
    "# get random words from the text\n",
    "english_word_set = set()\n",
    "\n",
    "while len(english_word_set) < 100:\n",
    "  word = np.random.choice(list(text_set))\n",
    "  english_word_set.add(word)\n",
    "\n",
    "english_word_list = list(english_word_set)\n",
    "english_word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87811,\n",
       " \"information, roots teaching dynasty. inverse industry. dramatically, abbasids (today's aymará third-\")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create 100 random sentences with 100 words\n",
    "english_text = \"\"\n",
    "for i in range(100):\n",
    "  english_text += \" \".join(np.random.choice(english_word_list, 100)) + \" \"\n",
    "\n",
    "len(english_text), english_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(english_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8453\n",
      "s third-largest kurdish  teaching judge britains toxic. explorers. harvest. cellular  grammminology \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "with open(\"val_tokens2.bin\", \"rb\") as f:\n",
    "  val_tokens = f.read()\n",
    "  val_tokens = [x for x in val_tokens]\n",
    "  print(len(val_tokens))\n",
    "  val_text = \"\".join([letter_decoder(x) for x in val_tokens])\n",
    "  print(val_text[:100])\n",
    "  tokens = np.frombuffer(f.read(), dtype=np.uint16)\n",
    "  print(len(tokens))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
