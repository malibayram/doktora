{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-chroma chromadb langchain-ollama --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this import for running in jupyter notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains import VectorDBQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're asking about LangChain! It's a pretty cool tool. Here's a breakdown: \\n\\n**LangChain** is like a toolbox for building applications with language models. Think of it this way: language models (like me!) are great at understanding and generating text, but they need some help to do more complex tasks. \\n\\nLangChain provides the pieces you need to connect language models to other tools and data sources, allowing you to create powerful applications.  For example, you could use LangChain to:\\n\\n* **Build chatbots:** Create conversational agents that can answer questions, provide information, and even complete tasks.\\n* **Summarize documents:** Automatically generate concise summaries of large chunks of text.\\n* **Translate languages:**  Quickly and accurately translate text between different languages.\\n* **Write different kinds of creative content:**\\n\\nGenerate stories, poems, articles, and more.\\n\\nLangChain is still under development, but it's already being used by developers to create some amazing things.\\n\\n\\nLet me know if you have any other questions about LangChain or want to explore specific use cases!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"gemma2\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1003, which is longer than the specified 1000\n",
      "Created a chunk of size 1266, which is longer than the specified 1000\n",
      "/Users/alibayram/Library/Python/3.9/lib/python/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/alibayram/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "documents = TextLoader(\"alice.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'alice.txt'}, page_content='Birinci tanık, Şapkacıydı. Buraya bir elinde çay fincanı, diğerinde bir yağlı ekmekle gelmişti. \"Böyle geldiğim için, özür dilerim, Majesteleri,\" diye başladı söze, \"fakat buraya çağırıldığımda henüz çayımı bitirmemiştim.\"\\n\\n\"Bitirmiş olmalıydın,\" dedi Kral. \"Ne zaman başlamıştın?\"\\n\\nŞapkacı, Mart Tavşanına baktı, mahkemeye gelirken Tarla Faresiyle kol kola peşinden gelmişti\". \"Martın on dördüydü sanırım,\" dedi.\\n\\n\"On beşi\" dedi Mart Tavşanı.\\n\\n\"On altısı\" diye ekledi Tarla Faresi.\\n\\nKral jüriye, \"Bunu yazın,\" dedi ve jüri bu üç tarihi özenle yazı tahtalarına geçirdiler, sonra bunları topladılar ve buldukları yanıtı şilin ve peniye çevirdiler.\\n\\n\\nKral, \"Şapkanı çıkar,\" dedi Şapkacıya.\\n\\n\"Şapka bana ait değil,\" dedi Şapkacı.\\n\\n\"Çalınmış!\" diye vurguladı Kral, ve hemen o anda durumu değerlendirmeye başlayan jüriye döndü.\\n\\n\"Ben bunları satmak için taşıyorum,\" diye açıklamasını yaptı Şapkacı; \"Bunların hiç biri bana ait değil. Ben bir şapkacıyım.\"'),\n",
       " Document(metadata={'source': 'alice.txt'}, page_content='Birinci tanık, Şapkacıydı. Buraya bir elinde çay fincanı, diğerinde bir yağlı ekmekle gelmişti. \"Böyle geldiğim için, özür dilerim, Majesteleri,\" diye başladı söze, \"fakat buraya çağırıldığımda henüz çayımı bitirmemiştim.\"\\n\\n\"Bitirmiş olmalıydın,\" dedi Kral. \"Ne zaman başlamıştın?\"\\n\\nŞapkacı, Mart Tavşanına baktı, mahkemeye gelirken Tarla Faresiyle kol kola peşinden gelmişti\". \"Martın on dördüydü sanırım,\" dedi.\\n\\n\"On beşi\" dedi Mart Tavşanı.\\n\\n\"On altısı\" diye ekledi Tarla Faresi.\\n\\nKral jüriye, \"Bunu yazın,\" dedi ve jüri bu üç tarihi özenle yazı tahtalarına geçirdiler, sonra bunları topladılar ve buldukları yanıtı şilin ve peniye çevirdiler.\\n\\n\\nKral, \"Şapkanı çıkar,\" dedi Şapkacıya.\\n\\n\"Şapka bana ait değil,\" dedi Şapkacı.\\n\\n\"Çalınmış!\" diye vurguladı Kral, ve hemen o anda durumu değerlendirmeye başlayan jüriye döndü.\\n\\n\"Ben bunları satmak için taşıyorum,\" diye açıklamasını yaptı Şapkacı; \"Bunların hiç biri bana ait değil. Ben bir şapkacıyım.\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Bunda PEK fazla şaşılacak bir şey yoktu.\"\n",
    "\n",
    "# Query the database\n",
    "results = vectordb.similarity_search(query, k=2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"kim 'Böyle geldiğim için, özür dilerim, Majesteleri.' dedi? Lütfen türkçe cevap veriniz.\",\n",
       " 'result': 'Şapkacı bu cümleyi söyledi. \\n\\n\\n'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qachain = RetrievalQA.from_chain_type(model, retriever=vectordb.as_retriever())\n",
    "res = qachain.invoke({\"query\": \"kim 'Böyle geldiğim için, özür dilerim, Majesteleri.' dedi? Lütfen türkçe cevap veriniz.\"})\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
